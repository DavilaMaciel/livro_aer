[{"path":"index.html","id":"bem-vindos","chapter":"Bem-vindos","heading":"Bem-vindos","text":" Essa é página online livro Análises Ecológicas R. Este livro contém 15 capítulos que vão desde como baixar e instalar o R, passando por manipulação de dados, análises de diversidade, estatística básica, métodos multidimensionais e visualização de dados geoespaciais. Este é um material introdutório destinado principalmente estudantes de graduação e cursos de pós-graduação em ecologia e áreas correlatas. Esperamos que ele seja utilizado tanto por quem quer se aprofundar em análises comumente utilizadas em ecologia, quanto por quem não tem nenhuma ou poucas habilidades quantitativas. Este livro é um dos poucos, senão único, material em português com essa abrangência e visa ser o principal manual de entrada para aprendizagem de R e suas aplicações em ecologia, conservação e ciências ambientais. Para isso, propomos uma estratégia que facilite escolha teste estatístico, por meio da seleção de questões/hipóteses claras e da ligação dessas hipóteses com teoria e o método. Ainda, cada capítulo contém exercícios propostos, e suas soluções, recomendações de literatura e links especializados para cada capítulo livro, e uma sessão sobre como utilizar o livro em grupos de discussão e sala de aula.versão -line está disponível gratuitamente e o código fonte necessário para reproduzir todo o conteúdo livro está depositado Github.Este livro foi escrito em Rmarkdown e compilado utilizando o pacote bookdown.","code":""},{"path":"index.html","id":"ebook","chapter":"Bem-vindos","heading":"eBook","text":"O livro também pode ser acessado gratuitamente formato de eBook.","code":""},{"path":"index.html","id":"físico","chapter":"Bem-vindos","heading":"Físico","text":"O livro físico pode ser adquirido pelo Clube de Autores ou pela Amazon.","code":""},{"path":"index.html","id":"como-citar","chapter":"Bem-vindos","heading":"Como citar","text":"Recomendamos aos(às) leitores() que sempre que possível, façam citação livro em seus trabalhos acadêmicos.Da Silva FR, Gonçalves-Souza T, Paterno GB, Provete DB, Vancine MH. 2022. Análises ecológicas R. Nupeea : Recife, PE, Canal 6 : São Paulo. 640 p. ISBN 978-85-7917-564-0.","code":"@book{da_silva_etal_r_2022,\n    address = {Recife, PE},\n    edition = {Primeira edição},\n    title = {Análises ecológicas no R},\n    isbn = {9788579175640},\n    shorttitle = {Análises ecológicas no R},\n    abstract = {\"Este livro introduz a você a análise de dados ecológicos através da linguagem R. O livro descreve como os códigos devem ser consequências das perguntas que a pesquisa ecológica pretende responder. O texto é intercalado com pedaços organizados e claros de código e gráficos, o que torna a leitura dos capítulos bastante fluida e dinâmica, principalmente para quem gosta de executar os códigos no seu computador conforme lê os capítulos. É como uma aula prática guiada.\"--},\n    publisher = {Nupeea},\n    author = {Da Silva FR, Gonçalves-Souza T, Paterno GB, Provete DB, Vancine MH},\n    year = {2022},\n    note = {},\n    keywords = {Análises dados – Ecologia, Linguagem de programação (Computadores), Métodos científicos, Tecnologia – Aspectos ambientais},\n}"},{"path":"index.html","id":"como-contribuir","chapter":"Bem-vindos","heading":"Como contribuir","text":"Caso encontre erros livro ou tenha sugestões para melhorar seu conteúdo, abra um chamado repositório (é necessário que você se cadastre GitHub) e descreva o problema encontrado ou apresente suas sugestões. Caso encontre erros de digitação, você pode corrigi-los clicando em “Edite esta página” na barra lateral direita. Depois de corrigir os erros, submeta um pull request pelo GitHub. Sua contribuição é muito importante e bem-vinda para que possamos aprimorar o conteúdo livro para próximas edições.","code":""},{"path":"index.html","id":"suporte-ao-projeto","chapter":"Bem-vindos","heading":"Suporte ao projeto","text":"Mesmo com explosão de ferramentas de tradução automática, bem como livros, blogs e tutoriais, ainda temos impressão de que este material é necessário e poderá impactar positivamente o ensino de estatística para discentes e profissionais de ecologia e áreas correlatas. Por esse motivo, criamos um projeto para atualizar e ampliar o livro ao longo tempo. ideia mais interessante é que não pretendemos investir somente na produção livro, mas também oferecer cursos em universidades e regiões com pouco investimento e recursos, criar podcasts e vídeos YouTube, entre outras ideias.É importante ressaltar que tudo isso é (e sempre será!) feito gratuitamente, em um projeto totalmente aberto e sem fins lucrativos. Além disso, para manter essa ideia funcionando, temos necessidade de adquirir computadores e outros equipamentos como câmeras e microfones. Assim, para que esse nosso sonho vire realidade, usuários() que têm interesse em ajudar o projeto podem fazê-lo de diferentes formas.Você pode recomendar o livro para outras pessoas e dizendo que essa iniciativa existeVocê pode recomendar o livro para outras pessoas e dizendo que essa iniciativa existeVocê pode divulgar o livro e o projeto em suas redes sociais por meio da hashtag #ecologiaR para ajudar dar visibilidade ao projetoVocê pode divulgar o livro e o projeto em suas redes sociais por meio da hashtag #ecologiaR para ajudar dar visibilidade ao projetoSe você é divulgador() científico() e pretende usar nosso livro em seu canal, fale brevemente nosso projeto e indique para seus(suas) seguidores()Se você é divulgador() científico() e pretende usar nosso livro em seu canal, fale brevemente nosso projeto e indique para seus(suas) seguidores()Você pode citar ou linkar o livro em seus trabalhos acadêmicosVocê pode citar ou linkar o livro em seus trabalhos acadêmicosVocê pode marcar com uma estrela (‘Starring’) o repositório GitHub livro_aerVocê pode marcar com uma estrela (‘Starring’) o repositório GitHub livro_aerVocê pode se inscrever nosso canal YouTube, curtir os vídeos, comentar e compartilharVocê pode se inscrever nosso canal YouTube, curtir os vídeos, comentar e compartilharVocê pode adquirir o livro físico pelo Clube de Autores ou pela Amazon e, desse modo, ajudar com os direitos autorais que retornam para o projetoVocê pode adquirir o livro físico pelo Clube de Autores ou pela Amazon e, desse modo, ajudar com os direitos autorais que retornam para o projetoSe você tem condições de “adotar” uma universidade, é possível adquirir o livro físico nas plataformas indicadas, comprar o livro e indicar biblioteca onde o livro será entregue/depositadoSe você tem condições de “adotar” uma universidade, é possível adquirir o livro físico nas plataformas indicadas, comprar o livro e indicar biblioteca onde o livro será entregue/depositadoVocê pode fazer doações diretas, em qualquer quantia, para o Pix analisesecologicasnor@gmail.com ou via PayPal (para Maurício Humberto Vancine, um dos coautores)Você pode fazer doações diretas, em qualquer quantia, para o Pix analisesecologicasnor@gmail.com ou via PayPal (para Maurício Humberto Vancine, um dos coautores)Observação: O valor arrecadado com venda dos livros físicos e doações, bem como forma como utilizaremos esses valores serão sempre informados site livro e nas mídias sociais com hashtag #ecologiaR.","code":""},{"path":"index.html","id":"youtube","chapter":"Bem-vindos","heading":"YouTube","text":"Criamos um canal YouTube para divulgarmos cursos, aulas e outros conteúdos. Inscreva-se para ter mais informações sobre o livro e o projeto.","code":""},{"path":"index.html","id":"licença","chapter":"Bem-vindos","heading":"Licença","text":"versão online deste livro será sempre gratuita e distribuída sob liceça CC -NC 4.0.página online livro está hospedada pela plataforma Netlify.","code":""},{"path":"prefácio.html","id":"prefácio","chapter":"Prefácio","heading":"Prefácio","text":"evolução dos computadores pessoais e ampliação acesso estes e à Internet têm transformado o jeito como aprendemos e ensinamos. Especificamente sobre investigação da natureza através de métodos científicos, maior oferta de ferramentas gratuitas para análise de dados e maior facilidade de acesso dados talvez estejam entre maiores transformações ocorridas nos últimos 20 anos. Hoje é relativamente comum, mesmo entre cientistas em formação, conversar sobre novos pacotes, códigos de programação, repositório de dados de acesso público e reprodutibilidade de análises. Para isso ter acontecido, além da evolução tecnológica, também foi preciso que muitas pessoas estivessem dispostas ensinar como usar essas ferramentas. Este livro é uma dessas contribuições para o contínuo avanço ensino de métodos computacionais, com um foco específico em análise de dados ecológicos através da linguagem R.Além da abrangência dos tópicos abordados (desde teste T até análises geoespaciais, passando por diversidade filogenética), há outro aspecto marcante neste livro – e que o torna diferente em relação ao material comumente encontrado em sites, listas, blogs e manuais: visão dos autores sobre como os códigos devem ser consequências das perguntas que pesquisa pretende responder (veja detalhes cap. 1). Essa visão tem como consequência um livro que começo ao fim conecta teoria ecológica, métodos científicos, análises quantitativas e programação. Isso é feito de modo explícito através de exemplos claros e didáticos que apresentam contexto e dados reais, um ou mais exemplos de perguntas que poderiam ser feitas, predições relacionadas às perguntas e teoria em questão, além das variáveis que poderiam ser utilizadas nas análises. O texto que descreve essas partes é intercalado com pedaços organizados e claros de código e gráficos, o que torna leitura dos capítulos bastante fluida e dinâmica, principalmente para quem gosta de executar os códigos seu computador conforme lê os capítulos. É como uma aula prática guiada. Um terceiro aspecto marcante livro é que boa parte das análises também é explicada conceitualmente. Por exemplo, quando o teste T é introduzido, isso não é feito somente com códigos que permitem o cálculo da estatística. Há explicações sobre o funcionamento e premissas da análise. Isso nos estimula tentarmos entender o que acontece por trás dos códigos.Quando eu comecei ensinar análise de dados ecológicos para discentes de graduação em Ecologia em 2012, na UNESP de Rio Claro, eu dizia para turmas que nós tínhamos um grande privilégio: um livro texto específico para nossa área e em português – “Princípios de Estatística em Ecologia” (Gotelli e Ellison) e uma apostila de onde eu tirava algumas instruções e alguns exercícios em R também em português. apostila, elaborada pelos pesquisadores Diogo B. Provete, Fernando R. da Silva e Thiago Gonçalves-Souza, era o embrião deste livro (veja detalhes sobre o histórico livro cap. 1) e um dos alunos para quem eu dizia sobre o privilégio era o Maurício Vancine. Portanto, é uma satisfação pessoal enorme escrever este prefácio, não só por conhecer quatro dos cinco autores e ter usado uma versão anterior material, como também por considerar o livro uma das fontes mais abrangentes sobre análise de dados em R. Eu sugiro que você marque o link livro como um favorito seu navegador de Internet.Apesar de os autores não terem separado os capítulos em partes, podemos considerar que há dois grupos de capítulos. primeiro grupo – que inclui dos capítulos 1 ao 6, nós somos apresentados aos aspectos mais gerais da estrutura livro, seus objetivos e sobre o funcionamento da linguagem R. Neste grupo, nós aprendemos desde como instalar o R e o RStudio (cap. 3), até como produzir gráficos bonitos e informativos (cap. 6). segundo grupo de capítulos (7 ao 15), temos contato com análises específicas e atualmente usadas em ecologia, incluindo desde modelos lineares simples (cap. 7), até análise de dados geoespaciais complexos (cap. 15), passando por diferentes tipos de estratégia para descrever biodiversidade (cap. 10-14).Este tipo de estrutura permite que o livro possa ser usado como um curso completo ou como guia que consultamos quando esquecemos de como fazer algo. Por exemplo, eu sempre consulto o capítulo 5, principalmente quando vou usar função map pacote purrr. Ou seja, é possível ler quase todos os capítulos de modo independente, pois todos têm uma seção de pré-requisitos ou uma seção de introdução/contextualização em que os objetivos capítulo são apresentados.Entretanto, se você está começando aprender R, sugiro que leia os capítulos conforme eles são apresentados, principalmente capítulo 1 ao 6. Depois, você pode ir para os capítulos que mais te interessam. E já que o assunto é aprendizagem, capítulo 1 os autores apresentam ideias criativas e úteis para quem tem interesse em ensinar e aprender com o livro. Gostei bastante das sugestões para grupos de pessoas que querem aprender de forma autônoma, por meio de grupos de estudo. O capítulo 2 apresenta uma maneira bastante interessante de se usar um dos métodos científicos mais usados – o método hipotético-dedutivo – com fluxogramas para: () identificar variáveis relevantes e como elas afetam umas às outras, (b) melhorar (quando necessário) o desenho amostral, (c) facilitar escolha de análises estatísticas, e (d) melhorar interpretação e comunicação dos dados e análises. Os capítulos 4 e 5 são essenciais para quem ainda não tem muita familiaridade com o uso R ou para quem quer revisitar aspectos fundamentais funcionamento da linguagem. O capítulo 4 inclui, por exemplo, explicações sobre console, script, operadores, objetos, funções, pacotes, ajuda e principais erros. O capítulo 5 apresenta uma das maiores inovações surgidas nos últimos anos na comunidade que trabalha com R, “uma filosofia de design, gramática e estruturas” agrupadas em conjuntos de pacotes sob nome guarda-chuva “tidyverse”. Podemos entender o tidyverse como um “dialeto novo” para linguagem R, onde tidy quer dizer “organizado, arrumado, ordenado”, e verse é “universo” (cap. 5). Para quem não usa ou não se acostumou usar o operador pipe %>% para o encadeamento de funções, o capítulo 5 é bastante útil, pois uma parte subsequente livro usa essa abordagem. O capítulo 6 é uma consequência natural capítulo 5, pois usa lógica da gramática dos gráficos, que está fortemente ligada ao tidyverse, e porta de entrada para os próximos capítulos mais específicos sobre análise de dados. Afinal, não fazemos (ou pelo menos não devemos fazer) nenhuma análise de dados sem antes explorá-los graficamente.Para quem já tem pelo menos alguma familiaridade com linguagem R, o segundo conjunto de capítulos (7 ao 15) pode ser acessado mais diretamente. Aqui também é possível pensar numa estrutura. capítulo 7 ao 9, o foco é em métodos estatísticos usados para explorar e estimar associação entre variáveis. Enquanto o capítulo 7 e 8 são totalmente focados em modelos lineares univariados – em que variável resposta é um único vetor numérico, o capítulo 9 trata de métodos em que variável resposta ou variável de interesse é uma matriz – .e., chamadas técnicas multidimensionais ou multivariadas. Eu gostei de ler estes três capítulos na sequência em que são apresentados. É assim que eu ensino também – começamos com uma variável resposta e uma variável explanatória e vamos adicionando variáveis e tipos de variáveis, primeiro lado direito da equação (parte explanatória) e depois na parte esquerda da equação (parte que está sendo modelada).capítulo 10 ao 14, aprendemos como estimar diferentes facetas da biodiversidade – taxonômica, funcional e filogenética, e os componentes alfa, beta e gama – e como podemos usar algumas dessas medidas para fazer inferências sobre processos de montagem de comunidades. O capítulo 13 sobre diversidade filogenética merece ser destacado; é um dos mais completos e abrangentes livro. Apesar de não ser exatamente minha área de atuação, acho que boa parte dos métodos atualmente mais usados está contemplada ali. Em todos estes capítulos há explicações sobre aspectos teóricos que suportam o uso das técnicas em diferentes situações. Tudo é feito com dados que podem ser acessados. Portanto, se você ainda não coletou os dados seu projeto, mas já sabe quais análises pretende usar, estes capítulos são uma fonte excelente para sua aprendizagem. Não precisa esperar até ter todos os dados em mãos.Finalmente, o capítulo 15 fecha o livro nos apresentado uma introdução aos principais conceitos sobre manipulação e visualização de dados geoespaciais R. Ainda é comum para quem trabalha com ecologia e quer usar técnicas de análises de dados geoespaciais como algo complementar convidar especialistas da área para uma parceria. O Maurício Vancine já não deve nem conseguir atender todos os pedidos de parceria. Aqui está então uma boa oportunidade para você aprender dar os primeiros passos, produzir os primeiros mapas, fazer predições espaciais sem ter de depender da ajuda de pessoal especializado. Mas mais importante que isso, o capítulo fornece base conceitual e terminologia apropriada para o bom uso deste tipo de análise – eu sempre confundo com terminologia associada ao sistema de referência de coordenadas e unidades.Há quem diga que velocidade com que tecnologia e ciência avançam tende tornar livros e manuais sobre métodos rapidamente obsoletos. Não acho que isso vai acontecer com o livro “Análises Ecológicas R”. Os métodos apresentados são bem estabelecidos e (alguns) aceitos pela comunidade científica há muitos anos e incluem técnicas modernas que ainda estão sendo entendidas e absorvidas por profissionais que atuam em ecologia. Que privilégio ter um livro como este em português e gratuito! Obrigado aos autores por isso. Boa leitura e prática.Tadeu Siqueira\nInstituto de Biociências\nUniversidade Estadual Paulista - UNESP","code":""},{"path":"dedicatória.html","id":"dedicatória","chapter":"Dedicatória","heading":"Dedicatória","text":"Dedicamos esse livro às nossas companheiras que nos apoiaram durante o longo processo de construção livro, e aos nossos filhos e filhas pelas distrações e alegrias quando estávamos longe dos infinitos erros durante compilações.","code":""},{"path":"sobre-os-autores.html","id":"sobre-os-autores","chapter":"Sobre os autores","heading":"Sobre os autores","text":"Fernando Rodrigues da SilvaPiadista descomedido de Barueri/SP, pai da Ária, companheiro da Winter, responsável pela Filomena (gata), Zé (gato), Cenoura (cachorra) e Chica (cachorra), admirador de botecos, viagens, mato, cerveja, poker, basquete e prosa com os amigos. Formado em Ciências Biológicas na UNESP-Assis e com mestrado e doutorado em Biologia Animal pela UNESP-São José Rio Preto. Realizou doutorado sanduíche na SUNY – College Environmental Science Forest. Atualmente é Professor Associado II Departamento de Ciências Ambientais, UFSCAR-Sorocaba e trabalha na linha de pesquisa de ecologia de comunidades, metacomunidades, macroecologia, biogeografia e história natural de anfíbios.Site: http://fernandoecologia.wix.com/fernandorodriguesThiago Gonçalves-SouzaCapixaba de Cachoeiro Itapemirim/ES, pai Lucas, companheiro da Natália, praticante de Muay Thai, Boxe e da arte de provar cervejas. Formado em Ciências Biológicas na Escola Superior São Francisco de Assis/ES e com mestrado e doutorado em Biologia Animal pela UNESP-São José Rio Preto. Realizou doutorado sanduíche na University Guelph (Canadá) e pós-doutorado na UNICAMP. Atualmente é Professor Adjunto III Departamento de Biologia da Universidade Federal Rural de Pernambuco, trabalhando nas linhas de pesquisa de ecologia de comunidades, ecologia funcional, macroecologia e metacomunidades.Site: https://thiagocalvesouza.wixsite.com/ecofunGustavo Brant PaternoCompositor de cantigas e violeiro, nascido em Ribeirão Preto/SP, pai Rudá e da Maria Flor, e apaixonado por Mila. Admira profundamente incrível diversidade da vida na Terra. Gosta de trilha, música, skate, surf, fotografia de estrelas e programação. É Ecólogo com muito orgulho pela formação na UFRN/Natal, com mestrado e doutorado em Ecologia pela mesma universidade. Realizou sanduíche na Macquarie University (Austrália). Possui grande interesse em Ciência aberta e software livre e é embaixador projeto Open Science Framework (OSF). Atualmente é pesquisador de pós-doutorado na Faculty Forest Sciences Forest Ecology da Universidade de Gottingen (Alemanha). Atua na interface entre ecologia evolutiva, biodiversidade-funcionamento de ecossistemas e ecologia da restauração.Site: https://gustavopaterno.netlify.app/Diogo Borges ProveteCapixaba da gema, nascido em Bom Jesus Norte/ES (na verdade registrado nessa cidade e nascido em Bom Jesus Itabapoana/RJ outro lado da ponte, porque ES não tinha hospital :D ), pai orgulhoso da Manuela e esposo apaixonado da Lilian. Apreciador das boas Ale Belgas e Inglesas e vinhos Chilenos e Brasileiros. Já fui nadador, jogador de futebol, e um péssimo jogador de Xadrez. Dizem que gosto de livros e HQs. Graduado em Ciências Biológicas na Universidade Federal de Alfenas-MG, com mestrado em Biologia Animal na UNESP-São José Rio Preto e Doutorado em Ecologia e Evolução na Universidade Federal de Goiás. Atualmente é Professor Adjunto na Universidade Federal de Mato Grosso Sul. Meu programa de pesquisa tenta integrar evolução fenotípica e processos micro- e macroevolutivos para entender padrões de distribuição de espécies em escala de metacomunidades, especialmente em ambientes de água doce.Site: http://diogoprovete.weebly.comMaurício Humberto VancineCaipira interior de Socorro/SP, pai Dudu, companheiro da Japa, amante de música instrumental, livros, games, softwares livres, programação, uma boa cerveja, além de um dedo de cachaça e uma longa prosa. Mais recentemente tenta não levar muitos tombos ao aprender andar de skate depois dos 30. Graduado em Ecologia e mestre em Zoologia, ambos pela UNESP-Rio Claro. Atualmente é Doutorando PPG de Ecologia, Evolução e Biodiversidade da UNESP-Rio Claro e atua na linha de pesquisa de Ecologia Espacial, Ecologia da Paisagem, Modelagem Ecológica e Ecologia de Anfíbios.Site: https://mauriciovancine.github.io/","code":""},{"path":"revisores-e-colaboradores.html","id":"revisores-e-colaboradores","chapter":"Revisores e colaboradores","heading":"Revisores e colaboradores","text":"Expressamos nossos sinceros agradecimentos aos pesquisadores, alunos e colegas indicados abaixo, pelo imprescindível trabalho de avaliação, revisão e crítica conteúdo deste livro.André Padial - Setor de Ciências Biológicas, Universidade Federal Paraná, Curitiba, ParanáAndré Padial - Setor de Ciências Biológicas, Universidade Federal Paraná, Curitiba, ParanáAdriano Sanches Melo - Instituto de Biociências, Universidade Federal Rio Grande Sul (UFRGS), campus Vale, Porto Alegre, Rio Grande SulAdriano Sanches Melo - Instituto de Biociências, Universidade Federal Rio Grande Sul (UFRGS), campus Vale, Porto Alegre, Rio Grande SulBeatriz Milz - Programa de Pós-Graduação em Ciência Ambiental, Instituto de Energia e Ambiente, Universidade de São Paulo (PROCAM/IEE/USP), São Paulo, São PauloBeatriz Milz - Programa de Pós-Graduação em Ciência Ambiental, Instituto de Energia e Ambiente, Universidade de São Paulo (PROCAM/IEE/USP), São Paulo, São PauloFelipe Sodré Mendes Barros - Departamento de Geografia, Instituto Superior Antonio Ruiz de Montoya, Instituto Misionero de Biodiversidad - IMiBio, Ambiental Analytics, Misiones, ArgentinaFelipe Sodré Mendes Barros - Departamento de Geografia, Instituto Superior Antonio Ruiz de Montoya, Instituto Misionero de Biodiversidad - IMiBio, Ambiental Analytics, Misiones, ArgentinaIngrid da Silva Lima - Programa de Etnobiologia e Conservação da Natureza (PPGEtno), Universidade Federal Rural de Pernambuco (UFRPE), Recife, PernambucoIngrid da Silva Lima - Programa de Etnobiologia e Conservação da Natureza (PPGEtno), Universidade Federal Rural de Pernambuco (UFRPE), Recife, PernambucoMaurício Cetra - Departamento de Ciências Ambientais, Universidade Federal de São Carlos (UFSCar), campus Sorocaba, São PauloMaurício Cetra - Departamento de Ciências Ambientais, Universidade Federal de São Carlos (UFSCar), campus Sorocaba, São PauloMarcos Rafael Severgnini - Programa de Pós Graduação em Ecologia e Conservação, Universidade Federal de Mato Grosso Sul, Campo Grande, Mato Grosso SulMarcos Rafael Severgnini - Programa de Pós Graduação em Ecologia e Conservação, Universidade Federal de Mato Grosso Sul, Campo Grande, Mato Grosso SulMarcos Robalinho Lima - Departamento de Biologia Animal e Plantas, Universidade Estadual de Londrina, Londrina, ParanáMarcos Robalinho Lima - Departamento de Biologia Animal e Plantas, Universidade Estadual de Londrina, Londrina, ParanáMichel Varajão Garey - Instituto Latino Americano de Ciências da Vida e da Natureza (ILACVN), Universidade Federal da Integração Latino-Americana (UNILA), Foz Iguaçu, ParanáMichel Varajão Garey - Instituto Latino Americano de Ciências da Vida e da Natureza (ILACVN), Universidade Federal da Integração Latino-Americana (UNILA), Foz Iguaçu, ParanáPaulo Mateus Martins Sobrinho - Programa de Etnobiologia e Conservação da Natureza (PPGEtno), Universidade Federal Rural de Pernambuco (UFRPE), Recife, PernambucoPaulo Mateus Martins Sobrinho - Programa de Etnobiologia e Conservação da Natureza (PPGEtno), Universidade Federal Rural de Pernambuco (UFRPE), Recife, PernambucoPaulo Sérgio Monteiro Ferreira - Secretaria Estado da Educação da Paraíba, ParaíbaPaulo Sérgio Monteiro Ferreira - Secretaria Estado da Educação da Paraíba, ParaíbaPedro Henrique Albuquerque Sena - Centro de Pesquisas Ambientais Nordeste (Cepan), Recife, PernambucoPedro Henrique Albuquerque Sena - Centro de Pesquisas Ambientais Nordeste (Cepan), Recife, PernambucoReginaldo Augusto Farias de Gusmão - Programa de Etnobiologia e Conservação da Natureza (PPGEtno), Universidade Federal Rural de Pernambuco (UFRPE), Recife, PernambucoReginaldo Augusto Farias de Gusmão - Programa de Etnobiologia e Conservação da Natureza (PPGEtno), Universidade Federal Rural de Pernambuco (UFRPE), Recife, PernambucoVictor Satoru Saito - Departamento de Ciências Ambientais, Universidade Federal de São Carlos (UFSCar), campus São Carlos, São PauloVictor Satoru Saito - Departamento de Ciências Ambientais, Universidade Federal de São Carlos (UFSCar), campus São Carlos, São Paulo","code":""},{"path":"cap1.html","id":"cap1","chapter":"Capítulo 1 Introdução","heading":"Capítulo 1 Introdução","text":"","code":""},{"path":"cap1.html","id":"histórico-deste-livro","chapter":"Capítulo 1 Introdução","heading":"1.1 Histórico deste livro","text":"Este livro foi estruturado partir da apostila elaborada pelos pesquisadores Diogo B. Provete, Fernando R. da Silva e Thiago Gonçalves-Souza para ministrar o curso Estatística aplicada à ecologia usando o R PPG em Biologia Animal da UNESP de São José Rio Preto/SP, em abril de 2011. Os três pesquisadores eram então alunos PPG em Biologia Animal quando elaboraram o material disponibilizado na apostila Estatística aplicada à ecologia usando o R. proposta de transformar apostila em livro sempre foi um tópico recorrente desde 2011, e concretizado agora, um pouco mais de 10 anos depois.Neste período, Diogo, Fernando e Thiago foram contratados pela Universidade Federal de Mato Grosso Sul, Universidade Federal de São Carlos campus Sorocaba, e Universidade Federal Rural de Pernambuco, respectivamente. Nestes anos eles ofertaram diferentes versões curso Estatística aplicada à ecologia usando o R para alunos de graduação e pós-graduação em diferentes instituições Brasil. possibilidade da oferta destes novos cursos fortaleceu ideia de transformar apostila em um livro com base nas experiências dos pesquisadores em sala de aula.Considerando que novas abordagens ecológicas vêm sendo descritas e criadas uma taxa elevada nos últimos anos, era de se esperar que informações disponíveis na apostila estivessem defasadas após uma década. Por este motivo, Diogo, Fernando e Thiago convidaram outros dois pesquisadores, Gustavo B. Paterno da Georg-August-University Göttingen e Maurício H. Vancine PPG em Ecologia, Evolução e Biodiversidade da UNESP de Rio Claro/SP, que são referências uso de estatística em ecologia usando o R. Com o time completo, passaram mais de um ano realizando reuniões, compartilhando scripts e pagando cerveja para os coautores por capítulos atrasados até chegarem nesta primeira versão livro.","code":""},{"path":"cap1.html","id":"objetivo-deste-livro","chapter":"Capítulo 1 Introdução","heading":"1.2 Objetivo deste livro","text":"Nossa proposta com este livro é produzir um conteúdo que possa ser utilizado tanto por quem quer se aprofundar em análises comumente utilizadas em ecologia, quanto por quem não tem nenhuma ou poucas habilidades quantitativas. Para isso, traçamos o melhor caminho, pelo menos nosso ponto de vista, entre questões ecológicas e os métodos estatísticos mais robustos para testá-las. Guiar seus passos nesse caminho (nem sempre linear) necessita que você utilize um requisito básico: o de se esforçar para caminhar. O nosso esforço, em contrapartida, será o de indicar melhores direções para que você adquira certa independência em análises ecológicas. Um dos nossos objetivos é mostrar que o conhecimento de teorias ecológicas e formulação de questões apropriadas são o primeiro passo na caminhada rumo à compreensão da lógica estatística. Não deixe que estatística se torne “pedra seu caminho”. Em nossa opinião, programas com ambiente de programação favorecem o entendimento da lógica estatística, uma vez que cada passo (lembre-se de que você está caminhando em uma estrada desconhecida e cheia de pedras) precisa ser coordenado, ou seja, linhas de código (detalhes abaixo) precisam ser compreendidas para que você teste suas hipóteses. entanto, tome cuidado ao copiar deliberadamente scripts sem entender cada um dos passos da análise ou gráfico realizado.primeira parte deste livro pretende utilizar uma estratégia que facilita escolha teste estatístico apropriado, por meio da seleção de questões/hipóteses claras e da ligação dessas hipóteses com teoria e o método (veja Figura 2.1 Capítulo 2). Enfatizamos que é fundamental ter em mente aonde se quer chegar, para poder escolher o que deve ser feito. Posteriormente à escolha de suas questões, é necessário transferir o contexto ecológico para um contexto meramente estatístico (hipótese nula/alternativa). partir da definição de uma hipótese nula, partiremos para aplicação de cada teste estatístico (de modelos lineares generalizados análises multivariadas) utilizando linguagem R.Antes de detalhar cada análise estatística, apresentaremos o funcionamento básico da utilização da linguagem R e os tipos de distribuição estatística que são essenciais para compreensão dos testes estatísticos. Para isso, organizamos um esquema que chamamos de “estrutura lógica” que facilita compreensão dos passos necessários para testar suas hipóteses (veja Figura 2.1 Capítulo 2) (Gonçalves-Souza, Provete, et al. 2019).","code":""},{"path":"cap1.html","id":"o-que-você-não-encontrará-neste-livro","chapter":"Capítulo 1 Introdução","heading":"1.3 O que você não encontrará neste livro","text":"Aprofundamento teórico, detalhes matemáticos e explicação dos algoritmos são informações que infelizmente não serão abordadas neste livro. O foco aqui é explicação de como cada teste funciona (teoria e procedimentos matemáticos básicos) e sua aplicação em testes ecológicos usando scripts na linguagem R. Recomendamos aos (às) leitores () que consultem os livros indicados final deste capítulo caso desejem maior aprofundamento teórico e prático.","code":""},{"path":"cap1.html","id":"por-que-usar-o-r","chapter":"Capítulo 1 Introdução","heading":"1.4 Por que usar o R?","text":"Os criadores R o chamam de uma linguagem e ambiente de programação estatística e gráfica (Venables Ripley 2002). linguagem R também é chamada de programação “orientada ao objeto” (object oriented programming), o que significa que utilizar o R envolve basicamente criação e manipulação de objetos em um terminal, em que o usuário tem de dizer exatamente o que deseja que o programa execute, ao invés de simplesmente clicar em botões. E vem daí uma das grandes vantagens em se usar o R: o usuário tem total controle sobre o que está acontecendo e também tem de compreender o que deseja antes de executar uma análise. Além disso, o R permite integração com outros programas escritos em Fortran, C++, Python e Java, permitindo que os usuários possam aplicar novas metodologias sem ter que aprender novas linguagens.Na página pessoal Prof. Nicolas J. Gotelli existem vários conselhos para um estudante iniciante de ecologia. Dentre esses conselhos, o Prof. Gotelli menciona que o domínio de uma linguagem de programação é uma das habilidades mais importantes, porque dá liberdade ao ecólogo para executar tarefas que vão além daquelas disponíveis em pacotes estatísticos comerciais. Além disso, maioria das novas análises propostas nos mais reconhecidos periódicos em ecologia normalmente são implementadas na linguagem R, e os autores geralmente incluem o código fonte material suplementar dos artigos, tornando análise acessível e reprodutível. partir momento que essas análises ficam disponíveis (seja por código fornecido pelo autor ou por implementação em pacotes pré-existentes), é mais simples entendermos lógica de análises complexas, especialmente multivariadas, utilizando nossos próprios dados, realizando-passo passo. Sem utilização R, normalmente temos que contatar os autores que nem sempre são tão acessíveis.Especificamente em Ecologia, o uso da linguagem R para análise de dados cresceu enormemente nas duas últimas décadas. Em um artigo de revisão, Lai et al. (2019) analisaram mais de 60.000 artigos revisados por pares publicados em 30 periódicos de Ecologia durante um período de 10 anos. O número de estudos usando R aumentou linearmente de 11,4% em 2008 para 58,0% em 2017, e os 10 principais pacotes utilizados e ordenados por maior frequência de uso foram: lme4, vegan, nlme, ape, MuMIn, MASS, mgcv, ade4, multcomp e car. Os autores afirmam que crescente popularidade R promoveu ciência aberta na pesquisa ecológica, melhorando reprodutibilidade das análises e o fluxo de trabalho, principalmente quando scripts e códigos foram incluídos e compartilhados nos artigos. Eles finalizam dizendo que partir dos resultados encontrados, linguagem R é um componente significativo das análises campo da Ecologia.Uma última vantagem é que por ser um software livre, citação R em artigos é permitida e até aconselhável. Para saber como citar o R, digite citation() na linha de comando. Para citar um pacote específico, digite citation() com o nome pacote entre aspas dentro dos parênteses. Mais detalhes sobre citações podem ser vistos Capítulo 4. Neste ponto, esperamos ter convencido você leitor(), de que aprender utilizar o R tem inúmeras vantagens. Entretanto, provavelmente vai ser difícil começo, mas continue e perceberá que o investimento vai valer à pena futuro.","code":""},{"path":"cap1.html","id":"indo-além-da-linguagem-de-programação-para-a-ecologia","chapter":"Capítulo 1 Introdução","heading":"1.5 Indo além da linguagem de programação para a Ecologia","text":"Um ponto em comum em que todos os autores deste livro concordaram em conversas durante sua estruturação, foi dificuldade que todos tivemos quando estávamos aprendendo linguagem:Como transcrever os objetivos (manipulação de dados, análises e gráficos) em linguagem RComo interpretar os resultados das análises estatísticas R para os objetivos ecológicosNum primeiro momento, quando estamos aprendendo linguagem R é muito desafiador pensar em como estruturar nossos códigos para que eles façam o que precisamos: importar dados, selecionar linhas ou colunas, qual pacote ou função usar para uma certa análise ou como fazer um gráfico que nas nossas anotações são simples, mas código parece impossível. Bem, não há um caminho fácil nesse sentido e ele depende muito da experiência e familiaridade adquirida com o tempo de uso da linguagem, assim como outra língua qualquer, como inglês ou espanhol. Entretanto, uma dica pode ajudar: estruture seus códigos antes de partir para o R. Por exemplo, escreva um papel os pontos que deseja executar em seus códigos, como se estivesse explicando para alguém os passos que precisa para realizar tarefas. Depois disso, transcreva para o script (arquivo onde os códigos são escritos, mas não se preocupe, iremos explicar esse conceito Capítulo 4) esses pontos em formato de texto. Por fim, traduza isso em linguagem R. Pode parecer maçante e cansativo começo, mas isso o ajudará ter maior domínio da linguagem, sendo que esse passo se tornará desnecessário quando se adquire bastante experiência.Uma vez que esta barreira inicial foi transposta e você conseguiu obter os primeiros resultados de suas análises com valores de estatísticas, parâmetros estimados, valores de p e R², gráficos, e etc., como interpretamos à luz da teoria ecológica? Esse ponto é talvez um dos mais complicados. Com o tempo, ter um valor final de uma estatística ou gráfico à partir da linguagem R é relativamente simples, mas o que esse valor ou gráfico significam para nossa hipótese ecológica é o ponto mais complexo. Essa dificuldade por ser por inexperiência teórica (ainda não lemos muito sobre um aspecto ecológico) ou inexperiência científica (ainda temos dificuldade para expandir nossos argumentos de forma indutiva). Destacamos esse ponto porque ele é fundamental processo científico e talvez seja o principal aspecto que diferencia os cientistas de outros profissionais: sua capacidade de entendimento dos padrões à partir dos processos e mecanismos atrelados. Nesse ponto, quase sempre recorremos aos nossos orientadores ou colegas mais experientes para nos ajudar, mas é natural e faz parte processo de aprendizado de uso da linguagem R junto à Ecologia como Ciência. Entretanto, contrapomos importância dessa extrapolação para não nos tornarmos apenas especialistas em linguagem R sem fundamental capacidade de entendimento sistema ecológico que estamos estudando.","code":""},{"path":"cap1.html","id":"como-usar-este-livro","chapter":"Capítulo 1 Introdução","heading":"1.6 Como usar este livro","text":"Os conteúdos apresentados em cada capítulo são independentes entre si. Portanto, você pode utilizar este livro de duas formas. primeira é seguir uma ordem sequencial (capítulos 1, 2, 3, …) que recomendamos, principalmente, para pessoas que não possuem familiaridade com linguagem R. segunda forma, é selecionar o capítulo que contém análise de seu interesse e mudar de um capítulo para outro sem seguir sequência apresentada livro.Com exceção dos capítulos 2, 3, 4, 5, 6 e 15, os outros capítulos foram elaborados seguindo mesma estrutura, contendo uma descrição da análise estatística (aspectos teóricos) e exemplos relacionados com questões ecológicas que podem ser respondidas por esta análise e exercícios. Todos os exemplos são compostos por: ) uma descrição dos dados utilizados, ii) pergunta e predição trabalho, iii) descrição das variáveis resposta(s) e preditora(s), e iv) descrição e explicação das linhas de código R necessárias para realização das análises. maioria dos exemplos utilizados são baseados em dados reais que já foram publicados em artigos científicos ou são dados coletados por um dos autores deste livro. Nós recomendamos que primeiro você utilize estes exemplos para se familiarizar com análises e formatação das linhas e colunas das planilhas. Em seguida, faça os exercícios propostos final de cada capítulo, e por fim, utilize seus próprios dados para realizar análises. Esta é melhor maneira de se familiarizar com linhas de código R. 📝 Importante \nMuitas das métricas ou índices apresentados neste livro não foram traduzidas para o português, porque seus acrônimos são clássicos e bem estabelecidos na literatura ecológica. Nestes casos, consideramos que tradução poderia confundir pessoas que estão começando se familiarizar com literatura específica. Além disso, optamos por manter versão padrão em alguns gráficos utilizados nos Capítulos 7 ao 15, principalmente aqueles gráficos que são “output” de análises como, por exemplo, visualização de normalidade de resíduos, homogeneidade de variâncias, entre outros. Em geral, esses gráficos são usados processo de decisão de algum passo da análise e não possuem qualidade de publicação. Como o usuário vai obter o mesmo gráfico quando replicar análises propostas aqui ou suas próprias análises, julgamos ser mais didático manter versão original, em inglês.Realçamos que não estamos abordando todas possibilidades disponíveis, e existem muitos outros pacotes e funções R que realizam mesmas análises. Contudo, esperamos que o conteúdo apresentado permita que os() leitores() adquiram independência e segurança para que possam caminhar sozinhos() na exploração de novos pacotes e funções para responderem suas perguntas biológicas e ecológicas.","code":""},{"path":"cap1.html","id":"como-ensinar-e-aprender-com-esse-livro","chapter":"Capítulo 1 Introdução","heading":"1.7 Como ensinar e aprender com esse livro","text":"Uma forma bastante interessante de aprender ou aprofundar seu conhecimento sobre um tema é partir de grupos de estudo. Aproveitando dinâmicas de estudos que os próprios autores fizeram em seus laboratórios (seja como discente ou professor), sugerimos abaixo alguns formatos que podem ser usados por um grupo de discentes (sem presença de um orientador) ou pelo laboratório. É importante ressaltar que esses formatos não são os únicos que podem ser testados. O leitor pode juntar ideias de diferentes propostas ou mesmo usar parte das propostas e inserir suas próprias ideias, tendo como base características grupo que irá se reunir.","code":""},{"path":"cap1.html","id":"em-laboratórios-ou-grupos-de-pesquisa","chapter":"Capítulo 1 Introdução","heading":"1.7.1 Em laboratórios ou grupos de pesquisa","text":"Líder aleatórioCada capítulo é sorteado para um integrante grupo que ficará responsável por estudar, apresentar e enviar outros materiais que julgar necessário. Neste formato, existem duas possibilidades interessantes. primeira é de um grupo de estudantes que é iniciante em determinado tema (e.g., análise multivariada) e, desse modo, todos integrantes serão estimulados participarem processo de ensino e de aprendizagem. O segundo ponto interessante é para grupos heterogêneos onde pessoas diferentes possuem domínio de diferentes ferramentas. Neste caso, é importante que mesmo que determinado integrante seja especialista na análise X, ele poderá aleatoriamente ter que aprender e ensinar análise Y. Como resultado, espera-se que os grupos de estudo neste formato tenham ampla discussão, uma vez que integrantes com baixo, médio ou alto conhecimento em determinada análise serão tanto professores como aprendizes.Líder especialista (discentes como líderes)Cada capítulo é liderado pelo “maior especialista” naquele determinado assunto, que ficará responsável por organizar toda dinâmica grupo. O ideal é que especialistas distintos liderem discussão de diferentes capítulos, para que todos os membros grupo sejam líderes em mínimo um capítulo.Líder especialista II (orientador ou pós-doc)O orientador (ou pós-doc ou ambos) selecionam os capítulos sobre o assunto de interesse (ou todos os capítulos livro) e se reúne regularmente para discussão com discentes. Além da leitura dos capítulos, o líder pode enviar atividades extras ou desafios para estimular que os discentes leiam o conteúdo e também executem comandos R. Por exemplo, em cada capítulo, o desafio pode ser criar hipóteses sobre um tema de estudo, gerar dados fictícios (ou usar dados reais disponíveis) e analisar os dados com determinado teste estatístico R.Líder especialista ou II integrando com teoria (específico para capítulos 8 15)Esta proposta expande o Líder especialista uma vez que não estará focada somente na análise estatística apresentada livro. Além de inserir este componente analítico, o líder irá apresentar o arcabouço teórico ecológico que é geralmente usado em estudos que utilizaram determinada análise. Por exemplo, o capítulo que apresenta regressão linear seria combinado com teoria de biogeografia de ilhas para entender relação espécies-área. Desse modo, não seria apresentado somente especificidades da regressão linear, mas também teoria de biogeografia de ilhas.","code":""},{"path":"cap1.html","id":"em-disciplinas-da-graduação-ou-pós-graduação","chapter":"Capítulo 1 Introdução","heading":"1.7.2 Em disciplinas da graduação ou pós-graduação","text":"Atividade em grupo em sala invertidaO professor pode sortear diferentes grupos que ficarão responsáveis por cada capítulo livro (depender conteúdo da disciplina). Cada componente grupo pode ficar responsável por diferentes partes capítulo. Por exemplo, se disciplina de gráficos, um discente pode discutir estrutura das funções pacote ggplot2, outro discente pode apresentar conexão entre tipos de variáveis e gráficos, enquanto um terceiro discente se responsabiliza por executar os comandos dos gráficos R. atividades devem ser realizadas e apresentadas antes da aula teórica/prática sobre aquela temática, e o docente ficará responsável por mediar apresentações e discussões.Sala convencionalO professor pode usar o livro como material didático seguindo o conteúdo de acordo com disciplina em questão, seja ela da linguagem R, de análises univariadas, multivariadas ou espaciais. Além disso, o professor pode fornecer dados para os discentes (ou estimular que discentes usem os próprios dados) e replicar gráficos e análises usando os scripts fornecidos livro.","code":""},{"path":"cap1.html","id":"livros-que-recomendamos-para-aprofundamento-teórico","chapter":"Capítulo 1 Introdução","heading":"1.8 Livros que recomendamos para aprofundamento teórico","text":"Hands-Programming R (Grolemund 2014): Esse livro é para quem quer se aprofundar e aprender programar em R, com exemplos práticos. Nas palavras autor, ele disse que escreveu o livro para não programadores, com o intuito de fornecer uma introdução amigável à linguagem R. Nele, é apresentado como carregar dados, montar e desmontar objetos de dados, navegar sistema de ambiente R, escrever funções e usar ferramentas de programação R para solução de problemas práticos de ciência de dados. O livro está disponível nesse link.Hands-Programming R (Grolemund 2014): Esse livro é para quem quer se aprofundar e aprender programar em R, com exemplos práticos. Nas palavras autor, ele disse que escreveu o livro para não programadores, com o intuito de fornecer uma introdução amigável à linguagem R. Nele, é apresentado como carregar dados, montar e desmontar objetos de dados, navegar sistema de ambiente R, escrever funções e usar ferramentas de programação R para solução de problemas práticos de ciência de dados. O livro está disponível nesse link.R Data Science: Import, Tidy, Transform, Visualize, Model Data (Wickham & Grolemund 2017): Também conhecido como R4DS, esse livro é uma das primeiras referências sobre tidyverse e de Ciência de Dados R. O livro aborda principais etapas de importação, conversão, exploração e modelagem de dados e comunicação dos resultados. Ele apresenta uma compreensão ciclo da ciência de dados, juntamente com ferramentas básicas necessárias para gerenciar os detalhes sobre cada etapa ciclo. Cada seção livro é combinada com exercícios para ajudar na fixação conteúdo. Os principais tópicos são: ) transformar conjuntos de dados em um formato conveniente para análise, ii) programação com ferramentas poderosas R para resolver problemas de dados com maior clareza e facilidade, iii) examinar os dados, gerar hipóteses e testá-las rapidamente, iv) gerar modelos que forneçam um resumo dos dados e que capture “sinais” conjunto de dados, e v) aprendar R Markdown para integrar texto, código e resultados. O livro está disponível nesse link.R Data Science: Import, Tidy, Transform, Visualize, Model Data (Wickham & Grolemund 2017): Também conhecido como R4DS, esse livro é uma das primeiras referências sobre tidyverse e de Ciência de Dados R. O livro aborda principais etapas de importação, conversão, exploração e modelagem de dados e comunicação dos resultados. Ele apresenta uma compreensão ciclo da ciência de dados, juntamente com ferramentas básicas necessárias para gerenciar os detalhes sobre cada etapa ciclo. Cada seção livro é combinada com exercícios para ajudar na fixação conteúdo. Os principais tópicos são: ) transformar conjuntos de dados em um formato conveniente para análise, ii) programação com ferramentas poderosas R para resolver problemas de dados com maior clareza e facilidade, iii) examinar os dados, gerar hipóteses e testá-las rapidamente, iv) gerar modelos que forneçam um resumo dos dados e que capture “sinais” conjunto de dados, e v) aprendar R Markdown para integrar texto, código e resultados. O livro está disponível nesse link.primer ecological statistics, 2a edição (Gotelli & Ellison 2012): Este livro traz um apanhado geral sobre desenhos amostrais voltados para experimentação e uma introdução à estatística multivariada. Existe uma tradução para o português da primeira edição, chamada “Princípios de Estatística em Ecologia” que saiu pela ed ArtMed em 2010. Este é uma excelente referência para quem quer começar estudar estatística básica, especialmente com aplicações em ecologia.primer ecological statistics, 2a edição (Gotelli & Ellison 2012): Este livro traz um apanhado geral sobre desenhos amostrais voltados para experimentação e uma introdução à estatística multivariada. Existe uma tradução para o português da primeira edição, chamada “Princípios de Estatística em Ecologia” que saiu pela ed ArtMed em 2010. Este é uma excelente referência para quem quer começar estudar estatística básica, especialmente com aplicações em ecologia.Experimental Design Data Analysis Biologists (Quinn & Keough 2002): Outro excelente livro introdutório sobre estatística com exemplos práticos para ecologia e um dos preferidos dos autores deste livro aqui. Ele aborda os modelos lineares mais comuns vistos em disciplinas de bioestatística, tais como regressão e ANOVA, mas também traz uma boa introdução sobre GLMs e métodos mais modernos de análise de dados. Mas o mais importante, lógica de ensino dos métodos segue muito o que preconizamos neste livro e não podemos recomendá-lo o bastante para quem está começando estudar estatística.Experimental Design Data Analysis Biologists (Quinn & Keough 2002): Outro excelente livro introdutório sobre estatística com exemplos práticos para ecologia e um dos preferidos dos autores deste livro aqui. Ele aborda os modelos lineares mais comuns vistos em disciplinas de bioestatística, tais como regressão e ANOVA, mas também traz uma boa introdução sobre GLMs e métodos mais modernos de análise de dados. Mas o mais importante, lógica de ensino dos métodos segue muito o que preconizamos neste livro e não podemos recomendá-lo o bastante para quem está começando estudar estatística.R book, 2a edição (Crawley 2012): Livro que vai básico ao avançado, tem informações sobre linguagem R, estatística univariada, multivariada e modelagem. Relativamente fácil de compreender. Capítulos trazem funções para criação e manipulação de gráficos passo--passo.R book, 2a edição (Crawley 2012): Livro que vai básico ao avançado, tem informações sobre linguagem R, estatística univariada, multivariada e modelagem. Relativamente fácil de compreender. Capítulos trazem funções para criação e manipulação de gráficos passo--passo.Numerical ecology, 3a edição (Legendre & Legendre 2012): Este é o manual teórico essencial e leitura obrigatória para entender mais fundo qualquer análise multivariada. Esta nova edição traz um capítulo novo sobre análises multiescalares em ecologia de comunidade, com exemplos de aplicação de Moran Eigenvector Maps (MEMs).Numerical ecology, 3a edição (Legendre & Legendre 2012): Este é o manual teórico essencial e leitura obrigatória para entender mais fundo qualquer análise multivariada. Esta nova edição traz um capítulo novo sobre análises multiescalares em ecologia de comunidade, com exemplos de aplicação de Moran Eigenvector Maps (MEMs).Biological Diversity: Frontiers Measurement Assessment (Magurran & McGill 2012): Livro editado com vários capítulos sobre medidas tradicionais e alternativas de biodiversidade. Também atualiza medidas de estimativa de diversidade, uma revisão sobre diversidade funcional e filogenética. Esse é uma boa porta de entrada para entender os aspectos teóricos e meandros da análise de dados de biodiversidade.Biological Diversity: Frontiers Measurement Assessment (Magurran & McGill 2012): Livro editado com vários capítulos sobre medidas tradicionais e alternativas de biodiversidade. Também atualiza medidas de estimativa de diversidade, uma revisão sobre diversidade funcional e filogenética. Esse é uma boa porta de entrada para entender os aspectos teóricos e meandros da análise de dados de biodiversidade.Mixed effects models extensions ecology R (Zuur et al. 2009): Este continua sendo melhor introdução para modelos lineares generalizados (e de efeito misto), modelos de mínimos quadrados generalizados, Modelos Aditivos Generalizados para biólogos e ecólogos. O livro contém vários capítulos em que o funcionamento dos modelos é explicado de maneira bastante atraente, mantendo matemática mínimo. Todos os exemplos são com dados reais produzidor por ecólogos. Este é um bom livro intermediário para quem quer se aprofundar nas análises mais modernas feitas em ecologia.Mixed effects models extensions ecology R (Zuur et al. 2009): Este continua sendo melhor introdução para modelos lineares generalizados (e de efeito misto), modelos de mínimos quadrados generalizados, Modelos Aditivos Generalizados para biólogos e ecólogos. O livro contém vários capítulos em que o funcionamento dos modelos é explicado de maneira bastante atraente, mantendo matemática mínimo. Todos os exemplos são com dados reais produzidor por ecólogos. Este é um bom livro intermediário para quem quer se aprofundar nas análises mais modernas feitas em ecologia.Geocomputation R (Lovelace et al. 2020): Esse livro tornou-se rapidamente principal referência sobre manipulação, visualização, análise e modelagem de dados geoespaciais R. O livro é dividido em três partes: ) fundamentos, ii) extensões e iii) aplicações. parte um é voltada para fundamentação dos dados geográficos R, descrevendo natureza dos conjuntos de dados espaciais e métodos para manipulá-los, assim como importação/exportação de dados geográficos e transformação de sistemas de referência de coordenadas. Parte II representa métodos que se baseiam nessas fundações, abrange criação de mapas avançados (incluindo mapeamento da web), “pontes” para GIS, compartilhamento de código reproduzível e como fazer validação cruzada na presença de autocorrelação espacial. Parte III aplica o conhecimento adquirido para resolver problemas mundo real, incluindo representação e modelagem de sistemas de transporte, localização ideal para lojas ou serviços e modelagem ecológica. Os exercícios final de cada capítulo fornecem habilidades necessárias para lidar com uma série de problemas geoespaciais. soluções para cada capítulo e materiais complementares estão disponíveis nesse link e o livro nesse link.Geocomputation R (Lovelace et al. 2020): Esse livro tornou-se rapidamente principal referência sobre manipulação, visualização, análise e modelagem de dados geoespaciais R. O livro é dividido em três partes: ) fundamentos, ii) extensões e iii) aplicações. parte um é voltada para fundamentação dos dados geográficos R, descrevendo natureza dos conjuntos de dados espaciais e métodos para manipulá-los, assim como importação/exportação de dados geográficos e transformação de sistemas de referência de coordenadas. Parte II representa métodos que se baseiam nessas fundações, abrange criação de mapas avançados (incluindo mapeamento da web), “pontes” para GIS, compartilhamento de código reproduzível e como fazer validação cruzada na presença de autocorrelação espacial. Parte III aplica o conhecimento adquirido para resolver problemas mundo real, incluindo representação e modelagem de sistemas de transporte, localização ideal para lojas ou serviços e modelagem ecológica. Os exercícios final de cada capítulo fornecem habilidades necessárias para lidar com uma série de problemas geoespaciais. soluções para cada capítulo e materiais complementares estão disponíveis nesse link e o livro nesse link.Recomendamos ainda para o amadurecimento em análises ecológicas seguintes leituras: Manly (1991), Pinheiro Bates (2000), Scheiner Gurevitch (2001), K. P. Burnham Anderson (2014), Venables Ripley (2002), Zar (2010), . F. Zuur, Ieno, Smith (2007), G. James et al. (2013), Fox, Negrete-Yankelevich, Sosa (2015), Thioulouse et al. (2018) e Touchon (2021).","code":""},{"path":"cap2.html","id":"cap2","chapter":"Capítulo 2 Voltando ao básico: como dominar a arte de fazer perguntas cientificamente relevantes","heading":"Capítulo 2 Voltando ao básico: como dominar a arte de fazer perguntas cientificamente relevantes","text":"Capítulo originalmente publicado por Gonçalves-Souza, Provete, Garey, da Silva & Albuquerque (2019), Methods Techniques Ethnobiology Ethnoecology (publicação autorizada por Springer, licença 5230220198680).","code":""},{"path":"cap2.html","id":"introdução","chapter":"Capítulo 2 Voltando ao básico: como dominar a arte de fazer perguntas cientificamente relevantes","heading":"2.1 Introdução","text":"Aquele que ama prática sem teoria é como um marinheiro que embarca em um barco sem um leme e uma bússola e nunca sabe onde pode atracar - Leonardo da Vinci.Qual é sua pergunta? Talvez esta seja frase que pesquisadores mais jovens ouvem quando começam suas atividades científicas. Apesar de aparentemente simples, responder esta pergunta se torna um dos maiores desafios da formação científica. Seja na pesquisa quantitativa ou qualitativa, todo processo de busca de conhecimento parte de uma questão/problema formulada pelo pesquisador início desse processo. Esta questão guiará o pesquisador em todas etapas da pesquisa. caso específico de pesquisa quantitativa, questão é porta de entrada de uma das formas mais poderosas de pensar cientificamente: o método hipotético-dedutivo (MHD) definido por Karl Popper (1959). Este capítulo propõe uma maneira de pensar sobre hipóteses (geradas dentro MHD) para melhorar o pensamento estatístico usando um fluxograma que relaciona variáveis por ligações causais. Além disso, argumentamos que você pode facilmente usar fluxogramas para: ) identificar variáveis relevantes e como elas afetam umas às outras; ii) melhorar (quando necessário) o desenho experimental/observacional; iii) facilitar escolha de análises estatísticas; e iv) melhorar interpretação e comunicação dos dados e análises.","code":""},{"path":"cap2.html","id":"perguntas-devem-preceder-as-análises-estatísticas","chapter":"Capítulo 2 Voltando ao básico: como dominar a arte de fazer perguntas cientificamente relevantes","heading":"2.2 Perguntas devem preceder as análises estatísticas","text":"","code":""},{"path":"cap2.html","id":"um-bestiárioperguntas_ecologia-1-para-o-teste-de-hipóteses-você-está-fazendo-a-pergunta-certa","chapter":"Capítulo 2 Voltando ao básico: como dominar a arte de fazer perguntas cientificamente relevantes","heading":"2.2.1 Um bestiário1 para o teste de hipóteses (Você está fazendo a pergunta certa?)","text":"maioria dos alunos e professores de ciências biológicas possuem aversão à palavra “estatística”. Não surpreendentemente, enquanto maioria das disciplinas acadêmicas que compõem o “STEM” (termo em inglês para aglomerar Ciência, Tecnologia, Engenharia e Matemática) têm uma sólida formação estatística durante graduação, cursos de ciências biológicas têm um currículo fraco ao integrar o pensamento estatístico dentro de um contexto biológico (Metz 2008). Esses cursos têm sido frequentemente ministrados sem qualquer abordagem prática para integrar os alunos em uma plataforma de solução de problemas (Horgan et al. 1999). Infelizmente, Etnobiologia, Ecologia e Conservação (daqui em diante EEC) não são exceções. Talvez mais importante, uma grande preocupação durante o treinamento estatístico de estudantes de EEC é necessidade de trabalhar com problemas complexos e multidimensionais que exigem soluções analíticas ainda mais complicadas para um público sem experiência em estatística e matemática. Por este motivo, muitos pesquisadores consideram estatística como parte mais problemática de sua pesquisa científica. Argumentamos neste capítulo que dificuldade de usar estatística em EEC está associada à ausência de uma plataforma de solução de problemas gerando hipóteses claras que são derivadas de uma teoria. entanto, concordamos que há um grande desafio em algumas disciplinas como Etnobiologia para integrar esta abordagem direcionada por hipóteses, uma vez que foi introduzida apenas recentemente (veja Phillips Gentry (1993); Albuquerque Hanazaki (2009)). Devido à falta de uma plataforma de solução de problemas, frequentemente percebemos que alunos/pesquisadores na EEC geralmente têm dificuldades de responder perguntas básicas para uma pesquisa científica, tais como:Qual é principal teoria ou raciocínio lógico seu estudo?Qual é questão principal seu estudo?Qual é sua hipótese? Quais são suas predições?Qual é unidade amostral, variável independente e dependente seu trabalho? Existe alguma covariável?Qual é o grupo controle?Como selecionar qualquer teste estatístico sem responder essas cinco perguntas? estrutura estatística frequentista fornece uma maneira de ir progressivamente suportando ou falseando uma hipótese (Neyman Pearson 1933; Popper 1959). decisão de rejeitar uma hipótese nula é feita usando um valor de probabilidade (geralmente P < 0,05) calculado pela comparação de eventos observados com observações repetidas obtidas partir de uma distribuição nula.Agora, vamos ensinar através de um exemplo e apresentar um “guia para o pensamento estatístico” que conecta alguns elementos essenciais para executar qualquer análise multivariada (ou univariada) (Figura 2.1). Primeiro, imagine que você observou os seguintes fenômenos na natureza: ) “indivíduos de uma população tradicional selecionar algumas plantas para fins médicos” e ii) “manchas monodominantes da árvore Prosopis juliflora, uma espécie invasora em várias regiões”. lado da etnobiologia, para entender como e porque o conhecimento tradicional é construído, existe uma teoria ou hipótese (por exemplo, hipótese de aparência: Gonçalves, Albuquerque, Medeiros (2016)) explicando os principais processos que ditam seleção da planta (Figura 2.1a). Então, você pode fazer uma ou mais perguntas relacionadas àquele fenômeno observado (Figura 2.1b). Por exemplo, como urbanização afeta o conhecimento das pessoas sobre o uso de plantas medicinais em diferentes biomas? lado ecológico/conservação, para entender por que espécies introduzidas afetam espécies nativas locais, você precisa entender teorias nicho ecológico e evolutiva (MacDougall, Gilbert, Levine 2009; Saul Jeschke 2015). Você pode perguntar, por exemplo, como plantas exóticas afetam estrutura de comunidades de plantas nativas? Questões complexas ou vagas dificultam construção fluxograma de pesquisa (ver descrição abaixo) e seleção de testes estatísticos. Em vez disso, uma pergunta útil deve indicar variáveis relevantes seu estudo, como independentes e dependentes, covariáveis, unidade amostral e escala espacial de interesse (Figura 2.1b). exemplo etnobiológico fornecido, urbanização e o conhecimento das pessoas são variáveis independentes e dependentes, respectivamente. Além disso, este estudo tem uma escala ampla, pois compara biomas diferentes. próxima etapa é construir hipótese biológica (Figura 2.1c), que indicará associação entre variáveis independentes e dependentes. exemplo etnobiológico, hipótese é que ) “urbanização afeta o conhecimento das pessoas sobre o uso de plantas medicinais”, enquanto hipótese ecológica é que ii) “espécies exóticas afetam estrutura de comunidades de plantas nativas”. Observe que isso é muito semelhante à questão principal. Mas você pode ter múltiplas hipóteses (Platt 1964) derivado de uma teoria. Depois de selecionar hipótese biológica (ou científica), é hora de pensar sobre derivação lógica da hipótese, que é chamada de predição ou previsão (Figura 2.1d). Os padrões preditos são uma etapa muito importante, pois após defini-los você pode operacionalizar suas variáveis e visualizar seus dados. Por exemplo, variável teórica “Urbanização” pode ser medida como “grau de urbanização ao longo das áreas urbanas, periurbanas e rurais” e “conhecimento das pessoas” como “o número e tipo de espécies de plantas úteis usadas para diferentes doenças”. Assim, predição é que o grau de urbanização diminua o número e tipo de espécies de plantas conhecidas utilizadas para fins medicinais. exemplo ecológico, variável “espécies exóticas” pode ser medida como “densidade da planta exótica Prosopis juliflora” e “Estrutura da comunidade” como “riqueza e composição de espécies nativas”. Depois de operacionalizar o seu trabalho à luz método hipotético-dedutivo (HDM), o próximo passo é “pensar estatisticamente” sobre hipótese biológica formulada (ver Figura 2.1e, f).\nFigura 2.1: Um guia para o pensamento estatístico combinando o método hipotético-dedutivo (– d, ) e estatística frequentista (e – ). Veja também Figura 1 em Underwood (1997), Figura 1 em Ford (2000) e Figura 1.3 em Legendre & Legendre (2012).\nEntão, você precisa definir hipóteses estatísticas nula (H0) e alternativa (H1). Duas “hipóteses estatísticas” diferentes podem ser derivadas de uma hipótese biológica (Figura 2.1e). Portanto, nós usamos o termo “hipótese estatística” entre aspas, porque chamadas hipóteses estatísticas são predições sensu stricto, e muitas vezes confundem jovens estudantes. hipótese estatística nula representa uma ausência de relação entre variáveis independentes e dependentes. Depois de definir hipótese estatística nula, você pode derivar uma ou várias hipóteses estatísticas alternativas, que demonstram (s) associação(ões) esperada(s) entre suas variáveis (Figura 2.1e). Em nosso exemplo, hipótese nula é que “o grau de urbanização não afeta o número de espécies de plantas úteis conhecidas pela população local”. Por sua vez, hipótese alternativa é que “o grau de urbanização afeta o número de espécies de plantas úteis conhecidas pela população local”. Depois de operacionalizar suas variáveis e definir o valor nulo e hipóteses alternativas, é hora de visualizar o resultado esperado (Figura 2.2, Caixa 1) e escolher um método estatístico adequado. Por exemplo, se você deseja comparar diferença na composição de plantas úteis entre áreas urbanas, periurbanas e rurais, você pode executar uma PERMANOVA (Gonçalves-Souza, Garey, et al. 2019) que usa uma estatística de teste chamada pseudo-F. Então, você deve escolher o limite de probabilidade (o valor P) teste estatístico para decidir se hipótese nula deve ou não deve ser rejeitada (Gotelli Ellison 2012). Se você encontrar um P < 0,05, você deve rejeitar hipótese estatística nula (urbanização não afeta o número e composição das plantas). Por outro lado, um P > 0,05 indica que você não pode rejeitar hipótese nula estatística. Assim, estatística teste e o valor P representam última parte teste de hipótese estatística, que é decisão e conclusões apropriadas que serão usadas para retroalimentar teoria principal (Figura 2.1g – ). Generalizando seus resultados e falseando (ou não) suas hipóteses, os estudos buscam refinar construção conceitual da teoria, que muda constantemente (Figura 1i, Ford 2000). entanto, há um ponto crítico nesta última frase, porque significância estatística não significa necessariamente relevância biológica (ver discussão em Gotelli e Ellison (2012) e Martínez-Abraín (2008)). Nas palavras de Ford (2000): “estatísticas são usadas para iluminar o problema, e não para apoiar uma posição”. Além disso, o procedimento de teste de hipótese tem alguma incerteza, que pode influenciar resultados “falso-positivos” (erro tipo 1) e “falso-negativos” (erro tipo 2) (Whitlock Schluter 2015). Para simplificar, não discutiremos em detalhes os prós e contras da estatística frequentista, bem como métodos alternativos (por exemplo, Bayesiano e Máxima Verossimilhança), e questões filosóficas relativas ao “valor P” (para uma discussão sobre esses tópicos, consulte o fórum em Ellison et al. (2014)).Caixa 1. Tipo de variáveis e visualização de dados. Conforme descrito na Seção 2.3, o fluxograma é essencial para conectar variáveis relevantes para pesquisa. Para aproveitar ao máximo esta abordagem, você pode desenhar suas próprias predições gráficas para te ajudar pensar sobre diferentes possibilidades analíticas. Aqui, nós fornecemos uma descrição completa dos tipos de variáveis que você deve saber antes de executar qualquer análise estatística e representar seus resultados. Além disso, mostramos uma breve galeria (Figura 2.2) com exemplos de boas práticas em visualização de dados (Figura 2.3b, veja também figuras em Gonçalves-Souza, Garey, et al. (2019)). Além de conectar diferentes variáveis fluxograma, você deve distinguir o tipo de variável. Primeiro você deve identificar variáveis independentes (também conhecidos como explicativas ou preditoras) e dependentes (também conhecidas como resposta). variável independente é aquela (ou aquelas) que prevê ou afeta variável resposta (por exemplo, fertilidade solo é variável independente capaz de afetar abundância de uma espécie de planta focal, variável dependente). Além disso, uma covariável é uma variável contínua que pode afetar tanto variável resposta quanto independente (ou ambos), mas geralmente não é interesse pesquisador. Depois de definir variáveis relevantes, conectando-fluxograma, é hora de diferenciar seu tipo: ) quantitativa ou contínua, e ii) categórica ou qualitativa (Figura 2.2, Caixa 1). O tipo de variável irá definir que tipo de figura você pode selecionar. Por exemplo, se você está comparando duas variáveis contínuas ou uma variável contínua e uma binária, melhor maneira de visualizá-los (Figura 2.2) é um gráfico de dispersão (Figura 2.2). linha representa os valores preditos pelo modelo estatístico usado (por exemplo, linear ou logístico). Se você está interessado em comparar gama de diferentes atributos (ou descrição de qualquer variável numérica) entre variáveis categóricas (por exemplo, espécies ou populações locais), um gráfico de halteres (inglês Dumbbell plot) é uma boa opção (Figura 2.2). Histogramas também podem ser usados para mostrar distribuição de duas variáveis contínuas de dois grupos ou fatores (Figura 2.2). entanto, se você quiser testar o efeito de uma variável categórica independente (como em um desenho de ANOVA) sobre uma variável dependente, boxplots (Figura 2.2) ou gráficos de violino podem resumir essas relações de maneira elegante. Conjuntos de dados multivariados, por sua vez, podem ser visualizados com ordenação (Figura 2.2) ou gráficos de agrupamento (não mostrados). Existe um site abrangente apresentando várias maneiras de visualizar dados chamado datavizproject.\nFigura 2.2: () Tipos de variáveis e (B) visualização de dados para representar relação entre variáveis independentes e dependentes ou covariáveis.\n","code":""},{"path":"cap2.html","id":"fluxograma-conectando-variáveis-para-melhorar-o-desenho-experimental-e-as-análises-estatísticas","chapter":"Capítulo 2 Voltando ao básico: como dominar a arte de fazer perguntas cientificamente relevantes","heading":"2.3 Fluxograma: Conectando variáveis para melhorar o desenho experimental e as análises estatísticas","text":"McIntosh e Pontius (2017) afirmaram que o pensamento estatístico representado na Figura 2.1 inclui quatro etapas importantes: ) quais perguntas você investigaria (Seção 2.4), ii) como e onde coletar os dados (Ruxton Colegrave 2016), iii) quais fatores devem ser considerados e como eles afetam suas variáveis de interesse (e como elas afetam umas às outras), e iv) qual análise estatística você deve usar e como interpretar e comunicar os resultados (Seção 2.4). entanto, etapa (3) deve ser feita antes de coletar os dados. Por exemplo, se você está interessado na investigação dos benefícios das matas ciliares para espécies nativas de peixes, quais variáveis devem ser incluídas estudo? Se você escolher rios com e sem mata ciliar como única variável preditora, seu projeto de amostragem irá omitir outras variáveis de confusão, como ordem rio e carbono orgânico solo montante. Vellend (2016) nomeou este problema como o “problema de três caixas”, que se refere à limitação em inferir que X (variável independente) causa variação em Y (variável dependente) quando outras variáveis criam ou ampliam correlação entre X e Y (ver Figura 2 em Ruxton Colegrave (2016)). Uma ferramenta útil para compreender relação entre todas variáveis relevantes seu estudo é um fluxograma. “fluxograma de pesquisa” (ver também Magnusson Mourão (2004)) proposto aqui, variáveis dependentes (também conhecidas como resposta) e independentes (ou preditora), bem como covariáveis são representadas como caixas (com formas distintas: Figura 2.3). Além disso, você pode usar uma seta para representar uma (possível) via causal indicando força e sinal (positivo ou negativo) da variável preditora na variável dependente (Figura 2.3). Ao fazer isso, você pode melhorar o desenho experimental ou observacional incluindo ou controlando variáveis de confusão o que, por sua vez, pode ajudar separar contribuição relativa de diferentes variáveis preditoras em seu sistema. Mais importante, fazer conexões entre variáveis melhora sua capacidade de visualizar o “Quadro geral” de sua pesquisa, o que pode afetar seu experimento, análise estatística e revisão da literatura. Na verdade, Arlidge et al. (2017) argumentam que fluxogramas facilitam construção de narrativas, melhorando: ) definição de múltiplas hipóteses, ii) coleta, interpretação e disseminação de dados e iii) comunicação conteúdo estudo. Você também pode ler os livros de Magnusson et al. (2004) para entender mais como usar fluxogramas para auxiliar análises estatísticas e Ford (2000) que recomenda o uso de uma abordagem analítica para fomentar o desenvolvimento da pesquisa. Além disso, o fluxograma de pesquisa pode ser usado como uma ferramenta forte para contemplar os conselhos de Ford (2000), que foram: ) definir pergunta da pesquisa, ii) definir teoria ser usada, iii) definir técnica de investigação (por exemplo, experimento, observação de campo), iv) definir medições, v) definir como fazer inferência, e vi) interpretar, generalizar e sintetizar partir de dados que, por sua vez, são usados para refinar teoria e modificar (quando necessário) questões futuras (Figura 2.1).\nFigura 2.3: Exemplo de como usar um fluxograma para melhorar o entendimento sistema estudado. pergunta teórica “Qual é o impacto da invasão na comunidade nativa e nas propriedades ecossistema?” pode gerar duas predições: ) planta exótica Prosopis juliflora reduz diversidade beta de comunidades de plantas nativas, e ii) Prosopis juliflora modifica composição das comunidades de plantas e reduz o estoque de carbono e taxas de decomposição. Após selecionar suas predições, você pode construir um fluxograma conectando variáveis relevantes e associações entre elas. Além disso, você pode usar informações na Caixa 1 para identificar que tipo de variável você irá coletar e quais figuras podem ser usadas (b).\n","code":""},{"path":"cap2.html","id":"questões-fundamentais-em-etnobiologia-ecologia-e-conservação","chapter":"Capítulo 2 Voltando ao básico: como dominar a arte de fazer perguntas cientificamente relevantes","heading":"2.4 Questões fundamentais em etnobiologia, ecologia e conservação","text":"teorias são generalizações. teorias contêm perguntas. Para algumas teorias, perguntas são explícitas e representam o que teoria pretende explicar. Para outras, questões são implícitas e se relacionam com quantidade e tipo de generalização, dada escolha de métodos e exemplos usados por pesquisadores na construção da teoria. teorias mudam continuamente, à medida que exceções são encontradas às suas generalizações e como questões implícitas sobre método e opções de estudos são expostas. - E. David Ford (2000)Como argumentamos antes, uma questão relevante e testável precede análises estatísticas. Assim, apresentamos seguir 12 questões que podem estimular pesquisas futuras na ECC. Observe, entanto, que não queremos dizer que eles são únicas questões relevantes serem testadas na EEC - ver, por exemplo, Sutherland et al. (2013) para uma avaliação completa da pesquisa de ponta em Ecologia; e Caixa 6.1 em Pickett et al. (2007) 2. Especificamente, essas questões são muito amplas e podem ser desenvolvidas em perguntas, hipóteses e predições mais restritas. Depois de cada questão teórica, apresentamos um estudo que testou essas hipóteses bem como variáveis relevantes que podem estimular estudos futuros.Como o uso da terra afeta manutenção da biodiversidade e distribuição de espécies em diferentes escalas espaciais?Exemplo: Vários estudos em diferentes ecossistemas e escalas investigaram como o uso da terra afeta biodiversidade. entanto, destacamos um estudo comparando os efeitos globais uso da terra (por exemplo, densidade populacional humana, paisagem para usos humanos, tempo desde conversão da floresta) em espécies terrestres (por exemplo, mudança líquida na riqueza local, dissimilaridade composicional média) (Newbold et al. 2015).Qual é o impacto da invasão biótica nas comunidades nativas e propriedades ecossistema?Exemplo: Investigar como o estabelecimento de espécies exóticas afetam riqueza de espécies receptor, comunidades nativas, bem como isso afeta entrega dos serviços ecossistêmicos. Estudos anteriores controlaram presença de espécies invasoras ou registros históricos comparados (estudos observacionais) dessas espécies e como elas impactam biodiversidade. Além disso, há algum esforço em compreender os preditores de invasibilidade (por exemplo, produto interno bruto de regiões, densidade populacional humana, litoral continental e ilhas) (Dawson et al. 2017).Como o declínio predador de topo afeta entrega de serviços ecossistêmicos?Exemplo: Investigar como remoção de grandes carnívoros afeta o fornecimento de serviços ecossistêmicos, como o sequestro de carbono, doenças e controle de danos às colheitas. Estudos anteriores investigaram esta questão controlando presença de predadores de topo ou comparando registros históricos (estudo observacionais) de espécies e vários preditores (por exemplo, perda e fragmentação de habitat, conflito entre humanos e espécies caçadas, utilização para medicina tradicional e superexploração de presas) (Ripple et al. 2014).Como acidificação dos oceanos afeta produtividade primária e teias alimentares em ecossistemas marinhos?Exemplo: Estudos recentes testaram os efeitos individuais e interativos da acidificação e aquecimento oceano nas interações tróficas em uma teia alimentar. acidificação e o aquecimento foram manipulados pela mudança dos níveis de CO2 e temperatura, respectivamente. Estudos anteriores demonstraram que elevação de CO2 e temperatura aumentou produtividade primária e afetou força controle de cima para baixo exercido por predadores (Goldenberg et al. 2017).Como podemos reconciliar necessidades da sociedade por recursos naturais com conservação da Natureza?Exemplo: Existe uma literatura crescente usando abordagens de paisagem para melhorar gestão da terra para reconciliar conservação e desenvolvimento econômico. Os estudos possuem diversos objetivos, mas em geral eles usaram o engajamento das partes interessadas, apoio institucional, estruturas eficazes de governança como variáveis preditoras e melhorias ambientais (por exemplo, conservação solo e da água, cobertura vegetal) e socioeconômicas (renda, capital social, saúde pública, emprego) como variáveis dependentes (Reed et al. 2017).Qual é o papel das áreas protegidas (UCs) para manutenção da biodiversidade e dos serviços ecossistêmicos?Exemplo: Houve um trabalho considerável na última década comparando eficácia das UCs para conservação da biodiversidade. Embora esta questão não esteja completamente separada da questão anterior, o desenho dos estudos é relativamente distinto. Em geral, os pesquisadores contrastam o número de espécies e o fornecimento de serviços ecossistêmicos (por exemplo, retenção de água e solo, sequestro de carbono) entre áreas legalmente protegidas (UCs) e não protegidas (Xu et al. 2017).Como integrar o conhecimento científico e das pessoas locais para mitigar os impactos negativos das mudanças climáticas e uso da terra na biodiversidade?Exemplo: Eventos climáticos extremos podem ter forte impacto sobre rendimento agrícola e produção de alimentos. Autores recentes têm argumentado que esse efeito pode ser mais forte para os pequenos agricultores. Estudos futuros podem investigar como precipitação e temperatura afetam o rendimento agrícola e como os agricultores tradicionais ou indígenas lidam com esse impacto negativo. Sistemas de agricultura tradicional têm menor erosão solo e emissões de N2O / CO2 que monoculturas e, portanto, podem ser vistos como uma atividade de mitigação viável em um mundo em constante mudança (Niggli et al. 2009; Altieri Nicholls 2017).Como mudanças climáticas afetam resiliência e estratégias adaptativas em sistemas socioecológicos?Exemplo: mudança clima altera tanto pesca quanto agricultura em todo o mundo, o que por sua vez obriga os humanos mudar suas estratégias de cultivo. Estudos recentes têm argumentado que agricultura em alguns países enfrentará riscos com mudanças climáticas. Esses estudos comparam diferentes sistemas de produção, de agricultura convencional outros tipos empregados por populações locais. Por exemplo, há uma forte conexão entre ) espécies ameaçadas e sobrepesca, ii) índice de desenvolvimento humano (IDH) e dependência média da pesca e aquicultura. Além disso, há evidências de que biodiversidade pode amortecer os impactos das mudanças climáticas aumentando resiliência da terra (Niggli et al. 2009; Altieri Nicholls 2017; Blanchard et al. 2017). Uma abordagem interessante é investigar como populações locais lidam com esses desafios em termos de percepções e comportamento.Como invasão biológica afeta espacial e temporalmente estrutura e funcionalidade dos sistemas sócio-ecológicos?Exemplo: Muitos estudos demonstraram que espécies invasoras têm consequências biológicas, econômicas e sociais negativas. Aqui, da mesma forma que pergunta B, os pesquisadores controlaram presença de espécies invasoras ou utilizaram registros históricos. entanto, trabalhos recentes quantificam não apenas riqueza e composição de espécies nativas, mas também atributos funcionais de animais/vegetais que afetam diretamente o fornecimento de serviços ecossistêmicos como abastecimento (comida, água), regulação (clima, controle de inundações), suporte (ciclagem de nutrientes, formação solo) e cultural (ecoturismo, patrimônio cultural) (Chaffin et al. 2016). Mas, espécies invasoras podem provocar efeitos positivos sistema sócio-ecológico aumentando disponibilidade de recursos naturais, impactando como pessoas gerenciam e usam biodiversidade local.Qual é relação entre diversidades filogenética e taxonômica com diversidade biocultural?Exemplo: Estudos recentes mostraram que existe um padrão filogenético e taxonômico nos recursos que pessoas incorporam em seus sistemas sócio-ecológicos, especialmente em plantas medicinais. Existe uma tendência para pessoas, em diferentes partes mundo, para usar plantas próximas filogeneticamente para os mesmos propósitos. Aqui, os pesquisadores podem testar o quanto isso afeta diversidade de práticas em um sistema sócio-ecológico considerando o ambiente, bem como sua estrutura e funções (Saslis-Lagoudakis et al. 2012, 2014).Quais variáveis ambientais e sócio-políticas mudam estrutura e funcionalidade dos sistemas sócio-ecológicos tropicais?Exemplo: Testar influência das mudanças ambientais afetadas pela espécie humana (por exemplo, fogo, exploração madeireira, aquecimento) em espécies-chave e, consequentemente, como esse efeito em cascata pode afetar outras espécies e serviços ecossistêmicos (por exemplo, armazenamento de carbono, ciclo da água e dinâmica fogo) (Lindenmayer Sato 2018).Os atributos das espécies influenciam como populações locais distinguem plantas ou animais úteis e não-úteis?Exemplo: Investigar se população local possui preferência ao selecionar espécies de animais ou plantas. Você pode avaliar se grupos diferentes (por exemplo, turistas) ou populações locais (por exemplo, pescadores) selecionam espécies com base em atributos das espécies. Estudos recentes têm mostrado uma ligação potencial entre planta (por exemplo, cor, folha, floração) e pássaro (por exemplo, cor, vocalização) e alguns serviços culturais ecossistema, como estética, recreativa e espiritual/religiosa (Goodness et al. 2016).Como você notou, questões eram mais teóricas e, consequentemente, você pode derivar predições testáveis (usando variáveis) partir delas (Figuras 2.1, 2.3). Por exemplo, da questão “Como o uso da terra afeta manutenção da biodiversidade e distribuição de espécies em diferentes escalas?” podemos derivar duas predições diferentes: ) densidade populacional (variável operacional de uso da terra) muda composição de espécies e reduz riqueza de espécies na escala da paisagem (predição derivada da hipótese da homogeneização biótica: Solar et al. 2015); ii) composição dos atributos funcionais das plantas é diferente em remanescentes florestais com diferentes matrizes (cana-de-açúcar, gado, cidade, etc.).","code":""},{"path":"cap2.html","id":"considerações-finais","chapter":"Capítulo 2 Voltando ao básico: como dominar a arte de fazer perguntas cientificamente relevantes","heading":"2.5 Considerações Finais","text":"Conte-seus segredosE faça-suas perguntasOh, vamos voltar para o inícioCorrendo em círculos, perseguindo caudasCabeças em uma ciência à parteNinguém disse que seria fácil(…) Desfazendo enigmasQuestões da ciência, ciência e progresso- O Cientista, ColdplayEste é um trecho de uma música da banda britânica de rock Coldplay, álbum de 2002 Rush Blood Head. letra é uma comparação incrível entre ciência e os altos e baixos de um relacionamento fadado ao fracasso. banda traz uma mensagem surpreendentemente clara de que como cientistas, nós deveríamos frequentemente fazer perguntas, voltar ao início após descobrir que estávamos errados (ou não) e que corremos em círculos tentando melhorar nosso conhecimento. banda descreveu de uma forma tão precisa o quão cíclico (mas não repetitivo) é o método científico. Como disse canção: não é fácil, mas aprender como fazer boas perguntas é um passo essencial para consolidação conhecimento. Ao incluir o teste de hipótese EEC, podemos ser mais precisos. Definitivamente, isso não significa que ciência descritiva seja inútil. Ao contrário, o desenvolvimento da ECC e principalmente da Etnobiologia, foi construído sobre uma linha de frente descritiva, o que significa que foi valioso para fundação da Etnobiologia como disciplina consolidada (Group 2003; Stepp 2005). entanto, estudos recentes defendem que etnobiologia deve dialogar com disciplinas com maior respaldo teórico, como ecologia e biologia evolutiva para melhorar pesquisa sobre biodiversidade (Albuquerque Ferreira Júnior 2017). Por sua vez, incorporando o conhecimento local em ecologia e evolução irá certamente refinar seu próprio desenvolvimento, que em última análise beneficia conservação biológica (Saslis-Lagoudakis Clarke 2013). Além disso, há uma necessidade urgente de formar jovens pesquisadores em filosofia e metodologia da ciência, bem como comunicação e produção científica (Albuquerque 2013). Como comentário final, acreditamos que formação dos alunos em EEC precisa de uma reavaliação que necessariamente volta aos conceitos e métodos básicos. Assim, os pesquisadores podem combinar o método hipotético-dedutivo com pensamento estatístico usando um fluxograma de pesquisa para ir além da descrição básica.","code":""},{"path":"cap3.html","id":"cap3","chapter":"Capítulo 3 Pré-requisitos","heading":"Capítulo 3 Pré-requisitos","text":"","code":""},{"path":"cap3.html","id":"introdução-1","chapter":"Capítulo 3 Pré-requisitos","heading":"3.1 Introdução","text":"O objetivo deste capítulo é informar como fazer instalação dos Programas R e RStudio, além de descrever os pacotes e dados necessários para reproduzir os exemplos livro.","code":""},{"path":"cap3.html","id":"instalação-do-r","chapter":"Capítulo 3 Pré-requisitos","heading":"3.2 Instalação do R","text":"Abaixo descrevemos os sete passos necessários para instalação programa R seu computador (Figura 3.1):Para começarmos trabalhar com o R é necessário baixá-lo na página R Project. Então, acesse esse site http://www.r-project.orgClique link download RNa página CRAN Mirros (Comprehensive R Archive Network), escolha uma das páginas espelho Brasil mais próxima de você para baixar o programaEscolha agora o sistema operacional seu computador (passos adicionais existem para diferentes distribuições Linux ou MacOS). Aqui faremos o exemplo com o WindowsClique em base para finalmente chegar à página de download com versão mais recente RClique arquivo Download R (versão mais recente) Windows que será instalado seu computadorAbra o arquivo que foi baixado seu computador e siga os passos indicados para finalizar instalação programa R\nFigura 3.1: Esquema ilustrativo demonstrando os passos necessários para instalação programa R computador. Fonte das figuras: imagem computador e imagem da lupa.\n 📝 Importante \nPara o Sistema Operacional () Windows, alguns pacotes são dependentes da instalação separada Rtools40. Da mesma forma, GNU/Linux e MacOS também possuem dependências de outras bibliotecas para pacotes específicos, mas que não abordaremos aqui. Essas informações de dependência geralmente são retornadas como erros e você pode procurar ajuda em fóruns específicos.","code":""},{"path":"cap3.html","id":"instalação-do-rstudio","chapter":"Capítulo 3 Pré-requisitos","heading":"3.3 Instalação do RStudio","text":"O RStudio possui algumas características que o tornam popular: várias janelas de visualização, marcação e preenchimento automático script, integração com controle de versão, dentre outras funcionalidades.Abaixo descrevemos os cinco passos necessários para instalação RStudio seu computador (Figura 3.2):Para fazer o download RStudio, acessamos o site https://www.rstudio.com/Clique em downloadEscolha versão gratuitaEscolha o instalador com base seu sistema operacionalAbra o arquivo que foi baixado seu computador e siga os passos indicados para finalizar instalação programa RStudio\nFigura 3.2: Esquema ilustrativo demonstrando os passos necessários para instalação programa RStudio computador. Fonte das figuras: imagem computador - https://pt.wikipedia.org/wiki/Computador_pessoal e imagem da lupa - https://openclipart.org/detail/185356/magnifier.\n","code":""},{"path":"cap3.html","id":"versão-do-r","chapter":"Capítulo 3 Pré-requisitos","heading":"3.4 Versão do R","text":"Todas os códigos, pacotes e análises disponibilizados livro foram realizados Programa R versão 4.1.2 (10-12-2021).","code":""},{"path":"cap3.html","id":"pacotes","chapter":"Capítulo 3 Pré-requisitos","heading":"3.5 Pacotes","text":"Descrevemos Capítulo 4 o que são e como instalar os pacotes para realizar análises estatísticas R. 📝 Importante \nCriamos o pacote ecodados que contém todas informações e dados utilizados neste livro. Assim, recomendamos que você instale e carregue este pacote início de cada capítulo para ter acesso aos dados necessários para executar funções R.Para instalação pacote ecodados macOS, você precisará ter instalado o programa XCode que pode ser baixado aqui. Este programa é disponibilizado gratuitamente pela Apple e é necessário para compilar quaisquer programas distribuídos em código fonte (ou seja, sem um binário). Após instalar esse programa e o pacote devtools, você poderá instalar o ecodados utilizando instruções abaixo.Abaixo, listamos todos os pacotes utilizados livro. Você pode instalar os pacotes agora ou esperar para instalá-los quando ler o Capítulo 4 e entender o que são funções install.packages(), library() e install_github(). Para fazer instalação, você vai precisar estar conectado à internet.Diferente dos pacotes anteriores que são baixados CRAN, alguns pacotes são baixados GitHub dos pesquisadores responsáveis pelos pacotes. GitHub é um repositório remoto de códigos que permite controle de versão, muito utilizado por desenvolvedores e programadores. Nestes casos, precisamos carregar o pacote devtools para acessar função install_github. Durante instalações destes pacotes, algumas vezes o R irá pedir para você digitar um número indicando os pacotes que você deseja fazer update. Neste caso, digite 1 para indicar que ele deve atualizar os pacotes dependentes antes de instalar os pacotes requeridos.","code":"\ninstall.packages(c(\"ade4\", \"adespatial\", \"ape\", \"bbmle\", \"betapart\", \"BiodiversityR\", \"car\", \"cati\", \"datasauRus\", \"devtools\", \"DHARMa\", \"dplyr\", \"ecolottery\", \"emmeans\", \"factoextra\", \"FactoMineR\", \"fasterize\", \"FD\", \"forcats\", \"geobr\", \"generalhoslem\", \"GGally\", \"ggExtra\", \"ggforce\", \"ggplot2\", \"ggpubr\", \"ggrepel\", \"ggspatial\", \"glmmTMB\", \"grid\", \"gridExtra\", \"here\", \"hillR\", \"iNEXT\", \"janitor\", \"kableExtra\", \"knitr\", \"labdsv\", \"lattice\", \"leaflet\", \"lmtest\", \"lsmeans\", \"lubridate\", \"mapview\", \"MASS\", \"MuMIn\", \"naniar\", \"nlme\", \"ordinal\", \"palmerpenguins\", \"performance\", \"pez\", \"phyloregion\", \"phytools\", \"picante\", \"piecewiseSEM\", \"purrr\", \"pvclust\", \"raster\", \"readr\", \"reshape2\", \"rgdal\" , \"Rmisc\", \"rnaturalearth\", \"RVAideMemoire\", \"sciplot\", \"sf\", \"sidrar\", \"sjPlot\", \"spData\", \"spdep\", \"stringr\", \"SYNCSA\", \"tibble\", \"tidyr\", \"tidyverse\", \"tmap\", \"tmaptools\", \"TPD\", \"vcd\", \"vegan\", \"viridis\", \"visdat\", \"mvabund\", \"rdist\", \"udunits2\"), dependencies = TRUE)\nlibrary(devtools) \ninstall_github(\"paternogbc/ecodados\")\ninstall_github(\"mwpennell/geiger-v2\")\ninstall_github(\"fawda123/ggord\")\ninstall_github(\"jinyizju/V.PhyloMaker\")\ninstall_github(\"ropensci/rnaturalearthhires\")"},{"path":"cap3.html","id":"dados","chapter":"Capítulo 3 Pré-requisitos","heading":"3.6 Dados","text":"maioria dos exemplos livro utilizam dados reais extraídos de artigos científicos que já foram publicados ou dados que foram coletados por um dos autores deste livro. Todos os dados, publicados ou simulados, estão disponíveis pacote ecodados. Além disso, em cada capítulo fazemos uma breve descrição dos dados para facilitar compreensão sobre o que é variável resposta ou preditora, como essas variáveis estão relacionadas com perguntas e predições exemplo.","code":""},{"path":"cap4.html","id":"cap4","chapter":"Capítulo 4 Introdução ao R","heading":"Capítulo 4 Introdução ao R","text":"","code":""},{"path":"cap4.html","id":"pré-requisitos-do-capítulo","chapter":"Capítulo 4 Introdução ao R","heading":"Pré-requisitos do capítulo","text":"Pacotes e dados que serão utilizados neste capítulo.","code":"\n## Pacotes\nlibrary(ecodados)\n\n## Dados necessários\nintror_anfibios_locais <- ecodados::intror_anfibios_locais"},{"path":"cap4.html","id":"contextualização","chapter":"Capítulo 4 Introdução ao R","heading":"4.1 Contextualização","text":"O objetivo deste capítulo é apresentar os aspectos básicos da linguagem R para realização dos principais passos para manipulação, visualização e análise de dados. Abordaremos aqui questões básicas sobre linguagem R, como: ) R e RStudio, ii) funcionamento da linguagem, iii) estrutura e manipulação de objetos, iv) exercícios e v) principais livros e material para se aprofundar nos seus estudos.Todo processo de aprendizagem torna-se mais efetivo quando teoria é combinada com prática. Assim, recomendamos fortemente que você, leitor() acompanhe os códigos e exercícios deste livro, ao mesmo tempo que os executa em seu computador e não só os leia passivamente. Além disso, se você tiver seus próprios dados é muito importante tentar executar e/ou replicar análises e/ou gráficos. Por motivos de espaço, não abordaremos todas questões relacionadas ao uso da linguagem R neste capítulo. Logo, aconselhamos que você consulte o material sugerido final capítulo para se aprofundar.Este capítulo, na maioria das vezes, pode desestimular pessoas que estão iniciando, uma vez que o mesmo não apresenta os códigos para realizar análises estatísticas. Contudo, ele é essencial para o entendimento e interpretação que está sendo informado nas linhas de código, além de facilitar manipulação dos dados antes de realizar análises estatísticas. Você perceberá que não usará este capítulo para fazer análises, mas voltará aqui diversas vezes para relembrar qual é o código ou o que significa determinada expressão ou função usada nos próximos capítulos.","code":""},{"path":"cap4.html","id":"r-e-rstudio","chapter":"Capítulo 4 Introdução ao R","heading":"4.2 R e RStudio","text":"Com o R é possível manipular, analisar e visualizar dados, além de escrever desde pequenas linhas de códigos até programas inteiros. O R é versão em código aberto de uma linguagem de programação chamada de S, criada por John M. Chambers (Stanford University, CA, EUA) nos anos 1980 Bell Labs, que contou com três versões: Old S (1976-1987), New S (1988-1997) e S4 (1998), utilizada na IDE S-PLUS (1988-2008). Essa linguagem tornou-se bastante popular e vários produtos comerciais que usam ainda estão disponíveis, como o SAS.final dos anos 1990, Robert Gentleman e Ross Ihaka (ambos da Universidade de Auckland, Nova Zelândia), iniciaram o desenvolvimento da versão livre da linguagem S, linguagem R, com o seguinte histórico: Desenvolvimento (1997-2000), Versão 1 (2000-2004), Versão 2 (2004-2013), Versão 3 (2013-2020) e Versão 4 (2020). Para mais detalhes histórico de desenvolvimento das linguagens S e R, consultar Wickham (2013). Atualmente linguagem R é mantida por uma rede de colaboradores denominada R Core Team. origem nome R é desconhecida, mas reza lenda que ao lançarem o nome da linguagem os autores se valeram da letra que vinha antes S, uma vez que linguagem R foi baseada nela e utilizaram letra “R”. Outra história conta que pelo fato nome dos dois autores iniciarem por “R”, batizaram linguagem com essa letra, vai saber.Um aspecto digno de nota é que linguagem R é uma linguagem de programação interpretada, assim como o Python, mas contrária outras linguagens como C e Java, que são compiladas. Isso faz ser mais fácil de ser utilizada, pois processa linhas de código e transforma em linguagem de máquina (código binário que o computador efetivamente lê), apesar desse fato diminuir velocidade de processamento.Para começarmos trabalhar com o R é necessário baixá-lo na página R Project. Os detalhes de instalação são apresentados Capítulo 3. Reserve um tempo para explorar esta página R-Project. Existem vários livros dedicados diversos assuntos baseados R. Além disso, estão disponíveis manuais em diversas línguas para serem baixados gratuitamente.Como o R é um software livre, não existe possibilidade de o usuário entrar em contato com um serviço de suporte de usuários, muito comuns em softwares pagos. Ao invés disso, existem várias listas de e-mails que fornecem suporte à comunidade de usuários. Nós, particularmente, recomendamos o ingresso nas seguintes listas: R-help, R-sig-ecolog, R-br e discourse.curso-r. Os dois últimos grupos reúnem pessoas usuárias brasileiras programa R.Apesar de podermos utilizar o R com o IDE (Ambiente de Desenvolvimento Integrado - Integrated Development Environment) RGui que vem com instalação da linguagem R para usuários Windows (Figura 4.1) ou próprio terminal para usuários Linux e MacOS, existem alguns IDEs específicos para facilitar nosso uso dessa linguagem.\nFigura 4.1: Interface RGui. Os números indicam: (1) R Script, (2) R Console, e (3) R Graphics.\nDessa forma, nós que escrevemos este livro utilizamos o IDE RStudio e assumimos que você que está lendo fará o mesmo.O RStudio permite diversas personalizações, grande parte delas contidas em Tools > Global options. Incentivamos leitoras e leitores “fuçar” com certa dose de cuidado, nas opções para personalização. Dentre essas mudanças, destacamos três:Tools > Global options > Appearance > Editor theme: para escolher um tema para seu RStudioTools > Global options > Code > [X] Soft-wrap R source files: com essa opção habilitada, quando escrevemos comentários longos ou mudamos largura da janela que estamos trabalhando, todo o texto e o código se ajustam janela automaticamenteTools > Global options > Code > Display > [X Show Margis] e Margin column (80): com essa opção habilitada e para esse valor (80), uma linha vertical irá aparecer script marcando 80 caracteres, um comprimento máximo recomendado para padronização dos scripts 📝 Importante \nPara evitar possíveis erros é importante instalar primeiro o software da linguagem R e depois o IDE RStudio.O RStudio permite também trabalhar com projetos. Projeto RStudio é uma forma de organizar os arquivos de scripts e dados dentro de um diretório, facilitando o compartilhamento de fluxo de análises de dados e aumentando assim reprodutibilidade. Podemos criar um Projeto RStudio indo em File > New Project ou ícone de cubo azul escuro que possui um R dentro com um um círculo verde com um sinal de + na parte superior esquerda ou ainda canto superior direito que possui cubo azul escrito Project que serve para gerenciar os projetos e depois em New Project. Depois de escolher uma dessas opções, uma janela se abrirá onde escolhemos uma das três opções: ) New Directory (para criar um diretório novo com diversas opções), ii) Existing Directory (para escolher um diretório já existente) e iii) Version Control (para criar um projeto que será versionado pelo git ou Subversion).","code":""},{"path":"cap4.html","id":"funcionamento-da-linguagem-r","chapter":"Capítulo 4 Introdução ao R","heading":"4.3 Funcionamento da linguagem R","text":"Nesta seção, veremos os principais conceitos para entender como linguagem R funciona ou como geralmente utilizamos o IDE RStudio dia dia, para executar nossas rotinas utilizando linguagem R. Veremos então: ) console, ii) script, iii) operadores, iv) objetos, v) funções, vi) pacotes, vii) ajuda (help), viii) ambiente (environment/workspace), ix) citações e x) principais erros.Antes de iniciarmos o uso R pelo RStudio é fundamental entendermos alguns pontos sobre janelas e o funcionamento delas RStudio (Figura 4.2).\nFigura 4.2: Interface RStudio. Os números indicam: (1) janela com abas de Script, R Markdown, dentre outras; (2) janela com abas de Console, Terminal e Jobs; (3) janela com abas de Environment, History, Conections e Tutorial; e (4) janela com abas de Files, Plots, Packages, Help e Viewer.\nDetalhando algumas dessas janelas e abas, temos:Console: painel onde os códigos são rodados e vemos saídasEditor/Script: painel onde escrevemos nossos códigos em R, R Markdown ou outro formatoEnvironment: painel com todos os objetos criados na sessãoHistory: painel com o histórico dos códigos rodadosFiles: painel que mostra os arquivos diretório de trabalhoPlots: painel onde os gráficos são apresentadosPackages: painel que lista os pacotesHelp: painel onde documentação das funções é exibidaNo RStudio, alguns atalhos são fundamentais para aumentar nossa produtividade:F1: abre o painel de Help quando digitado em cima nome de uma funçãoCtrl + Enter: roda linha de código selecionada scriptCtrl + Shift + N: abre um novo scriptCtrl + S: salva um scriptCtrl + Z: desfaz uma operaçãoCtrl + Shift + Z: refaz uma operaçãoAlt + -: insere um sinal de atribuição (<-)Ctrl + Shift + M: insere um operador pipe (%>%)Ctrl + Shift + C: comenta uma linha script - insere um (#)Ctrl + : indenta (recuo inicial das linhas) linhasCtrl + Shift + : reformata o códigoCtrl + Shift + R: insere uma sessão (# ———————-)Ctrl + Shift + H: abre uma janela para selecionar o diretório de trabalhoCtrl + Shift + F10: reinicia o consoleCtrl + L: limpa os códigos consoleAlt + Shift + K: abre uma janela com todos os atalhos disponíveis","code":""},{"path":"cap4.html","id":"console","chapter":"Capítulo 4 Introdução ao R","heading":"4.3.1 Console","text":"O console é onde versão da linguagem R instalada é carregada para executar os códigos da linguagem R (Figura 4.2 janela 2). Na janela console aparecerá o símbolo >, seguido de uma barra vertical | que fica piscando (cursor), onde digitamos ou enviamos nossos códigos script. Podemos fazer um pequeno exercício: vamos digitar 10 + 2, seguido da tecla Enter para que essa operação seja executada.O resultado retorna o valor 12, precedido de um valor entre colchetes. Esses colchetes demonstram posição elemento numa sequência de valores. Se fizermos essa outra operação 1:42, o R vai criar uma sequência unitária de valores de 1 42. depender da largura da janela console, vai aparecer um número diferente entre colchetes indicando sua posição na sequência: antes número 1 vai aparecer o [1], depois quando sequência quebrada, vai aparecer o número correspondente da posição elemento, por exemplo, [37].Podemos ver o histórico dos códigos executados console na aba History (Figura 4.2 janela 3).","code":"\n10 + 2\n#> [1] 12\n1:42\n#>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42"},{"path":"cap4.html","id":"scripts","chapter":"Capítulo 4 Introdução ao R","heading":"4.3.2 Scripts","text":"Scripts são arquivos de texto simples, criados com extensão (terminação) .R (Figura 4.2 janela 1). Para criar um script, basta ir em File > New File > R Script, ou clicar ícone com uma folha branca e um círculo verde com um sinal de +, logo abaixo de File, ou ainda usando o atalho Ctrl + Shift + N.Uma vez escrito os códigos script podemos rodar esses códigos de duas formas: ) todo o script de uma vez, clicando em Source (que fina canto superior direito da aba script) ou usando o atalho Ctrl + Shift + Enter; ou ii) apenas linha onde o cursor estiver posicionado, independentemente de sua posição naquela linha, clicando em Run ou usando o atalho Ctrl + Enter.Devemos sempre salvar nossos scripts, tomando por via de regra: primeiro criar o arquivo e depois ir salvando nesse mesmo arquivo cada passo de desenvolvimento das análises (não é raro o RStudio fechar sozinho e você perder algum tempo de trabalho). Há diversos motivos para se criar um script: continuar o desenvolvimento desse script em outro momento ou em outro computador, preservar trabalhos passados, ou ainda compartilhar seus códigos com outras pessoas. Para criar ou salvar um script basta ir em File > Save, escolher um diretório e nome para o script e salvá-lo. Podemos ainda utilizar o atalho Ctrl + S.Em relação aos scripts, há ainda os comentários, representados pelos símbolos # (hash), #' (hash-linha) e #> (hash-maior). diferença entre eles é que para o segundo e terceiro, quando pressionamos tecla Enter o comentário #' e #> são inseridos automaticamente na linha seguinte. Linhas de códigos script contendo comentários em seu início não são lidos pelo console R. Se o comentário estiver final da linha, essa linha de código ainda será lida. Os comentários são utilizados geralmente para: ) descrever informações sobre dados ou funções e/ou ii) suprimir linhas de código.É interessante ter início de cada script um cabeçalho identificando o objetivo ou análise, autor e data para facilitar o compartilhamento e reprodutibilidade. Os comentários podem ser inseridos ou retirados das linhas com o atalho Ctrl + Shift + C.Além disso, podemos usar comentários para adicionar informações sobre os códigos.Por fim, outro ponto fundamental é ter boas práticas de estilo de código. Quanto mais organizado e padronizado estiver seus scripts, mais fácil de entendê-los e de procurar possíveis erros. Existem dois guias de boas práticas para adequar seus scripts: Hadley Wickham e Google.Ainda em relação aos scripts, temos os Code Snippets (Fragmentos de código), que são macros de texto usadas para inserir rapidamente fragmentos comuns de código. Por exemplo, o snippet fun insere uma definição de função R. Para mais detalhes, ler o artigo RStudio Code Snippets.Uma aplicação bem interessante dos Code Snippets script é o ts. Basta digitar esse código e em seguida pressionar tecla Tab para inserir rapidamente data e horário atuais script em forma de comentário.","code":"\n#' ---\n#' Título: Capítulo 04 - Introdução ao R\n#' Autor: Maurício Vancine\n#' Data: 11-11-2021\n#' ---\n## Comentários\n# O R não lê a linha do código depois do # (hash).\n42 # Essas palavras não são executadas, apenas o 42, a resposta para questão fundamental da vida, o universo e tudo mais.\n#> [1] 42\n# fun {snippet}\nfun\nname <- function(variables) {\n    \n}\n# ts {snippet}\n# Thu Nov 11 18:19:26 2021 ------------------------------"},{"path":"cap4.html","id":"operadores","chapter":"Capítulo 4 Introdução ao R","heading":"4.3.3 Operadores","text":"R, podemos agrupar os operadores em cinco tipos: aritméticos, relacionais, lógicos, atribuição e diversos. Grande parte deles são descritos na Tabela 4.1.Tabela 4.1: Principais operadores R.Como exemplo, podemos fazer operações simples usando os operadores aritméticos.Precisamos ficar atentos à prioridade dos operadores aritméticos:PRIORITÁRIO () > ^ > * ou / > + ou - NÃO PRIORITÁRIOVeja exemplo abaixo como o uso dos parênteses muda o resultado.","code":"\n## Operações aritméticas\n10 + 2 # adição\n#> [1] 12\n10 * 2 # multiplicação\n#> [1] 20\n## Sem especificar a ordem\n# Segue a ordem dos operadores.\n1 * 2 + 2 / 2 ^ 2\n#> [1] 2.5\n\n## Especificando a ordem\n# Segue a ordem dos parenteses.\n((1 * 2) + (2 / 2)) ^ 2\n#> [1] 9"},{"path":"cap4.html","id":"objetos","chapter":"Capítulo 4 Introdução ao R","heading":"4.3.4 Objetos","text":"Objetos são palavras às quais são atribuídos dados. atribuição possibilita manipulação de dados ou armazenamento dos resultados de análises. Utilizaremos os símbolos < (menor), seguido de - (menos), sem espaço, dessa forma <-. Também podemos utilizar o símbolo de igual (=), mas não recomendamos, por não fazer parte das boas práticas de escrita de códigos em R. Podemos inserir essa combinação de símbolos com o atalho Alt + -. Para demonstrar, vamos atribuir o valor 10 à palavra obj_10, e chamar esse objeto novamente para verificar seu conteúdo. 📝 Importante \nRecomendamos sempre verificar o conteúdo dos objetos chamando-os novamente para confirmar se atribuição foi realizada corretamente e se o conteúdo corresponde à operação realizada.Todos os objetos criados numa sessão R ficam listados na aba Environment (Figura 4.2 janela 3). Além disso, o RStudio possui função autocomplete, ou seja, podemos digitar primeiras letras de um objeto (ou função) e em seguida apertar Tab para que o RStudio liste tudo que começar com essas letras.Dois pontos importantes sobre atribuições: primeiro, o R sobrescreve os valores dos objetos com o mesmo nome, deixando o objeto com o valor da última atribuição.Segundo, o R tem limitações ao nomear objetos:nome de objetos só podem começar por letras (-z ou -Z) ou pontos (.)nome de objetos só podem conter letras (-z ou -Z), números (0-9), underscores (_) ou pontos (.)R é case-sensitive, .e., ele reconhece letras maiúsculas como diferentes de letras minúsculas. Assim, um objeto chamado “resposta” é diferente objeto “RESPOSTA”devemos evitar acentos ou cedilha (ç) para facilitar memorização dos objetos e também para evitar erros de codificação (encoding) de caracteresnomes de objetos não podem ser iguais nomes especiais, reservados para programação (break, else, FALSE, , function, , Inf, NA, NaN, next, repeat, return, TRUE, )Podemos ainda utilizar objetos para fazer operações e criar objetos. Isso pode parecer um pouco confuso para os iniciantes, mas é fundamental aprender essa lógica para passar para os próximos passos.","code":"\n## Atribuição - símbolo (<-)\nobj_10 <- 10\nobj_10\n#> [1] 10\n## Sobrescreve o valor dos objetos\nobj <- 100\nobj\n#> [1] 100\n\n## O objeto 'obj' agora vale 2\nobj <- 2\nobj\n#> [1] 2\n## Definir dois objetos\nva1 <- 10\nva2 <- 2\n\n## Operações com objetos e atribuicão\nadi <- va1 + va2\nadi\n#> [1] 12"},{"path":"cap4.html","id":"funções","chapter":"Capítulo 4 Introdução ao R","heading":"4.3.5 Funções","text":"Funções são códigos preparados para realizar uma tarefa específica de modo simples. Outra forma de entender uma função é: códigos que realizam operações em argumentos. Devemos retomar ao conceito ensino médio de funções: os dados de entrada são argumentos e função realizará alguma operação para modificar esses dados de entrada. estrutura de uma função é muito similar à sintaxe usada em planilhas eletrônicas, sendo composta por:nome_da_função(argumento1, argumento2, …)Nome da função: remete ao que ela fazParênteses: limitam funçãoArgumentos: valores, parâmetros ou expressões onde função atuaráVírgulas: separam os argumentosOs argumentos de uma função podem ser de dois tipos:Valores ou objetos: função alterará os valores em si ou os valores atribuídos aos objetosParâmetros: valores fixos que informam um método ou realização de uma operação. Informa-se o nome desse argumento, seguido de “=” e um número, texto ou TRUE ou FALSEAlguns exemplos de argumentos como valores ou objetos.Vamos ver agora alguns exemplos de argumentos usados como parâmetros. Note que apesar valor argumento ser o mesmo (10), seu efeito resultado da função rep() muda drasticamente. Aqui também é importante destacar um ponto: ) podemos informar os argumentos sequencialmente, sem explicitar seus nomes, ou ii) independente da ordem, mas explicitando seus nomes. Entretanto, como exemplo abaixo, devemos informar o nome argumento (.e., parâmetro), para que seu efeito seja o que desejamos.Um ponto fundamental e que deve ser entendido é o fluxo de atribuições resultado da operação de funções novos objetos. desenvolvimento de qualquer script na linguagem R, grande parte da estrutura mesmo será dessa forma: atribuição de dados objetos > operações com funções > atribuição dos resultados novos objetos > operações com funções desses novos objetos > atribuição dos resultados novos objetos. Ao entender esse funcionamento, começamos entender como devemos pensar na organização nosso script para montar análises que precisamos.Por fim, é fundamental também entender origem das funções que usamos R. Todas funções são advindas de pacotes. Esses pacotes possuem duas origens.pacotes já instalados por padrão e que são carregados quando abrimos o R (R Base)pacotes que instalamos e carregamos com funções","code":"\n## Funções - argumentos como valores\nsum(10, 2)\n#> [1] 12\n\n## Funções - argumentos como objetos\nsum(va1, va2)\n#> [1] 12\n## Funções - argumentos como parâmetros\n## Repetição - repete todos os elementos\nrep(x = 1:5, times = 10)\n#>  [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n\n## Repetição - repete cada um dos elementos\nrep(x = 1:5, each = 10)\n#>  [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5\n## Atribuicão dos resultados\n## Repetição\nrep_times <- rep(1:5, times = 10)\nrep_times\n#>  [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n\n## Somar e atribuir\nrep_times_soma <- sum(rep_times)\nrep_times_soma\n#> [1] 150\n\n## Raiz e atribuir\nrep_times_soma_raiz <- sqrt(rep_times_soma)\nrep_times_soma_raiz\n#> [1] 12.24745"},{"path":"cap4.html","id":"pacotes-1","chapter":"Capítulo 4 Introdução ao R","heading":"4.3.6 Pacotes","text":"Pacotes são conjuntos extras de funções para executar tarefas específicas, além dos pacotes instalados R Base. Existe literalmente milhares de pacotes (~19,000 enquanto estamos escrevendo esse livro) para mais diversas tarefas: estatística, ecologia, geografia, sensoriamento remoto, econometria, ciências sociais, gráficos, machine learning, etc. Podemos verificar este vasto conjunto de pacotes pelo link que lista por nome os pacotes oficiais, ou seja, que passaram pelo crivo CRAN. Existem ainda muito mais pacotes em desenvolvimento, geralmente disponibilizados em repositórios GitHub ou GitLab.Podemos listar esses pacotes disponíveis CRAN com esse código.Primeiramente, com uma sessão R sem carregar nenhum pacote extra, podemos verificar pacotes carregados pelo R Base utilizando função search().Podemos ainda verificar todos pacotes instalados em nosso computador com função library().R, quando tratamos de pacotes, devemos destacar diferença de dois conceitos: instalar um pacote e carregar um pacote. instalação de pacotes possui algumas características:Instala-se um pacote apenas uma vezPrecisamos estar conectados à internetO nome pacote precisa estar entre aspas na função de instalaçãoFunção (CRAN): install.packages()Vamos instalar o pacote vegan diretamente CRAN, que possui funções para realizar uma série de análise em ecologia (veja mais Capítulo 10). Para isso, podemos ir em Tools > Install Packages..., ou ir na aba Packages (Figura 4.2 janela 4), procurar o pacote e simplesmente clicar em “Install”. Podemos ainda utilizar função install.packages().Podemos conferir em que diretórios um pacote será instalado com função .libPaths(). 📝 Importante \nUma vez instalado um pacote, não há necessidade de instalá-lo novamente. Entretanto, todas às vezes que iniciarmos uma sessão R, precisamos carregar os pacotes com funções que precisamos utilizar.O carregamento de pacotes possui algumas características:Carrega-se o pacote toda vez que se abre uma nova sessão RNão precisamos estar conectados à internetO nome pacote não precisa estar entre aspas na função de carregamentoFunções: library() ou require()Vamos carregar o pacote vegan que instalamos anteriormente. Podemos ir na aba Packages (Figura 4.2 janela 4) e assinalar o pacote que queremos carregar ou utilizar função library().Como dissemos, alguns pacotes em desenvolvimento encontram-se disponíveis em repositórios GitHub, GitLab e Bioconductor. Para instalar pacotes GitHub, por exemplo, precisamos instalar e carregar o pacote devtools.Uma vez instalado e carregado esse pacote, podemos instalar o pacote GitHub, utilizando função devtools::install_github(). Precisamos atentar para usar essa forma “nome_usuario/nome_repositorio”, retirados link repositório de interesse. Como exemplo, podemos instalar o pacote ecodados repositório GitHub paternogbc/ecodados e depois utilizar função library() para carregá-lo.Pode ser que em algumas circunstâncias precisaremos instalar pacotes com versões específicas para algumas análises. forma mais simples de fazer isso é instalar um pacote partir de um arquivo compactado .tar.gz. Para isso podemos ir à base CRAN e realizar o download: https://cran.r-project.org/src/contrib/Archive/. Para exemplificar, vamos instalar o pacote vegan 2.4.0.Podemos ver descrição de um pacote com função packageDescription().maioria dos pacotes possui conjuntos de dados que podem ser acessados pela função data(). Esses conjuntos de dados podem ser usados para testar funções pacote. Se estiver com dúvida na maneira como você deve preparar planilha para realizar uma análise específica, entre na Ajuda (Help) da função e veja os conjuntos de dados que estão exemplo desta função. Como exemplo, vamos carregar os dados dune pacote vegan, que são dados de observações de 30 espécies vegetais em 20 locais.Se por algum motivo precisarmos desinstalar um pacote, podemos utilizar função remove.packages(). Já para descarregar um pacote de uma sessão R, podemos usar função detach().E um último ponto fundamental sobre pacotes, diz respeito à atualização dos mesmos. Os pacotes são atualizados com frequência, e infelizmente (ou felizmente, pois atualizações podem oferecer algumas quebras entre pacotes), não se atualizam sozinhos. Muitas vezes, instalação de um pacote pode depender da versão dos pacotes dependentes, e geralmente uma janela com diversas opções numéricas se abre perguntando se você quer que todos os pacotes dependentes sejam atualizados. Podemos ir na aba Packages (Figura 4.2 janela 4) e clicar em “Update” ou usar função update.packages(checkBuilt = TRUE, ask = FALSE) para atualizá-los, entretanto, essa é uma função que costuma demorar muito para terminar de ser executada.Para fazer atualização dos pacotes instalados pelo GitHub, recomendamos o uso pacote dtupdate.Destacamos e incentivamos ainda uma prática que achamos interessante para aumentar reprodutibilidade de nossos códigos e scripts: de chamar funções de pacotes carregados dessa forma pacote::função(). Com o uso dessa prática, deixamos claro o pacote em que função está implementada. Esta prática é importante por que com frequência pacotes diferentes criam funções com mesmo nome, mas com características internas (argumentos) diferentes. Assim, não expressar o pacote de interesse pode gerar erros na execução de suas análises. Destacamos aqui o exemplo de como instalar pacotes GitHub pacote devtools.","code":"\n## Número atual de pacotes no CRAN\nnrow(available.packages())\n#> [1] 18639\n## Verificar pacotes carregados\nsearch()\n## Verificar pacotes instalados\nlibrary()\n## Instalar pacotes\ninstall.packages(\"vegan\")\n## Diretórios de intalação dos pacotes\n.libPaths()\n#> [1] \"/home/mude/R/x86_64-pc-linux-gnu-library/4.2\" \"/usr/local/lib/R/site-library\"                \"/usr/lib/R/site-library\"                     \n#> [4] \"/usr/lib/R/library\"\n## Carregar pacotes\nlibrary(vegan)\n#> Loading required package: permute\n#> Loading required package: lattice\n#> This is vegan 2.6-2\n## Instalar pacote devtools\ninstall.packages(\"devtools\")\n\n## Carregar pacote devtools\nlibrary(devtools)\n## Instalar pacote do github\ndevtools::install_github(\"paternogbc/ecodados\")\n\n## Carregar pacote do github\nlibrary(\"ecodados\")\n## Download do arquivo .tar.gz\ndownload.file(url = \"https://cran.r-project.org/src/contrib/Archive/vegan/vegan_2.4-0.tar.gz\",\n              destfile = \"vegan_2.4-0.tar.gz\", mode = \"auto\")\n\n## Instalar o pacote vegan 2.4.0\ninstall.packages(\"vegan_2.4-0.tar.gz\", repos = NULL, type = \"source\")\n## Descrição de um pacote\npackageDescription(\"vegan\")\n#> Package: vegan\n#> Title: Community Ecology Package\n#> Version: 2.6-2\n#> Authors@R: c(person(\"Jari\", \"Oksanen\", role=c(\"aut\",\"cre\"), email=\"jhoksane@gmail.com\"), person(\"Gavin L.\", \"Simpson\", role=\"aut\",\n#>               email=\"ucfagls@gmail.com\"), person(\"F. Guillaume\", \"Blanchet\", role=\"aut\"), person(\"Roeland\", \"Kindt\", role=\"aut\"),\n#>               person(\"Pierre\", \"Legendre\", role=\"aut\"), person(\"Peter R.\", \"Minchin\", role=\"aut\"), person(\"R.B.\", \"O'Hara\",\n#>               role=\"aut\"), person(\"Peter\", \"Solymos\", role=\"aut\"), person(\"M. Henry H.\", \"Stevens\", role=\"aut\"), person(\"Eduard\",\n#>               \"Szoecs\", role=\"aut\"), person(\"Helene\", \"Wagner\", role=\"aut\"), person(\"Matt\", \"Barbour\", role=\"aut\"),\n#>               person(\"Michael\", \"Bedward\", role=\"aut\"), person(\"Ben\", \"Bolker\", role=\"aut\"), person(\"Daniel\", \"Borcard\",\n#>               role=\"aut\"), person(\"Gustavo\", \"Carvalho\", role=\"aut\"), person(\"Michael\", \"Chirico\", role=\"aut\"), person(\"Miquel\",\n#>               \"De Caceres\", role=\"aut\"), person(\"Sebastien\", \"Durand\", role=\"aut\"), person(\"Heloisa Beatriz Antoniazi\",\n#>               \"Evangelista\", role=\"aut\"), person(\"Rich\", \"FitzJohn\", role=\"aut\"), person(\"Michael\", \"Friendly\", role=\"aut\"),\n#>               person(\"Brendan\",\"Furneaux\", role=\"aut\"), person(\"Geoffrey\", \"Hannigan\", role=\"aut\"), person(\"Mark O.\", \"Hill\",\n#>               role=\"aut\"), person(\"Leo\", \"Lahti\", role=\"aut\"), person(\"Dan\", \"McGlinn\", role=\"aut\"), person(\"Marie-Helene\",\n#>               \"Ouellette\", role=\"aut\"), person(\"Eduardo\", \"Ribeiro Cunha\", role=\"aut\"), person(\"Tyler\", \"Smith\", role=\"aut\"),\n#>               person(\"Adrian\", \"Stier\", role=\"aut\"), person(\"Cajo J.F.\", \"Ter Braak\", role=\"aut\"), person(\"James\", \"Weedon\",\n#>               role=\"aut\"))\n#> Depends: permute (>= 0.9-0), lattice, R (>= 3.4.0)\n#> Suggests: parallel, tcltk, knitr, markdown\n#> Imports: MASS, cluster, mgcv\n#> VignetteBuilder: utils, knitr\n#> Description: Ordination methods, diversity analysis and other functions for community and vegetation ecologists.\n#> License: GPL-2\n#> BugReports: https://github.com/vegandevs/vegan/issues\n#> URL: https://github.com/vegandevs/vegan\n#> NeedsCompilation: yes\n#> Packaged: 2022-04-17 15:30:56 UTC; jarioksa\n#> Author: Jari Oksanen [aut, cre], Gavin L. Simpson [aut], F. Guillaume Blanchet [aut], Roeland Kindt [aut], Pierre Legendre [aut],\n#>               Peter R. Minchin [aut], R.B. O'Hara [aut], Peter Solymos [aut], M. Henry H. Stevens [aut], Eduard Szoecs [aut],\n#>               Helene Wagner [aut], Matt Barbour [aut], Michael Bedward [aut], Ben Bolker [aut], Daniel Borcard [aut], Gustavo\n#>               Carvalho [aut], Michael Chirico [aut], Miquel De Caceres [aut], Sebastien Durand [aut], Heloisa Beatriz Antoniazi\n#>               Evangelista [aut], Rich FitzJohn [aut], Michael Friendly [aut], Brendan Furneaux [aut], Geoffrey Hannigan [aut],\n#>               Mark O. Hill [aut], Leo Lahti [aut], Dan McGlinn [aut], Marie-Helene Ouellette [aut], Eduardo Ribeiro Cunha [aut],\n#>               Tyler Smith [aut], Adrian Stier [aut], Cajo J.F. Ter Braak [aut], James Weedon [aut]\n#> Maintainer: Jari Oksanen <jhoksane@gmail.com>\n#> Repository: CRAN\n#> Date/Publication: 2022-04-17 17:00:02 UTC\n#> Built: R 4.2.0; x86_64-pc-linux-gnu; 2022-05-02 14:06:10 UTC; unix\n#> \n#> -- File: /home/mude/R/x86_64-pc-linux-gnu-library/4.2/vegan/Meta/package.rds\n## Carregar dados de um pacote\nlibrary(vegan)\ndata(dune)\ndune[1:6, 1:6]\n#>   Achimill Agrostol Airaprae Alopgeni Anthodor Bellpere\n#> 1        1        0        0        0        0        0\n#> 2        3        0        0        2        0        3\n#> 3        0        4        0        7        0        2\n#> 4        0        8        0        2        0        2\n#> 5        2        0        0        0        4        2\n#> 6        2        0        0        0        3        0\n## Desinstalar um pacote\nremove.packages(\"vegan\")\n\n## Descarregar um pacote\ndetach(\"package:vegan\", unload = TRUE)\n## Atualização dos pacotes\nupdate.packages(checkBuilt = TRUE, ask = FALSE)\n## Atualização dos pacotes instalados pelo GitHub\ndtupdate::github_update(auto.install = TRUE, ask = FALSE)\n## Pacote seguido da função implementada daquele pacote\ndevtools::install_github()"},{"path":"cap4.html","id":"ajuda-help","chapter":"Capítulo 4 Introdução ao R","heading":"4.3.7 Ajuda (Help)","text":"Um importante passo para melhorar usabilidade e ter mais familiaridade com linguagem R é aprender usar ajuda (help) de cada função. Para tanto, podemos utilizar função help() ou o operador ?, depois de ter carregado o pacote, para abrir uma nova aba (Figura 4.2 janela 4) que possui diversas informações sobre função de interesse. O arquivo de ajuda R possui geralmente nove ou dez tópicos, que nos auxiliam muito entendimento dos dados de entrada, argumentos e que operações estão sendo realizadas.Abaixo descrevemos esses tópicos:Description: resumo da funçãoUsage: como utilizar função e quais os seus argumentosArguments: detalha os argumentos e como os mesmos devem ser especificadosDetails: detalhes importantes para se usar funçãoValue: mostra como interpretar saída (output) da função (os resultados)Note: notas gerais sobre funçãoAuthors: autores da funçãoReferences: referências bibliográficas para os métodos usados para construção da funçãoSee also: funções relacionadasExamples: exemplos uso da função. Às vezes pode ser útil copiar esse trecho e colar R para ver como funciona e como usar função.Vamos realizar um exemplo, buscando o help da função aov(), que realiza uma análise de variância (veja detalhes Capítulo 7).Além das funções, podemos buscar detalhes de um pacote específico, para uma página simples help utilizando função help() ou o operador ?. Entretanto, para uma opção que ofereça uma descrição detalhada e um índice de todas funções pacote, podemos utilizar função library(), mas agora utilizando o argumento help, indicando o pacote de interesse entre aspas.Podemos ainda procurar o nome de uma função para realizar uma análise específica utilizando função help.search() com o termo que queremos em inglês e entre aspas.Outra ferramenta de busca é página rseek, na qual é possível buscar por um termo não só nos pacotes R, mas também em listas de emails, manuais, páginas na internet e livros sobre o programa.","code":"\n## Ajuda\nhelp(aov)\n?aov\n## Ajuda do pacote\nhelp(vegan)\n?vegan\n\n## Help detalhado\nlibrary(help = \"vegan\")\n## Procurar por funções que realizam modelos lineares\nhelp.search(\"linear models\")"},{"path":"cap4.html","id":"ambiente-environment","chapter":"Capítulo 4 Introdução ao R","heading":"4.3.8 Ambiente (Environment)","text":"O ambiente (environment), como vimos, é onde os objetos criados são armazenados. É fundamental entender que um objeto é uma alocação de um pequeno espaço na memória RAM nosso computador, onde o R armazenará um valor ou o resultado de uma função, utilizando o nome dos objetos que definimos na atribuição. Sendo assim, se fizermos atribuição de um objeto maior que o tamanho da memória RAM nosso computador, esse objeto não será alocado, e atribuição não funcionará, retornando um erro. Existem opções para contornar esse tipo de limitação, mas não abordaremos aqui. Entretanto, podemos utilizar função object.size() para saber quanto espaço nosso objeto criado está alocando de memória RAM.Podemos listar todos os objetos criados com função ls() ou objects().Podemos ainda remover todos os objetos criados com função rm() ou remove(). Ou ainda fazer uma função composta para remover todos os objetos Environment.Quando usamos função ls() agora, nenhum objeto é listado.Toda vez que fechamos o R os objetos criados são apagados Environment. Dessa forma, em algumas ocasiões, por exemplo, análises estatísticas que demoram um grande tempo para serem realizadas, pode ser interessante exportar alguns ou todos os objetos criados.Para salvar todos os objetos, ou seja, todo o Workspace, podemos ir em Session -> Save Workspace ... e escolher o nome arquivo Workspace, por exemplo, “meu_workspace.RData”. Podemos ainda utilizar funções para essas tarefas. função save.image() salva todo Workspace com extensão .RData.Depois disso, podemos fechar o RStudio tranquilamente e quando formos trabalhar novamente, podemos carregar os objetos criados indo em Session -> Load Workspace... ou utilizando função load().Entretanto, em algumas ocasiões, não precisamos salvar todos os objetos. Dessa forma, podemos salvar apenas alguns objetos específicos usando função save(), também com extensão .RData.Ou ainda, podemos salvar apenas um objeto com extensão .rds. Para isso, usamos funções saveRDS() e readRDS(), para exportar e importar esses dados, respectivamente. É importante ressaltar que nesse formato .rds, apenas um objeto é salvo por arquivo criado e que para que o objeto seja criado Workspace R, ele precisa ser lido e atribuído à um objeto.","code":"\n## Tamanho de um objeto\nobject.size(adi)\n#> 56 bytes\n## Listar todos os objetos\nls()\n## Remover um objeto\nrm(adi)\n\n## Remover todos os objetos criados\nrm(list = ls())\n## Listar todos os objetos\nls()\n#> character(0)\n## Salvar todo o workspace\nsave.image(file = \"meu_workspace.RData\")\n## Carregar todo o workspace\nload(\"meu_workspace.RData\")\n## Salvar apenas um objeto\nsave(obj1, file = \"meu_obj.RData\")\n\n## Salvar apenas um objeto\nsave(obj1, obj2, file = \"meus_objs.RData\")\n\n## Carregar os objetos\nload(\"meus_objs.RData\")\n## Salvar um objeto para um arquivo\nsaveRDS(obj, file = \"meu_obj.rds\")\n\n## Carregar esse objeto\nobj <- readRDS(file = \"meu_obj.rds\")"},{"path":"cap4.html","id":"citações","chapter":"Capítulo 4 Introdução ao R","heading":"4.3.9 Citações","text":"Ao utilizar o R para realizar alguma análise em nossos estudos, é fundamental citação mesmo. Para saber como citar o R em artigos, existe uma função denominada citation(), que provê um formato genérico de citação e um BibTeX para arquivos LaTeX e R Markdown.resultado dessa função, há uma mensagem muito interessante: “See also ‘citation(“pkgname”)’ citing R packages.”. Dessa forma, aconselhamos, sempre que possível, claro, citar também os pacotes utilizados nas análises para dar os devidos créditos aos desenvolvedores e desenvolvedoras das funções implementadas nos pacotes. Como exemplo, vamos ver como fica citação pacote vegan.Podemos ainda utilizar função write_bib() pacote knitr para exportar citação pacote formato .bib.","code":"\n## Citação do R\ncitation()\n#> \n#> To cite R in publications use:\n#> \n#>   R Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna,\n#>   Austria. URL https://www.R-project.org/.\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Manual{,\n#>     title = {R: A Language and Environment for Statistical Computing},\n#>     author = {{R Core Team}},\n#>     organization = {R Foundation for Statistical Computing},\n#>     address = {Vienna, Austria},\n#>     year = {2022},\n#>     url = {https://www.R-project.org/},\n#>   }\n#> \n#> We have invested a lot of time and effort in creating R, please cite it when using it for data analysis. See also\n#> 'citation(\"pkgname\")' for citing R packages.\n## Citação do pacote vegan\ncitation(\"vegan\")\n#> \n#> To cite package 'vegan' in publications use:\n#> \n#>   Oksanen J, Simpson G, Blanchet F, Kindt R, Legendre P, Minchin P, O'Hara R, Solymos P, Stevens M, Szoecs E, Wagner H, Barbour M,\n#>   Bedward M, Bolker B, Borcard D, Carvalho G, Chirico M, De Caceres M, Durand S, Evangelista H, FitzJohn R, Friendly M, Furneaux\n#>   B, Hannigan G, Hill M, Lahti L, McGlinn D, Ouellette M, Ribeiro Cunha E, Smith T, Stier A, Ter Braak C, Weedon J (2022). _vegan:\n#>   Community Ecology Package_. R package version 2.6-2, <https://CRAN.R-project.org/package=vegan>.\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Manual{,\n#>     title = {vegan: Community Ecology Package},\n#>     author = {Jari Oksanen and Gavin L. Simpson and F. Guillaume Blanchet and Roeland Kindt and Pierre Legendre and Peter R. Minchin and R.B. O'Hara and Peter Solymos and M. Henry H. Stevens and Eduard Szoecs and Helene Wagner and Matt Barbour and Michael Bedward and Ben Bolker and Daniel Borcard and Gustavo Carvalho and Michael Chirico and Miquel {De Caceres} and Sebastien Durand and Heloisa Beatriz Antoniazi Evangelista and Rich FitzJohn and Michael Friendly and Brendan Furneaux and Geoffrey Hannigan and Mark O. Hill and Leo Lahti and Dan McGlinn and Marie-Helene Ouellette and Eduardo {Ribeiro Cunha} and Tyler Smith and Adrian Stier and Cajo J.F. {Ter Braak} and James Weedon},\n#>     year = {2022},\n#>     note = {R package version 2.6-2},\n#>     url = {https://CRAN.R-project.org/package=vegan},\n#>   }\n## Exportar uma citação em formato .bib\nknitr::write_bib(\"vegan\", file = \"vegan_ex.bib\")"},{"path":"cap4.html","id":"principais-erros-de-iniciantes","chapter":"Capítulo 4 Introdução ao R","heading":"4.3.10 Principais erros de iniciantes","text":"Errar quando se está começando usar o R é muito comum e faz parte aprendizado. Entretanto, os erros nunca devem ser encarados como uma forma de desestímulo, mas sim como um desafio para continuar tentando. Todos nós, autores deste livro inclusive, e provavelmente usuários mais ou menos experientes, já passaram por um momento em que se quer desistir de tudo. Jovem aprendiz de R, única diferença entre você que está iniciando agora e nós que usamos o R há mais tempo são horas mais de uso (e ódio). O que temos mais é experiência para olhar o erro, lê-lo e conseguir interpretar o que está errado e saber buscar ajuda.Dessa forma, o ponto mais importante de quem está iniciando é ter paciência, calma, bom humor, ler e entender mensagens de erros. Recomendamos uma prática que pode ajudar: caso não esteja conseguindo resolver alguma parte seu código, deixe ele de lado um tempo, descanse, faça uma caminhada, tome um banho, converse com seus animais de estimação ou plantas, tenha um pato de borracha ou outro objeto inanimado (um dos autores tem um sapinho de madeira), explique esse código para esse pato (processo conhecido como Debug com Pato de Borracha), logo solução deve aparecer.Listaremos aqui o que consideramos os principais erros dos iniciantes R.1. Esquecer de completar uma função ou bloco de códigosEsquecer de completar uma função ou bloco de códigos é algo bem comum. Geralmente esquecemos de fechar aspas \"\" ou parênteses (), mas geralmente o R nos informa isso, indicando um símbolo de + console. Se você cometeu esse erro, lembre-se de apertar tecla esc seu computador clicando antes com o cursor mouse console R.2. Esquecer de vírgulas dentro de funçõesOutro erro bastante comum é esquecer de acrescentar vírgula , para separar argumentos dentro de uma função, principalmente se estamos compondo várias funções acopladas, .e., uma função dentro da outra.3. Chamar um objeto pelo nome erradoPode parecer simples, mas esse é de longe o erro mais comum que pessoas iniciantes comentem. Quando temos um script longo, é de se esperar que tenhamos atribuído diversos objetos e em algum momento atribuímos um nome qual não lembramos. Dessa forma, quando chamamos o objeto ele não existe e o console informa um erro. Entretanto, esse tipo de erro pode ser facilmente identificado, como o exemplo abaixo.4. Esquecer de carregar um pacoteEsse também é um erro recorrente, mesmo para usuários mais experientes. Em scripts de análises complexas, que requerem vários pacotes, geralmente esquecemos de um ou outro pacote. melhor forma de evitar esse tipo de erro é listar os pacotes que vamos precisar usar logo início script.Geralmente mensagem de erro será de que função não foi encontrada ou algo nesse sentido. Carregando o pacote, esse erro é contornado.5. Usar o nome da função de forma errôneaEsse erro não é tão comum, mas pode ser incômodo às vezes. Algumas funções possuem nomes padrão “Camel Case”, .e., com letras maiúsculas meio nome da função. Isso às vezes pode confundir, ou ainda, funções podem ou não ser separadas com ., como row.names() e rownames(). Capítulo 5 sobre tidyverse, veremos que houve uma tentativa de padronização nos nomes das funções para “Snake Case”, .e, todas funções possuem letras minúsculas, com palavras separadas por underscore _.6. Atentar para o diretório corretoMuitas vezes o erro é simplesmente porque o usuário() não definiu o diretório correto onde está o arquivo ser importado ou exportado. Por isso é fundamental sempre verificar se o diretório foi definido corretamente, geralmente usando funções dir() ou list.files() para listar console lista de arquivos diretório. Podemos ainda usar o argumento pattern para listar arquivos por um padrão textual.Além disso, é fundamental ressaltar importância de verificar se o nome arquivo que importaremos foi digitado corretamente, atentando-se também para extensão: .csv, .txt, .xlsx, etc.","code":"sum(1, 2\n  +\n#> Error: <text>:3:0: unexpected end of input\n#> 1: sum(1, 2\n#> 2:   +\n#>   ^sum(1 2)\n#> Error: <text>:1:7: unexpected numeric constant\n#> 1: sum(1 2\n#>           ^\nobj <- 10\nOBJ\n#> Error in eval(expr, envir, enclos): object 'OBJ' not found\n## Carregar dados\ndata(dune)\n\n## Função do pacote vegan\ndecostand(dune[1:6, 1:6], \"hell\")\n#> Error in decostand(dune[1:6, 1:6], \"hell\"): could not find function \"decostand\"\n## Carregar o pacote\nlibrary(vegan)\n\n## Carregar dados\ndata(dune)\n\n## Função do pacote vegan\ndecostand(dune[1:6, 1:6], \"hell\")\n#>    Achimill  Agrostol Airaprae  Alopgeni  Anthodor  Bellpere\n#> 1 1.0000000 0.0000000        0 0.0000000 0.0000000 0.0000000\n#> 2 0.6123724 0.0000000        0 0.5000000 0.0000000 0.6123724\n#> 3 0.0000000 0.5547002        0 0.7337994 0.0000000 0.3922323\n#> 4 0.0000000 0.8164966        0 0.4082483 0.0000000 0.4082483\n#> 5 0.5000000 0.0000000        0 0.0000000 0.7071068 0.5000000\n#> 6 0.6324555 0.0000000        0 0.0000000 0.7745967 0.0000000\n## Soma das colunas\ncolsums(dune)\n#> Error in colsums(dune): could not find function \"colsums\"\n## Soma das colunas\ncolSums(dune)\n#> Achimill Agrostol Airaprae Alopgeni Anthodor Bellpere Bromhord Chenalbu Cirsarve Comapalu Eleopalu Elymrepe Empenigr Hyporadi Juncarti Juncbufo \n#>       16       48        5       36       21       13       15        1        2        4       25       26        2        9       18       13 \n#> Lolipere Planlanc  Poaprat  Poatriv Ranuflam Rumeacet Sagiproc Salirepe Scorautu Trifprat Trifrepe Vicilath Bracruta Callcusp \n#>       58       26       48       63       14       18       20       11       54        9       47        4       49       10\n## Listar os arquivos do diretório definido\ndir()\nlist.files()\n\n## Listar os arquivos do diretório definido por um padrão\ndir(pattern = \".csv\")"},{"path":"cap4.html","id":"estrutura-e-manipulação-de-objetos","chapter":"Capítulo 4 Introdução ao R","heading":"4.4 Estrutura e manipulação de objetos","text":"O conhecimento sobre estrutura e manipulação de objetos é fundamental para ter domínio e entendimento funcionamento da linguagem R. Nesta seção, trataremos da estrutura e manipulação de dados R, que ficou conhecido como modo R Base, em contrapartida ao tidyverse, tópico tratado Capítulo 5. Abordaremos aqui temas chaves, como: ) atributos de objetos, ii) manipulação de objetos unidimensionais e multidimensionais, iii) valores faltantes e especiais, iv) diretório de trabalho e v) importar, conferir e exportar dados tabulares.","code":""},{"path":"cap4.html","id":"atributo-dos-objetos","chapter":"Capítulo 4 Introdução ao R","heading":"4.4.1 Atributo dos objetos","text":"Quando fazemos atribuições de dados R (<-), os objetos gerados possuem três características.Nome: palavra que o R reconhece os dados atribuídosConteúdo: dados em siAtributos: modos (natureza) e estruturas (organização) dos elementosVamos explorar mais fundo os modos e estruturas dos objetos. Vale ressaltar que isso é uma simplificação, pois há muitas classes de objetos, como funções e saídas de funções que possuem outros atributos.Podemos verificar os atributos dos objetos com função attributes().Modo dos objetosA depender da natureza dos elementos que compõem os dados e que foram atribuídos aos objetos, esses objetos podem ser, de forma simples um dos cinco modos: numérico tipo inteiro (integer), numérico tipo flutuante (double), texto (character), lógico (logical) ou complexo (complex).atribuição de números R pode gerar dois tipos de modos: integer para números inteiros e double para números flutuantes ou com decimais.título de praticidade, ambos são incorporados como o modo numeric, com o tipo double, menos que especifiquemos que seja inteiro com letra L depois número, representando palavra Larger, geralmente usando para armazenar números muito grandes.Além de números, podemos atribuir textos, utilizando para isso aspas \"\".Em algumas situações, precisamos indicar ocorrência ou não de um evento ou uma operação. Para isso, utilizamos palavras reservadas (TRUE e FALSE), chamadas de variáveis booleanas, pois assumem apenas duas possibilidades: falso (0) ou verdadeiro (1). Devemos nos ater para o fato dessas palavras serem escritas com letras maiúsculas e sem aspas.Por fim, existe um modo pouco utilizado que cria números complexos (raiz de números negativos).Podemos verificar o modo dos objetos ou fazer conversão entre esses modos com diversas funções.Estrutura dos objetosUma vez entendido natureza dos modos dos elementos dos objetos R, podemos passar para o passo seguinte e entender como esses elementos são estruturados dentro dos objetos.Essa estruturação irá nos contar sobre organização dos elementos, com relação aos modos e dimensionalidade da disposição desses elementos (Figura 4.3). De modo bem simples, os elementos podem ser estruturados em cinco tipos:Vetores e fatores: homogêneo (um modo) e unidimensional (uma dimensão). Um tipo especial de vetor são os fatores, usados para designar variáveis categóricasMatrizes: homogêneo (um modo) e bidimensional (duas dimensões)Arrays: homogêneo (um modo) e multidimensional (mais de duas dimensões)Data frames: heterogêneo (mais de um modo) e bidimensional (duas dimensões)Listas: heterogêneo (mais de um modo) e unidimensional (uma dimensão)\nFigura 4.3: Estruturas de dados mais comuns R: vetores, matrizes, arrays, listas e data frames. Adaptado de: Grolemund (2014).\nVetorVetores representam o encadeamento de elementos numa sequência unidimensional. Capítulo 2 na Figura 2.2, vimos o conceito de variável aleatória e seus tipos. R, essas variáveis podem ser operacionalizadas como vetores. Dessa forma, essa estrutura de dados pode ser traduzida como medidas de uma variável numérica (discretas ou contínuas), variável binária (booleana - TRUE e FALSE) ou descrição (informações em texto).Há diversas formas de se criar um vetor R:Concatenando elementos com função c()Criando sequências unitárias : ou com função seq()Criando repetições com função rep()“Colar” palavras com uma sequência numérica com função paste() ou paste0()Amostrando aleatoriamente elementos com função sample()Como os vetores são homogêneos, .e., só comportam um modo, quando combinamos mais de um modo mesmo objeto ocorre uma dominância de modos. Existe, dessa forma, uma coerção dos elementos combinados para que todos fiquem iguais. Essa dominância segue essa ordem:DOMINANTE character > double > integer > logical RECESSIVOAlém disso, podemos utilizar conversões listadas anteriormente para alterar os modos. Vamos exemplificar combinando os vetores criados anteriormente e convertendo-os.FatorO fator representa medidas de uma variável categórica, podendo ser nominal ou ordinal. É fundamental destacar que fatores R devem ser entendidos como um vetor de integer, .e., ele é composto por números inteiros representando os níveis da variável categórica.Para criar um fator R usamos uma função específica factor(), na qual podemos especificar os níveis com o argumento level, ou fazemos uma conversão usando função .factor(). Trabalhar com fatores R Base não é das tarefas mais agradáveis, sendo assim, Capítulo 5 usamos versão tidyverse usando o pacote forcats. Destacamos ainda existência de fatores nominais para variáveis categóricas nominais e fatores ordinais para variáveis categóricas ordinais, quando há ordenamento entre os níveis, como dias da semana ou classes de altura.MatrizA matriz representa dados formato de tabela, com linhas e colunas. linhas geralmente representam unidades amostrais (locais, transectos, parcelas) e colunas representam variáveis numéricas (discretas ou contínuas), variáveis binárias (TRUE ou FALSE) ou descrições (informações em texto).Podemos criar matrizes R de duas formas. primeira delas dispondo elementos de um vetor em um certo número de linhas e colunas com função matrix(), podendo preencher essa matriz com os elementos vetor por linhas ou por colunas alterando o argumento byrow.segundo forma, podemos combinar vetores, utilizando função rbind() para combinar vetores por linha, .e., um vetor embaixo outro, e cbind() para combinar vetores por coluna, .e., um vetor ao lado outro.ArrayO array representa combinação de tabelas, com linhas, colunas e dimensões. Essa combinação pode ser feita em múltiplas dimensões, mas apesar disso, geralmente é mais comum o uso em Ecologia para três dimensões, por exemplo: linhas (unidades amostrais), colunas (espécies) e dimensão (tempo). Isso gera um “cubo mágico” ou “cartas de um baralho”, onde podemos comparar, nesse caso, comunidades ao longo tempo. Além disso, arrays também são muito comuns em morfometria geométrica ou sensoriamento remoto.Podemos criar arrays R dispondo elementos de um vetor em um certo número de linhas, colunas e dimensões com função array(). Em nosso exemplo, vamos compor cinco comunidades de cinco espécies ao longo de três períodos.Data frameO data frame também representa dados formato de tabela, com linhas e colunas, muito semelhante à matriz. Mas diferentemente das matrizes, os data frames comportam mais de um modo em suas colunas. Dessa forma, linhas data frame ainda representam unidades amostrais (locais, transectos, parcelas), mas colunas agora podem representar descrições (informações em texto), variáveis numéricas (discretas ou contínuas), variáveis binárias (TRUE ou FALSE) e variáveis categóricas (nominais ou ordinais).forma mais simples de se criar data frames R é através da combinação de vetores. Essa combinação é feita com função data.frame() e ocorre de forma horizontal, semelhante à função cbind(). Sendo assim, todos os vetores precisam ter o mesmo número de elementos, ou seja, o mesmo comprimento. Podemos ainda nomear colunas de cada vetor. Outra forma, seria converter uma matriz em um data frame, utilizando função .data.frame().ListaA lista é um tipo especial de vetor que aceita objetos como elementos. Ela é estrutura de dados utilizada para agrupar objetos, e é geralmente saída de muitas funções.Podemos criar listas através da função list(). Essa função funciona de forma semelhante à função c() para criação de vetores, mas agora estamos concatenando objetos. Podemos ainda nomear os elementos (objetos) que estamos combinando.Um ponto interessante para entender data frames, é que eles são listas, em que todos os elementos (colunas) possuem o mesmo número de elementos, ou seja, mesmo comprimento.FunçõesUma última estrutura de objetos criados R são funções. Elas são objetos criados pelo usuário e reutilizados para fazer operações específicas. criação de funções geralmente é um tópico tratado num segundo momento, quando o usuário de R adquire certo conhecimento da linguagem. Aqui abordaremos apenas seu funcionamento básico, diferenciando sua estrutura para entendimento e sua diferenciação das demais estruturas.Vamos criar uma função simples que retorna multiplicação de dois termos. Criaremos função com o nome multi, à qual será atribuída uma função com o nome function(), com dois argumentos x e y. Depois disso abrimos chaves {}, que é onde iremos incluir nosso bloco de código. Nosso bloco de código é composto por duas linhas, primeira contendo operação de multiplicação dos argumento com atribuição ao objeto mu e segunda contendo função return() para retornar o valor da multiplicação.","code":"\n## Atributos\nattributes(dune)\n#> $names\n#>  [1] \"Achimill\" \"Agrostol\" \"Airaprae\" \"Alopgeni\" \"Anthodor\" \"Bellpere\" \"Bromhord\" \"Chenalbu\" \"Cirsarve\" \"Comapalu\" \"Eleopalu\" \"Elymrepe\"\n#> [13] \"Empenigr\" \"Hyporadi\" \"Juncarti\" \"Juncbufo\" \"Lolipere\" \"Planlanc\" \"Poaprat\"  \"Poatriv\"  \"Ranuflam\" \"Rumeacet\" \"Sagiproc\" \"Salirepe\"\n#> [25] \"Scorautu\" \"Trifprat\" \"Trifrepe\" \"Vicilath\" \"Bracruta\" \"Callcusp\"\n#> \n#> $row.names\n#>  [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\" \"16\" \"17\" \"18\" \"19\" \"20\"\n#> \n#> $class\n#> [1] \"data.frame\"\n## Numérico double\nobj_numerico_double <- 1\n\n## Modo\nmode(obj_numerico_double)\n#> [1] \"numeric\"\n\n## Tipo\ntypeof(obj_numerico_double)\n#> [1] \"double\"\n## Numérico integer\nobj_numerico_inteiro <- 1L\n\n## Modo\nmode(obj_numerico_inteiro)\n#> [1] \"numeric\"\n\n## Tipo\ntypeof(obj_numerico_inteiro)\n#> [1] \"integer\"\n## Caracter ou string\nobj_caracter <- \"a\" # atencao para as aspas\n\n## Modo\nmode(obj_caracter)\n#> [1] \"character\"\n## Lógico\nobj_logico <- TRUE # maiusculas e sem aspas\n\n## Modo\nmode(obj_logico)\n#> [1] \"logical\"\n## Complexo\nobj_complexo <- 1+1i\n\n## Modo\nmode(obj_complexo)\n#> [1] \"complex\"\n## Verificar o modo dos objetos\nis.numeric()\nis.integer()\nis.character()\nis.logical()\nis.complex()\n\n## Conversões entre modos\nas.numeric()\nas.integer()\nas.character()\nas.logical()\nas.complex()\n\n## Exemplo\nnum <- 1:5\nnum\nmode(num)\n\ncha <- as.character(num)\ncha\nmode(cha)\n## Concatenar elementos numéricos\nconcatenar <- c(15, 18, 20, 22, 18)\nconcatenar\n#> [1] 15 18 20 22 18\n\n## Sequência unitária (x1:x2)\nsequencia <- 1:10\nsequencia\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n\n## Sequência com diferentes espaçamentos \nsequencia_esp <- seq(from = 0, to = 100, by = 10) \nsequencia_esp\n#>  [1]   0  10  20  30  40  50  60  70  80  90 100\n\n## Repetição\nrepeticao <- rep(x = c(TRUE, FALSE), times = 5)\nrepeticao\n#>  [1]  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n\n## Cola palavra e sequência numérica\ncolar <- paste(\"amostra\", 1:5)\ncolar\n#> [1] \"amostra 1\" \"amostra 2\" \"amostra 3\" \"amostra 4\" \"amostra 5\"\n\n## Amostragem aleatória\namostragem <- sample(x = 1:100, size = 10)\namostragem\n#>  [1] 45 23 76 63 47 31 68 73 69  5\n## Coerção\nc(colar, amostragem)\n#>  [1] \"amostra 1\" \"amostra 2\" \"amostra 3\" \"amostra 4\" \"amostra 5\" \"45\"        \"23\"        \"76\"        \"63\"        \"47\"        \"31\"       \n#> [12] \"68\"        \"73\"        \"69\"        \"5\"\n\n## Conversão\nas.numeric(repeticao)\n#>  [1] 1 0 1 0 1 0 1 0 1 0\n## Fator nominal\nfator_nominal <- factor(x = sample(x = c(\"floresta\", \"pastagem\", \"cerrado\"), \n                                   size = 20, replace = TRUE),\n                        levels = c(\"floresta\", \"pastagem\", \"cerrado\"))\nfator_nominal\n#>  [1] cerrado  cerrado  floresta pastagem pastagem cerrado  cerrado  pastagem cerrado  floresta floresta floresta pastagem pastagem cerrado \n#> [16] cerrado  pastagem cerrado  floresta pastagem\n#> Levels: floresta pastagem cerrado\n\n## Fator ordinal\nfator_ordinal <- factor(x = sample(x = c(\"baixa\", \"media\", \"alta\"), \n                                   size = 20, replace = TRUE),\n                        levels = c(\"baixa\", \"media\", \"alta\"), ordered = TRUE)\nfator_ordinal\n#>  [1] alta  alta  baixa media baixa media alta  media baixa media baixa media alta  baixa media media alta  media baixa baixa\n#> Levels: baixa < media < alta\n\n## Conversão\nfator <- as.factor(x = sample(x = c(\"floresta\", \"pastagem\", \"cerrado\"), \n                              size = 20, replace = TRUE))\nfator\n#>  [1] cerrado  pastagem floresta floresta cerrado  cerrado  pastagem cerrado  pastagem cerrado  pastagem cerrado  pastagem pastagem floresta\n#> [16] cerrado  pastagem pastagem cerrado  floresta\n#> Levels: cerrado floresta pastagem\n## Vetor\nve <- 1:12\n\n## Matrix - preenchimento por linhas - horizontal\nma_row <- matrix(data = ve, nrow = 4, ncol = 3, byrow = TRUE)\nma_row\n#>      [,1] [,2] [,3]\n#> [1,]    1    2    3\n#> [2,]    4    5    6\n#> [3,]    7    8    9\n#> [4,]   10   11   12\n\n## Matrix - preenchimento por colunas - vertical\nma_col <- matrix(data = ve, nrow = 4, ncol = 3, byrow = FALSE)\nma_col\n#>      [,1] [,2] [,3]\n#> [1,]    1    5    9\n#> [2,]    2    6   10\n#> [3,]    3    7   11\n#> [4,]    4    8   12\n## Criar dois vetores\nvec_1 <- c(1, 2, 3)\nvec_2 <- c(4, 5, 6)\n\n## Combinar por linhas - vertical - um embaixo do outro\nma_rbind <- rbind(vec_1, vec_2)\nma_rbind\n#>       [,1] [,2] [,3]\n#> vec_1    1    2    3\n#> vec_2    4    5    6\n\n## Combinar por colunas - horizontal - um ao lado do outro\nma_cbind <- cbind(vec_1, vec_2)\nma_cbind\n#>      vec_1 vec_2\n#> [1,]     1     4\n#> [2,]     2     5\n#> [3,]     3     6\n## Array\nar <- array(data = sample(x = c(0, 1), size = 75, rep = TRUE), \n            dim = c(5, 5, 3))\nar\n#> , , 1\n#> \n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]    1    0    1    1    0\n#> [2,]    1    0    1    1    0\n#> [3,]    1    1    1    1    1\n#> [4,]    0    1    0    1    0\n#> [5,]    1    0    1    0    0\n#> \n#> , , 2\n#> \n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]    0    0    0    1    0\n#> [2,]    0    1    0    1    0\n#> [3,]    1    1    1    1    0\n#> [4,]    0    0    0    0    1\n#> [5,]    0    0    0    0    0\n#> \n#> , , 3\n#> \n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]    0    1    1    1    1\n#> [2,]    0    0    0    0    0\n#> [3,]    0    1    1    1    1\n#> [4,]    0    0    1    0    0\n#> [5,]    0    1    0    1    0\n## Criar três vetores\nvec_ch <- c(\"sp1\", \"sp2\", \"sp3\")\nvec_nu <- c(4, 5, 6)\nvec_fa <- factor(c(\"campo\", \"floresta\", \"floresta\"))\n\n## Data frame - combinar por colunas - horizontal - um ao lado do outro\ndf <- data.frame(vec_ch, vec_nu, vec_fa)\n\ndf\n#>   vec_ch vec_nu   vec_fa\n#> 1    sp1      4    campo\n#> 2    sp2      5 floresta\n#> 3    sp3      6 floresta\n\n## Data frame - nomear as colunas\ndf <- data.frame(especies = vec_ch, \n                 abundancia = vec_nu, \n                 vegetacao = vec_fa)\ndf\n#>   especies abundancia vegetacao\n#> 1      sp1          4     campo\n#> 2      sp2          5  floresta\n#> 3      sp3          6  floresta\n\n## Data frame - converter uma matriz\nma <- matrix(data = ve, nrow = 4, ncol = 3, byrow = TRUE)\nma\n#>      [,1] [,2] [,3]\n#> [1,]    1    2    3\n#> [2,]    4    5    6\n#> [3,]    7    8    9\n#> [4,]   10   11   12\n\ndf_ma <- as.data.frame(ma)\ndf_ma\n#>   V1 V2 V3\n#> 1  1  2  3\n#> 2  4  5  6\n#> 3  7  8  9\n#> 4 10 11 12\n## Lista\nlista <- list(rep(1, 20), # vector\n              factor(1, 1), # factor\n              cbind(c(1, 2), c(1, 2))) # matrix\nlista\n#> [[1]]\n#>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#> \n#> [[2]]\n#> [1] 1\n#> Levels: 1\n#> \n#> [[3]]\n#>      [,1] [,2]\n#> [1,]    1    1\n#> [2,]    2    2\n\n## Lista - nomear os elementos\nlista_nome <- list(vector = rep(1, 20), # vector\n              factor = factor(1, 1), # factor\n              matrix = cbind(c(1, 2), c(1, 2))) # matrix\nlista_nome\n#> $vector\n#>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#> \n#> $factor\n#> [1] 1\n#> Levels: 1\n#> \n#> $matrix\n#>      [,1] [,2]\n#> [1,]    1    1\n#> [2,]    2    2\n## Criar uma função\nmulti <- function(x, y){\n  \n  mu <- (x * y)\n  return(mu)\n \n}\nmulti\n#> function(x, y){\n#>   \n#>   mu <- (x * y)\n#>   return(mu)\n#>  \n#> }\n\n## Uso da função\nmulti(42, 23)\n#> [1] 966"},{"path":"cap4.html","id":"manipulação-de-objetos-unidimensionais","chapter":"Capítulo 4 Introdução ao R","heading":"4.4.2 Manipulação de objetos unidimensionais","text":"Vamos agora explorar formas de manipular elementos de objetos unidimensionais, ou seja, vetores, fatores e listas.primeira forma de manipulação é através da indexação, utilizando os operadores []. Com indexação podemos acessar elementos de vetores e fatores por sua posição. Utilizaremos números, sequência de números ou operações booleanas para retornar partes dos vetores ou fatores. Podemos ainda retirar elementos dessas estruturas com o operador aritmético -.exemplo seguir, iremos fixar o ponto de partida da amostragem da função sample(), utilizando função set.seed(42) (usamos 42 porque é resposta para vida, o universo e tudo mais - O Guia Mochileiro das Galáxias, mas poderia ser outro número qualquer). Isso permite que o resultado da amostragem aleatória seja igual em diferentes computadores.Podemos ainda fazer uma seleção condicional vetor. Ao utilizarmos operadores relacionais, teremos como resposta um vetor lógico. Esse vetor lógico pode ser utilizado dentro da indexação para seleção de elementos.Além da indexação, temos algumas funções que nos auxiliam em algumas operações com objetos unidimensionais, listadas na Tabela 4.2.Tabela 4.2: Funções para verificação e resumo de dados unidimensionais.Para listas, também podemos usar indexação [] para acessar ou retirar elementos.Podemos ainda usar indexação dupla [[]] para acessar os valores desses elementos.Para listas nomeadas, podemos ainda utilizar o operador $ para acessar elementos pelo seu nome.E ainda podemos utilizar funções para medir o comprimento dessa lista, listar os nomes dos elementos ou ainda renomear os elementos: length() e names().","code":"\n## Fixar a amostragem\nset.seed(42)\n\n## Amostrar 10 elementos de uma sequência\nve <- sample(x = seq(0, 2, .05), size = 10)\nve\n#>  [1] 1.80 0.00 1.20 0.45 1.75 0.85 1.15 0.30 1.90 0.20\n\n## Seleciona o quinto elemento\nve[5]\n#> [1] 1.75\n\n## Seleciona os elementos de 1 a 5\nve[1:5]\n#> [1] 1.80 0.00 1.20 0.45 1.75\n\n## Retira o décimo elemento\nve[-10]\n#> [1] 1.80 0.00 1.20 0.45 1.75 0.85 1.15 0.30 1.90\n\n## Retira os elementos 2 a 9\nve[-(2:9)]\n#> [1] 1.8 0.2\n## Quais valores sao maiores que 1?\nve > 1\n#>  [1]  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n\n## Selecionar os valores acima de 1 no vetor ve\nve[ve > 1]\n#> [1] 1.80 1.20 1.75 1.15 1.90\n## Lista\nli <- list(elem1 = 1, elem2 = 2, elem3 = 3)\n\n## Acessar o primeiro elemento\nli[1]\n#> $elem1\n#> [1] 1\n\n## Retirar o primeiro elemento\nli[-1]\n#> $elem2\n#> [1] 2\n#> \n#> $elem3\n#> [1] 3\n## Acessar o valor do primeiro elemento\nli[[1]]\n#> [1] 1\n\n## Acessar o valor do segundo elemento\nli[[2]]\n#> [1] 2\n## Acessar o primeiro elemento\nli$elem1\n#> [1] 1\n## Comprimento\nlength(li)\n#> [1] 3\n\n## Nomes\nnames(li)\n#> [1] \"elem1\" \"elem2\" \"elem3\"\n\n## Renomear\nnames(li) <- paste0(\"elemento0\", 1:3)\nli\n#> $elemento01\n#> [1] 1\n#> \n#> $elemento02\n#> [1] 2\n#> \n#> $elemento03\n#> [1] 3"},{"path":"cap4.html","id":"manipulação-de-objetos-multidimensionais","chapter":"Capítulo 4 Introdução ao R","heading":"4.4.3 Manipulação de objetos multidimensionais","text":"Da mesma forma que para objetos unidimensionais, podemos manipular elementos de objetos multidimensionais, ou seja, matrizes, data frames e arrays.Novamente, primeira forma de manipulação é através da indexação, utilizando os operadores []. Com indexação podemos acessar elementos de matrizes, data frames e arrays por sua posição. Podemos ainda retirar elementos dessas estruturas com o operador aritmético -.Entretanto, agora temos mais de uma dimensão na estruturação dos elementos dentro dos objetos. Assim, utilizamos números, sequência de números ou operação booleanas para retornar partes desses objetos, mas dimensões têm de ser explicitadas e separadas por vírgulas para acessar linhas e colunas. Essa indexação funciona para matrizes e data frames. Para arrays, especificamos também dimensões, também separadas por vírgulas para acessar essas dimensões.Para data frames, além de utilizar números e/ou sequências de números dentro operador [] simples, podemos utilizar o operador [[]] duplo para retornar apenas os valores de uma linha ou uma coluna. Se colunas estiverem nomeadas, podemos utilizar o nome da coluna de interesse entre aspas dentro dos operadores [] (retornar coluna) e [[]] (retornar apenas os valores), assim como ainda podemos utilizar o operador $ para data frames. Essas últimas operações retornam um vetor, para o qual podemos fazer operações de vetores ou ainda atualizar o valor dessa coluna selecionada ou adicionar outra coluna.Podemos ainda fazer seleções condicionais para retornar linhas com valores que temos interesse, semelhante ao uso de filtro de uma planilha eletrônica.Além disso, há uma série de funções para conferência e manipulação de dados que listamos na Tabela 4.3.Tabela 4.3: Funções para verificação e resumo de dados multidimensionais.","code":"\n## Matriz\nma <- matrix(1:12, 4, 3)\nma\n#>      [,1] [,2] [,3]\n#> [1,]    1    5    9\n#> [2,]    2    6   10\n#> [3,]    3    7   11\n#> [4,]    4    8   12\n\n## Indexação\nma[3, ] # linha 3\n#> [1]  3  7 11\nma[, 2] # coluna 2\n#> [1] 5 6 7 8\nma[1, 2] # elemento da linha 1 e coluna 2\n#> [1] 5\nma[1, 1:2] # elementos da linha 1 e coluna 1 e 2\n#> [1] 1 5\nma[1, c(1, 3)] # elementos da linha 1 e coluna 1 e 3\n#> [1] 1 9\nma[-1, ] # retirar a linha 1\n#>      [,1] [,2] [,3]\n#> [1,]    2    6   10\n#> [2,]    3    7   11\n#> [3,]    4    8   12\nma[, -3] # retirar a coluna 3\n#>      [,1] [,2]\n#> [1,]    1    5\n#> [2,]    2    6\n#> [3,]    3    7\n#> [4,]    4    8\n## Criar três vetores\nsp <- paste(\"sp\", 1:10, sep = \"\")\nabu <- 1:10\nflo <- factor(rep(c(\"campo\", \"floresta\"), each = 5))\n\n## data frame\ndf <- data.frame(sp, abu, flo)\ndf\n#>      sp abu      flo\n#> 1   sp1   1    campo\n#> 2   sp2   2    campo\n#> 3   sp3   3    campo\n#> 4   sp4   4    campo\n#> 5   sp5   5    campo\n#> 6   sp6   6 floresta\n#> 7   sp7   7 floresta\n#> 8   sp8   8 floresta\n#> 9   sp9   9 floresta\n#> 10 sp10  10 floresta\n\n## [] - números\ndf[, 1]\n#>  [1] \"sp1\"  \"sp2\"  \"sp3\"  \"sp4\"  \"sp5\"  \"sp6\"  \"sp7\"  \"sp8\"  \"sp9\"  \"sp10\"\n\n## [] - nome das colunas - retorna coluna\ndf[\"flo\"]\n#>         flo\n#> 1     campo\n#> 2     campo\n#> 3     campo\n#> 4     campo\n#> 5     campo\n#> 6  floresta\n#> 7  floresta\n#> 8  floresta\n#> 9  floresta\n#> 10 floresta\n\n## [[]] - nome das colunas - retorna apenas os valores\ndf[[\"flo\"]]\n#>  [1] campo    campo    campo    campo    campo    floresta floresta floresta floresta floresta\n#> Levels: campo floresta\n\n## $ funciona apenas para data frame \ndf$sp\n#>  [1] \"sp1\"  \"sp2\"  \"sp3\"  \"sp4\"  \"sp5\"  \"sp6\"  \"sp7\"  \"sp8\"  \"sp9\"  \"sp10\"\n\n## Operação de vetors\nlength(df$abu)\n#> [1] 10\n\n## Converter colunas\ndf$abu <- as.character(df$abu)\nmode(df$abu)\n#> [1] \"character\"\n\n## Adicionar ou mudar colunas\nset.seed(42)\ndf$abu2 <- sample(x = 0:1, size = nrow(df), rep = TRUE)\ndf\n#>      sp abu      flo abu2\n#> 1   sp1   1    campo    0\n#> 2   sp2   2    campo    0\n#> 3   sp3   3    campo    0\n#> 4   sp4   4    campo    0\n#> 5   sp5   5    campo    1\n#> 6   sp6   6 floresta    1\n#> 7   sp7   7 floresta    1\n#> 8   sp8   8 floresta    1\n#> 9   sp9   9 floresta    0\n#> 10 sp10  10 floresta    1\n## Selecionar linhas de uma matriz ou data frame \ndf[df$abu > 4, ]\n#>    sp abu      flo abu2\n#> 5 sp5   5    campo    1\n#> 6 sp6   6 floresta    1\n#> 7 sp7   7 floresta    1\n#> 8 sp8   8 floresta    1\n#> 9 sp9   9 floresta    0\n\ndf[df$flo == \"floresta\", ]\n#>      sp abu      flo abu2\n#> 6   sp6   6 floresta    1\n#> 7   sp7   7 floresta    1\n#> 8   sp8   8 floresta    1\n#> 9   sp9   9 floresta    0\n#> 10 sp10  10 floresta    1"},{"path":"cap4.html","id":"valores-faltantes-e-especiais","chapter":"Capítulo 4 Introdução ao R","heading":"4.4.4 Valores faltantes e especiais","text":"Valores faltantes e especiais são valores reservados que representam dados faltantes, indefinições matemáticas, infinitos e objetos nulos.NA (Available): significa dado faltante ou indisponívelNaN (Number): representa indefinições matemáticasInf (Infinito): é um número muito grande ou um limite matemáticoNULL (Nulo): representa um objeto nulo, sendo útil para preenchimento em aplicações de programação","code":"\n## Data frame com elemento NA\ndf <- data.frame(var1 = c(1, 4, 2, NA), var2 = c(1, 4, 5, 2))\ndf\n#>   var1 var2\n#> 1    1    1\n#> 2    4    4\n#> 3    2    5\n#> 4   NA    2\n\n## Resposta booleana para elementos NA\nis.na(df)\n#>       var1  var2\n#> [1,] FALSE FALSE\n#> [2,] FALSE FALSE\n#> [3,] FALSE FALSE\n#> [4,]  TRUE FALSE\n\n## Algum elemento é NA?\nany(is.na(df))\n#> [1] TRUE\n\n## Remover as linhas com NAs\ndf_sem_na <- na.omit(df)\ndf_sem_na\n#>   var1 var2\n#> 1    1    1\n#> 2    4    4\n#> 3    2    5\n\n## Substituir NAs por 0\ndf[is.na(df)] <- 0\ndf\n#>   var1 var2\n#> 1    1    1\n#> 2    4    4\n#> 3    2    5\n#> 4    0    2\n\n## Desconsiderar os NAs em funções com o argumento rm.na = TRUE\nsum(1, 2, 3, 4, NA, na.rm = TRUE)\n#> [1] 10\n\n## NaN - not a number\n0/0\n#> [1] NaN\nlog(-1)\n#> [1] NaN\n\n## Limite matemático\n1/0\n#> [1] Inf\n\n## Número grande\n10^310\n#> [1] Inf\n\n## Objeto nulo\nnulo <- NULL\nnulo\n#> NULL"},{"path":"cap4.html","id":"diretório-de-trabalho","chapter":"Capítulo 4 Introdução ao R","heading":"4.4.5 Diretório de trabalho","text":"O diretório de trabalho é o endereço da pasta (ou diretório) de onde o R importará ou exportar nossos dados.Podemos utilizar o próprio RStudio para tal tarefa, indo em Session > Set Work Directory > Choose Directory... ou simplesmente utilizar o atalho Ctrl + Shift + H.Podemos ainda utilizar funções R para definir o diretório. Para tanto, podemos navegar com o aplicativo de gerenciador de arquivos (e.g., Windows Explorer) até nosso diretório de interesse e copiar o endereço na barra superior. Voltamos para o R e colamos esse endereço entre aspas como argumento da função setwd(). É fundamental destacar que Windows é necessário inverter barras (\\ por / ou duplicar elas \\\\).Aconselhamos ainda utilizar funções getwd() para retornar o diretório definido na sessão R, assim como funções dir() ou list.files() para listagem dos arquivos diretório, ambas medidas de conferência diretório correto.Outra forma de definir o diretório é digitar tecla tab dentro da função setwd(\"tab\"). Quando apertamos tab dentro das aspas conseguimos selecionar o diretório manualmente, pois abre-se uma lista de diretório que podemos ir selecionando até chegar diretório de interesse.","code":"\n## Definir o diretório de trabalho\nsetwd(\"/home/mude/data/github/livro_aer/dados\")\n\n## Verificar o diretório\ngetwd()\n\n## Listar os arquivos no diretório\ndir()\nlist.files()\n## Mudar o diretório com a tecla tab\nsetwd(\"`tab`\")"},{"path":"cap4.html","id":"importar-dados","chapter":"Capítulo 4 Introdução ao R","heading":"4.4.6 Importar dados","text":"Uma das operações mais corriqueiras R, antes de realizar alguma análise ou plotar um gráfico, é de importar dados que foram tabulados numa planilha eletrônica e salvos formato .csv, .txt ou .xlsx. Ao importar esse tipo de dado para o R, o formato que o mesmo assume, se nenhum parâmetro especificado, é o da classe data frame, prevendo que planilha de dados possua colunas com diferentes modos.Existem diversas formas de importar dados para o R. Podemos importar utilizando o RStudio, indo na janela Environment (Figura 4.2 janela 3) e clicar em “Importar Dataset”.Entretanto, aconselhamos o uso de funções que fiquem salvas em um script para aumentar reprodutibilidade mesmo. Dessa forma, três principais funções para importar os arquivos nos três principais extensões (.csv, .txt ou .xlsx) são, respectivamente: read.csv(), read.table() e openxlsx::read.xlsx(), sendo o último pacote openxlsx.Para exemplificar como importar dados R, vamos usar os dados de comunidades de anfíbios da Mata Atlântica (Vancine et al. 2018). Faremos o download diretamente site da fonte dos dados.Vamos antes escolher um diretório de trabalho com função setwd(), e em seguida criar um diretório com função dir.create() chamado “dados”. Em seguida, vamos mudar nosso diretório para essa pasta e criar mais um diretório chamado “tabelas”, e por fim, definir esse diretório para que o conteúdo download seja armazenado ali.Agora podemos fazer o download arquivo .zip e extrair tabelas usando função unzip() nesse mesmo diretório.Agora podemos importar tabela de dados com função read.csv(), atribuindo ao objeto intror_anfibios_locais. Devemos atentar para o argumento encoding, que selecionamos aqui como latin1 para corrigir um erro de caracteres, que o autor dos dados cometeu quando publicou esse data paper. Geralmente não precisamos especificar esse tipo de informação, mas caso depois de importar os dados e na conferência, os caracteres como acento não estiverem formatados, procure especificar argumento encoding um padrão para corrigir esse erro.Esse arquivo foi criado com separador de decimais sendo . e separador de colunas sendo ,. Caso tivesse sido criado com separador de decimais sendo , e separador de colunas sendo ;, usaríamos função read.csv2().Para outros formatos, basta usar outras funções apresentadas, atentando-se para os argumentos específicos de cada função.Outra forma de importar dados, principalmente quando não sabemos exatamente o nome arquivo e também para evitar erros de digitação, é utilizar tecla tab dentro das aspas da função de importação. Dessa forma, conseguimos ter acesso aos arquivos nosso diretório e temos possibilidade de selecioná-los sem erros de digitação.Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.","code":"\n## Escolher um diretório\nsetwd(\"/home/mude/data/github/livro_aer\")\n\n## Criar um diretório 'dados'\ndir.create(\"dados\")\n\n## Escolher diretório 'dados'\nsetwd(\"dados\")\n\n## Criar um diretório 'tabelas'\ndir.create(\"tabelas\")\n\n## Escolher diretório 'tabelas'\nsetwd(\"tabelas\")\n## Download\ndownload.file(url = \"https://esajournals.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1002%2Fecy.2392&file=ecy2392-sup-0001-DataS1.zip\",\n              destfile = \"atlantic_amphibians.zip\", mode = \"auto\")\n\n## Unzip\nunzip(zipfile = \"atlantic_amphibians.zip\")\n## Importar a tabela de locais\nintror_anfibios_locais <- read.csv(\"dados/tabelas/ATLANTIC_AMPHIBIANS_sites.csv\", encoding = \"latin1\")\n## Importar usando a tecla tab\nintror_anfibios_locais <- read.csv(\"`tab`\")\nintror_anfibios_locais\n## Importar os dados pelo pacote ecodados\nintror_anfibios_locais <- ecodados::intror_anfibios_locais\nhead(intror_anfibios_locais)\n#>        id reference_number species_number record sampled_habitat active_methods passive_methods complementary_methods      period month_start\n#> 1 amp1001             1001             19     ab           fo,ll             as              pt                  <NA> mo,da,tw,ni           9\n#> 2 amp1002             1002             16     co        fo,la,ll             as              pt                  <NA> mo,da,tw,ni          12\n#> 3 amp1003             1002             14     co        fo,la,ll             as              pt                  <NA> mo,da,tw,ni          12\n#> 4 amp1004             1002             13     co        fo,la,ll             as              pt                  <NA> mo,da,tw,ni          12\n#> 5 amp1005             1003             30     co        fo,ll,br             as            <NA>                  <NA>    mo,da,ni           7\n#> 6 amp1006             1004             42     co  tp,pp,la,ll,is           <NA>            <NA>                  <NA>        <NA>          NA\n#>   year_start month_finish year_finish effort_months country state state_abbreviation            municipality                                site\n#> 1       2000            1        2002            16  Brazil Piauí              BR-PI         Canto do Buriti Parque Nacional Serra das Confusões\n#> 2       2007            5        2009            17  Brazil Ceará              BR-CE São Gonçalo do Amarante                               Dunas\n#> 3       2007            5        2009            17  Brazil Ceará              BR-CE São Gonçalo do Amarante  Jardim Botânico Municipal de Bauru\n#> 4       2007            5        2009            17  Brazil Ceará              BR-CE São Gonçalo do Amarante                               Taíba\n#> 5       1988            8        2001           157  Brazil Ceará              BR-CE                Baturité                   Serra de Baturité\n#> 6         NA           NA          NA            NA  Brazil Ceará              BR-CE             Quebrangulo  Reserva Biológica de Pedra Talhada\n#>    latitude longitude coordinate_precision altitude temperature precipitation\n#> 1 -8.680000 -43.42194                   gm      543       24.98           853\n#> 2 -3.545527 -38.85783                   dd       15       26.53          1318\n#> 3 -3.574194 -38.88869                   dd       29       26.45          1248\n#> 4 -3.515250 -38.91880                   dd       25       26.55          1376\n#> 5 -4.280556 -38.91083                   gm      750       21.35          1689\n#> 6 -9.229167 -36.42806                 <NA>      745       20.45          1249"},{"path":"cap4.html","id":"conferência-dos-dados-importados","chapter":"Capítulo 4 Introdução ao R","heading":"4.4.7 Conferência dos dados importados","text":"Uma vez importados os dados para o R, geralmente antes de iniciarmos qualquer manipulação, visualização ou análise de dados, fazemos conferência desses dados. Para isso, podemos utilizar funções listadas na Tabela 4.3.Dentre todas funções de verificação, destacamos importância destas funções apresentadas abaixo para saber se variáveis foram importadas e interpretadas corretamente e reconhecer erros de digitação, por exemplo. 📝 Importante \nfunção na.omit() retira linha inteira que possui algum NA, inclusive colunas que possuem dados que você não tem interesse em excluir. Dessa forma, tenha em mente quais dados você realmente quer remover da sua tabela.Além das funções apresentadas, recomendamos olhar os seguintes pacotes que ajudam na conferência dos dados importados: Hmisc, skimr e inspectDF.","code":"\n## Primeiras linhas\nhead(intror_anfibios_locais)\n#>        id reference_number species_number record sampled_habitat active_methods passive_methods complementary_methods      period month_start\n#> 1 amp1001             1001             19     ab           fo,ll             as              pt                  <NA> mo,da,tw,ni           9\n#> 2 amp1002             1002             16     co        fo,la,ll             as              pt                  <NA> mo,da,tw,ni          12\n#> 3 amp1003             1002             14     co        fo,la,ll             as              pt                  <NA> mo,da,tw,ni          12\n#> 4 amp1004             1002             13     co        fo,la,ll             as              pt                  <NA> mo,da,tw,ni          12\n#> 5 amp1005             1003             30     co        fo,ll,br             as            <NA>                  <NA>    mo,da,ni           7\n#> 6 amp1006             1004             42     co  tp,pp,la,ll,is           <NA>            <NA>                  <NA>        <NA>          NA\n#>   year_start month_finish year_finish effort_months country state state_abbreviation            municipality                                site\n#> 1       2000            1        2002            16  Brazil Piauí              BR-PI         Canto do Buriti Parque Nacional Serra das Confusões\n#> 2       2007            5        2009            17  Brazil Ceará              BR-CE São Gonçalo do Amarante                               Dunas\n#> 3       2007            5        2009            17  Brazil Ceará              BR-CE São Gonçalo do Amarante  Jardim Botânico Municipal de Bauru\n#> 4       2007            5        2009            17  Brazil Ceará              BR-CE São Gonçalo do Amarante                               Taíba\n#> 5       1988            8        2001           157  Brazil Ceará              BR-CE                Baturité                   Serra de Baturité\n#> 6         NA           NA          NA            NA  Brazil Ceará              BR-CE             Quebrangulo  Reserva Biológica de Pedra Talhada\n#>    latitude longitude coordinate_precision altitude temperature precipitation\n#> 1 -8.680000 -43.42194                   gm      543       24.98           853\n#> 2 -3.545527 -38.85783                   dd       15       26.53          1318\n#> 3 -3.574194 -38.88869                   dd       29       26.45          1248\n#> 4 -3.515250 -38.91880                   dd       25       26.55          1376\n#> 5 -4.280556 -38.91083                   gm      750       21.35          1689\n#> 6 -9.229167 -36.42806                 <NA>      745       20.45          1249\n\n## Últimas linhas\ntail(intror_anfibios_locais)\n#>           id reference_number species_number record sampled_habitat active_methods passive_methods complementary_methods period month_start\n#> 1158 amp2158             1389              3     co            <NA>           <NA>            <NA>                  <NA>   <NA>          NA\n#> 1159 amp2159             1389              9     co            <NA>           <NA>            <NA>                  <NA>   <NA>          NA\n#> 1160 amp2160             1389              6     co            <NA>           <NA>            <NA>                  <NA>   <NA>          NA\n#> 1161 amp2161             1389              1     co            <NA>           <NA>            <NA>                  <NA>   <NA>          NA\n#> 1162 amp2162             1389              2     co            <NA>           <NA>            <NA>                  <NA>   <NA>          NA\n#> 1163 amp2163             1389              2     co            <NA>           <NA>            <NA>                  <NA>   <NA>          NA\n#>      year_start month_finish year_finish effort_months   country    state state_abbreviation    municipality                          site\n#> 1158         NA           NA          NA            NA Argentina Misiones               AR-N Manuel Belgrano          Comandante Andresito\n#> 1159         NA           NA          NA            NA Argentina Misiones               AR-N         Posadas                       Posadas\n#> 1160         NA           NA          NA            NA Argentina Misiones               AR-N      Montecarlo                    Montecarlo\n#> 1161         NA           NA          NA            NA Argentina Misiones               AR-N       San Pedro                Refugio Moconá\n#> 1162         NA           NA          NA            NA Argentina Misiones               AR-N        Cainguás Balneario Municipal Cuñá Pirú\n#> 1163         NA           NA          NA            NA Argentina Misiones               AR-N           Oberá       Chacra San Juan de Dios\n#>       latitude longitude coordinate_precision altitude temperature precipitation\n#> 1158 -25.66944 -54.04556                  gms      251       19.94          1780\n#> 1159 -27.45333 -55.89250                  gms      105       21.30          1768\n#> 1160 -26.56889 -53.60889                  gms      597       18.35          1954\n#> 1161 -27.14083 -53.92611                  gms      202       19.92          1850\n#> 1162 -27.08722 -54.95278                  gms      213       21.04          1553\n#> 1163 -27.47333 -55.17194                  gms      254       20.67          1683\n\n## Número de linhas e colunas\nnrow(intror_anfibios_locais)\n#> [1] 1163\nncol(intror_anfibios_locais)\n#> [1] 25\ndim(intror_anfibios_locais)\n#> [1] 1163   25\n## Nome das linhas e colunas\nrownames(intror_anfibios_locais)\ncolnames(intror_anfibios_locais)\n\n## Estrutura dos dados\nstr(intror_anfibios_locais)\n\n## Resumo dos dados\nsummary(intror_anfibios_locais)\n\n## Verificar NAs\nany(is.na(intror_anfibios_locais))\nwhich(is.na(intror_anfibios_locais))\n\n## Remover as linhas com NAs\nintror_anfibios_locais_na <- na.omit(intror_anfibios_locais)"},{"path":"cap4.html","id":"exportar-dados","chapter":"Capítulo 4 Introdução ao R","heading":"4.4.8 Exportar dados","text":"Uma vez realizado operações de manipulação ou tendo dados que foram analisados e armazenados num objeto formato de data frame ou matriz, podemos exportar esses dados R para o diretório que definimos anteriormente.Para tanto, podemos utilizar funções de escrita de dados, como write.csv(), write.table() e openxlsx::write.xlsx(). Dois pontos são fundamentais: ) o nome arquivo tem de estar entre aspas e final dele deve constar extensão que pretendemos que o arquivo tenha, e ii) é interessante utilizar os argumentos row.names = FALSE e quote=FALSE, para que o arquivo escrito não tenha o nome das linhas ou aspas em todas células, respectivamente.","code":"\n## Exportar dados na extensão .csv\nwrite.csv(intror_anfibios_locais_na, \"ATLANTIC_AMPHIBIAN_sites_na.csv\", \n          row.names = FALSE, quote = FALSE)\n\n## Exportar dados na extensão .txt\nwrite.table(intror_anfibios_locais_na, \"ATLANTIC_AMPHIBIAN_sites_na.txt\", \n            row.names = FALSE, quote = FALSE)\n\n## Exportar dados na extensão .xlsx\nopenxlsx::write.xlsx(intror_anfibios_locais_na, \"ATLANTIC_AMPHIBIAN_sites_na.xlsx\", \n                     row.names = FALSE, quote = FALSE)"},{"path":"cap4.html","id":"para-se-aprofundar","chapter":"Capítulo 4 Introdução ao R","heading":"4.5 Para se aprofundar","text":"Listamos seguir livros e links com material que recomendamos para seguir com sua aprendizagem em R Base.","code":""},{"path":"cap4.html","id":"livros","chapter":"Capítulo 4 Introdução ao R","heading":"4.5.1 Livros","text":"Recomendamos aos (às) interessados() os livros: ) Crawley (2012) R Book, ii) Davies (2016) Book R: First Course Programming Statistics, iii) Gillespie e Lovelace (2017) Efficient R programming, iv) Holmes e Huber (2019) Modern Statistics Modern Biology, v) Irizarry e Love (2017) Data Analysis Life Sciences R, vi) James e colaboradores (2013) Introduction Statistical Learning: Applications R, vii) Kabacoff (2011) R Action: Data analysis graphics R, viii) Matloff (2011) Art R Programming: Tour Statistical Software Design, ix) Long e Teetor (2019) R Cookbook e x) Wickham (2019) Advanced R.","code":""},{"path":"cap4.html","id":"links","chapter":"Capítulo 4 Introdução ao R","heading":"4.5.2 Links","text":"Existem centenas de ferramentas online para aprender e explorar o R. Dentre elas, indicamos os seguintes links (em português e inglês):Introdução ao RAn Introduction R - Douglas , Roos D, Mancini F, Couto , Lusseau DA () shortintroduction R - Paul Torfs & Claudia BrauerR Beginners - Emmanuel ParadisCiência de dados\n- Ciência de Dados em R - Curso-R\n- Data Science Ecologists Environmental Scientists - Coding ClubEstatística\n- Estatística Computacional com R - Mayer F. P., Bonat W. H., Zeviani W. M., Krainski E. T., Ribeiro Jr. P. J\n- Data Analysis Visualization R Ecologists - Data CarpentryMiscelâneaMateriais sobre R - Beatriz MilzR resources (free courses, books, tutorials, & cheat sheets) - Paul van der Laken","code":""},{"path":"cap4.html","id":"exercícios","chapter":"Capítulo 4 Introdução ao R","heading":"4.6 Exercícios","text":"4.1\nUse o R para verificar o resultado da operação 7 + 7 ÷ 7 + 7 x 7 - 7.4.2\nVerifique através R se 3x2³ é maior que 2x3².4.3\nCrie dois objetos (qualquer nome) com os valores 100 e 300. Multiplique esses objetos (função prod()) e atribuam ao objeto mult. Faça o logaritmo natural (função log()) objeto mult e atribuam ao objeto ln.4.4\nQuantos pacotes existem CRAN nesse momento? Execute essa combinação Console: nrow(available.packages(repos = \"http://cran.r-project.org\")).4.5\nInstale o pacote tidyverse CRAN.4.6\nEscolha números para jogar na mega-sena usando o R, nomeando o objeto como mega. Lembrando: são 6 valores de 1 60 e atribuam um objeto.4.7\nCrie um fator chamado tr, com dois níveis (“cont” e “trat”) para descrever 300 locais de amostragem, 15 de cada tratamento. O fator deve ser dessa forma cont, cont, cont, ...., cont, trat, trat, ...., trat.4.8\nCrie uma matriz chamada ma, resultante da disposição de um vetor composto por 130 valores aleatórios entre 0 e 10. matriz deve conter 100 linhas e ser disposta por colunas.4.9\nCrie um data frame chamado df, resultante da composição desses vetores:id: 1:30sp: sp01, sp02, ..., sp29, sp30ab: 30 valores aleatórios entre 0 54.10\nCrie uma lista com os objetos criados anteriormente: mega, tr, ma e df.4.11\nSelecione os elementos ímpares objeto tr e atribua ao objeto tr_impar.4.12\nSelecione linhas com ids pares objeto df e atribua ao objeto df_ids_par.4.13\nFaça uma amostragem de 10 linhas objeto df e atribua ao objeto df_amos10. Use função set.seed() para fixar amostragem.4.14\nAmostre 10 linhas objeto ma, mas utilizando linhas amostradas df_amos10 e atribua ao objeto ma_amos10.4.15\nUna colunas dos objetos df_amos10 e ma_amos10 e atribua ao objeto dados_amos10.Soluções dos exercícios.","code":""},{"path":"cap5.html","id":"cap5","chapter":"Capítulo 5 Tidyverse","heading":"Capítulo 5 Tidyverse","text":"","code":""},{"path":"cap5.html","id":"pré-requisitos-do-capítulo-1","chapter":"Capítulo 5 Tidyverse","heading":"Pré-requisitos do capítulo","text":"Pacotes e dados que serão utilizados neste capítulo.","code":"\n## Pacotes\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(forcats)\nlibrary(palmerpenguins)\nlibrary(lubridate)\n\n## Dados\npenguins <- palmerpenguins::penguins\npenguins_raw <- palmerpenguins::penguins_raw\ntidy_anfibios_locais <- ecodados::tidy_anfibios_locais"},{"path":"cap5.html","id":"contextualização-1","chapter":"Capítulo 5 Tidyverse","heading":"5.1 Contextualização","text":"Como todo idioma, linguagem R vem passando por transformações nos últimos anos. Grande parte dessas mudanças estão dentro paradigma de Ciência de Dados (Data Science), uma nova área de conhecimento que vem se moldando partir desenvolvimento da sociedade em torno da era digital e da grande quantidade de dados gerados e disponíveis pela internet, de onde advém os pilares das inovações tecnológicas: Big Data, Machine Learning e Internet Things. grande necessidade de computação para desenvolver esse novo paradigma colocaram o R e o python como principais linguagens de programação frente esses novos desafios. Apesar de não serem únicas ferramentas utilizadas para esse propósito, elas rapidamente se tornaram uma das melhores escolhas, dado vários fatores como: ser de código-aberto e gratuitas, possuir grandes comunidades contribuidoras, ser linguagens de interpretação (orientadas objeto) e relativamente fáceis de serem aprendidas e aplicadas.Essas mudanças e expansões na utilização da linguagem R para Ciência de Dados começaram ser implementadas principalmente devido um pesquisador: Hadley Wickham, que iniciou sua contribuição à comunidade R com o desenvolvimento já consagrado pacote ggplot2 (Wickham 2016) para composição de gráficos R (ver mais Capítulo 6), baseado na gramática de gráficos (Wilkinson Wills 2005). Depois disso, Wickham dedicou-se ao desenvolvimento pensamento de uma nova abordagem dentro da manipulação de dados, denominada Tidy Data (Dados organizados) (Wickham 2014), na qual focou na limpeza e organização dos mesmos. ideia postula que dados estão tidy quando: ) variáveis estão nas colunas, ii) observações estão nas linhas e iii) valores estão nas células, sendo que para esse último, não deve haver mais de um valor por célula.partir dessas ideias, o tidyverse foi operacionalizado R como uma coleção de pacotes que atuam fluxo de trabalho comum da ciência de dados: importação, manipulação, exploração, visualização, análise e comunicação de dados e análises (Wickham et al. 2019) (Figura 5.1). O principal objetivo tidyverse é aproximar linguagem para melhorar interação entre ser humano e computador sobre dados, de modo que os pacotes compartilham uma filosofia de design de alto nível e gramática, além da estrutura de dados de baixo nível (Wickham et al. 2019). principais leituras sobre o tema R são os artigos “Tidy Data” (Wickham 2014) e “Welcome Tidyverse” (Wickham et al. 2019), e o livro “R Data Science” (Wickham Grolemund 2017), além Tidyverse que possui muito mais informações.\nFigura 5.1: Modelo das ferramentas necessárias em um projeto típico de ciência de dados: importar, organizar, entender (transformar, visualizar, modelar) e comunicar, envolto à essas ferramentas está programação. Adaptado de: Wickham & Grolemund (2017).\n","code":""},{"path":"cap5.html","id":"tidyverse","chapter":"Capítulo 5 Tidyverse","heading":"5.2 tidyverse","text":"Uma vez instalado e carregado, o pacote tidyverse disponibiliza um conjunto de ferramentas através de vários pacotes. Esses pacotes compartilham uma filosofia de design, gramática e estruturas. Podemos entender o tidyverse como um “dialeto novo” para linguagem R, onde tidy quer dizer organizado, arrumado, ordenado, e verse é universo. seguir, listamos os principais pacotes e suas funcionalidades.readr: importa dados tabulares (e.g. .csv e .txt)tibble: implementa classe tibbletidyr: transformação de dados para tidydplyr: manipulação de dadosstringr: manipulação de caracteresforcats: manipulação de fatoresggplot2: possibilita visualização de dadospurrr: disponibiliza ferramentas para programação funcionalAlém dos pacotes principais, fazemos também menção outros pacotes que estão dentro dessa abordagem e que trataremos ainda neste capítulo, em outro momento livro, ou que você leitor() deve se familiarizar. Alguns pacotes compõem o tidyverse outros são mais gerais, entretanto, todos estão envolvidos de alguma forma com ciência de dados.readxl e writexl: importa e exporta dados tabulares (.xlsx)janitor: examina e limpa dados sujosDBI: interface de banco de dados Rhaven: importa e exporta dados SPSS, Stata e SAShttr: ferramentas para trabalhar com URLs e HTTPrvest: coleta facilmente (raspagem de dados) páginas da webxml2: trabalha com arquivos XMLjsonlite: um analisador e gerador JSON simples e robusto para Rhms: hora dialubridate: facilita o tratamento de datasmagrittr: provê os operadores pipe (%>%, %$%, %<>%)glue: facilita combinação de dados e caracteresrmarkdown: cria documentos de análise dinâmica que combinam código, saída renderizada (como figuras) e textoknitr: projetado para ser um mecanismo transparente para geração de relatórios dinâmicos com Rshiny: framework de aplicativo Web para Rflexdashboard: painéis interativos para Rhere: facilita definição de diretóriosusethis: automatiza tarefas durante configuração e desenvolvimento de projetos (Git, ‘GitHub’ e Projetos RStudio)data.table: pacote que fornece uma versão de alto desempenho data.frame (importar, manipular e expotar)reticulate: pacote que fornece ferramentas para integrar Python e Rsparklyr: interface R para Apache Sparkbroom: converte objetos estatísticos em tibbles organizadosmodelr: funções de modelagem que funcionam com o pipetidymodels: coleção de pacotes para modelagem e aprendizado de máquina usando os princípios tidyverseDestacamos grande expansão e aplicabilidade dos pacotes rmarkdown, knitr e bookdown, que permitiram escrita deste livro usando essas ferramentas e linguagem de marcação, chamada Markdown.Para instalar os principais pacotes que integram o tidyverse podemos instalar o pacote tidyverse.Quando carregamos o pacote tidyverse podemos notar uma mensagem indicando quais pacotes foram carregados, suas respectivas versões e os conflitos com outros pacotes.Podemos ainda listar todos os pacotes tidyverse com função tidyverse::tidyverse_packages().Também podemos verificar se os pacotes estão atualizados, senão, podemos atualizá-los com função tidyverse::tidyverse_update().Todas funções dos pacotes tidyverse usam fonte minúscula e _ (underscore) para separar os nomes internos das funções, seguindo mesma sintaxe Python (“Snake Case”). Neste sentido de padronização, é importante destacar ainda que existe um guia próprio para que os scripts sigam recomendação de padronização, o tidyverse style guide, criado pelo próprio Hadley Wickham. Para pessoas que desenvolvem funções e pacotes existe o Tidyverse design guide criado pelo Tidyverse team.Por fim, para evitar possíveis conflitos de funções com o mesmo nome entre pacotes, recomendamos fortemente o hábito de usar funções precedidas operador :: e o respectivo pacote. Assim, garante-se que função utilizada é referente ao pacote daquela função. Segue um exemplo com funções apresentadas anteriormente.Seguindo essas ideias novo paradigma da Ciência de Dados, outro conjunto de pacotes foi desenvolvido, chamado de tidymodels que atuam fluxo de trabalho da análise de dados em ciência de dados: separação e reamostragem, pré-processamento, ajuste de modelos e métricas de performasse de ajustes. Por razões de espaço e especificidade, não entraremos em detalhes desses pacotes.Seguindo estrutura da Figura 5.1, iremos ver nos itens das próximas seções como esses passos são realizados com funções de cada pacote.","code":"\n## Instalar o pacote tidyverse\ninstall.packages(\"tidyverse\")\n## Carregar o pacote tidyverse\nlibrary(tidyverse)\n#> ── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.1 ──\n#> ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n#> ✔ tibble  3.1.7     ✔ dplyr   1.0.9\n#> ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n#> ✔ readr   2.1.2     ✔ forcats 0.5.1\n#> ── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n## Listar todos os pacotes do tidyverse \ntidyverse::tidyverse_packages()\n#>  [1] \"broom\"         \"cli\"           \"crayon\"        \"dbplyr\"        \"dplyr\"         \"dtplyr\"        \"forcats\"       \"googledrive\"  \n#>  [9] \"googlesheets4\" \"ggplot2\"       \"haven\"         \"hms\"           \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n#> [17] \"modelr\"        \"pillar\"        \"purrr\"         \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"         \"rstudioapi\"   \n#> [25] \"rvest\"         \"stringr\"       \"tibble\"        \"tidyr\"         \"xml2\"          \"tidyverse\"\n## Verificar e atualizar os pacotes do tidyverse \ntidyverse::tidyverse_update(repos = \"http://cran.us.r-project.org\")\n## Funções no formato snake case\nread_csv()\nread_tsv()\nas_tibble()\nleft_join()\ngroup_by()\n## Funções seguidas de seus respectivos pacotes\nreadr::read_csv()\nreadr::read_tsv()\ntibble::as_tibble()\ndplyr::left_join()\ndplyr::group_by()"},{"path":"cap5.html","id":"here","chapter":"Capítulo 5 Tidyverse","heading":"5.3 here","text":"Dentro fluxo de trabalho tidyverse, devemos sempre trabalhar com Projetos RStudio (ver Capítulo 4). Junto com o projeto, também podemos fazer uso pacote . Ele permite construir caminhos para os arquivos projeto de forma mais simples e com maior reprodutibilidade.Esse pacote cobre o ponto de mudarmos o diretório de trabalho que discutimos Capítulo 4, dado que muitas vezes mudar o diretório com função setwd() tende ser demorado e tedioso, principalmente quando se trata de um script em que várias pessoas estão trabalhando em diferentes computadores e sistemas operacionais. Além disso, ele elimina questão da fragilidade dos scripts, pois geralmente um script está com os diretórios conectados exatamente um lugar e um momento. Por fim, ele também simplifica o trabalho com subdiretórios, facilitando importar ou exportar arquivos para subdiretórios.Seu uso é relativamente simples: uma vez criado e aberto o RStudio pelo Projeto RStudio, o diretório automaticamente é definido para o diretório projeto. Depois disso, podemos usar função ::() para definir os subdiretórios onde estão os dados. O exemplo da aplicação fica para seção seguinte, quando iremos de fato importar um arquivo tabular para o R. Logo abaixo, mostramos como instalar e carregar o pacote .","code":"\n## Instalar\ninstall.packages(\"here\")\n\n## Carregar\nlibrary(here)"},{"path":"cap5.html","id":"readr-readxl-e-writexl","chapter":"Capítulo 5 Tidyverse","heading":"5.4 readr, readxl e writexl","text":"Dado que possuímos um conjunto de dados e que geralmente esse conjunto de dados estará formato tabular com umas das extensões: .csv, .txt ou .xlsx, usaremos o pacote readr ou readxl para importar esses dados para o R. Esses pacotes leem e escrevem grandes arquivos de forma mais rápida, além de fornecerem medidores de progresso de importação e exportação, e imprimir informação dos modos das colunas momento da importação. Outro ponto bastante positivo é que também classificam automaticamente o modo dos dados de cada coluna, .e., se uma coluna possui dados numéricos ou apenas texto, essa informação será considerada para classificar o modo da coluna toda. classe objeto atribuído quando lido por esses pacotes é automaticamente um tibble, que veremos melhor na seção seguinte. Todas funções deste pacote são listadas na página de referência pacote.Usamos funções readr::read_csv() e readr::write_csv() para importar e exportar arquivos .csv R, respectivamente. Para dados com extensão .txt, podemos utilizar funções readr::read_tsv() ou ainda readr::read_delim(). Para arquivos tabulares com extensão .xlsx, temos de instalar e carregar dois pacotes adicionais: readxl e writexl, dos quais usaremos funções readxl::read_excel(), readxl::read_xlsx() ou readxl::read_xls() para importar dados, atentado para o fato de podermos indicar aba com os dados com o argumento sheet, e writexl::write_xlsx() para exportar.Se o arquivo .csv foi criado com separador de decimais sendo . e separador de colunas sendo ,, usamos funções listadas acima normalmente. Caso seja criado com separador de decimais sendo , e separador de colunas sendo ;, devemos usar função readr::read_csv2() para importar e readr::write_csv2() para exportar nesse formato, que é mais comum Brasil.Para exemplificar como essas funções funcionam, vamos importar novamente os dados de comunidades de anfíbios da Mata Atlântica (Vancine et al. 2018), que fizemos o download Capítulo 4. Estamos usando função readr::read_csv(), indicando os diretórios com função ::(), e classe arquivo é tibble. Devemos atentar para o argumento locale = readr::locale(encoding = \"latin1\"),que selecionamos aqui como latin1 para corrigir um erro de caracteres, que o autor dos dados cometeu quando publicou esse data paper.Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.Para se aprofundar tema, recomendamos leitura Capítulo 11 Data import de Wickham & Grolemund (2017).","code":"\n## Importar locais\ntidy_anfibios_locais <- readr::read_csv(\n    here::here(\"dados\", \"tabelas\", \"ATLANTIC_AMPHIBIANS_sites.csv\"),\n    locale = readr::locale(encoding = \"latin1\")\n)\n#> Rows: 1163 Columns: 25\n#> ── Column specification ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (13): id, record, sampled_habitat, active_methods, passive_methods, complementary_methods, period, country, state, state_abbreviation, mun...\n#> dbl (12): reference_number, species_number, month_start, year_start, month_finish, year_finish, effort_months, latitude, longitude, altitude, ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n## Importar os dados pelo pacote ecodados\ntidy_anfibios_locais <- ecodados::tidy_anfibios_locais\nhead(tidy_anfibios_locais)"},{"path":"cap5.html","id":"tibble","chapter":"Capítulo 5 Tidyverse","heading":"5.5 tibble","text":"O tibble (tbl_sf) é uma versão aprimorada data frame (data.frame). Ele é classe aconselhada para que funções tidyverse funcionem melhor sobre conjuntos de dados tabulares importados para o R.Geralmente, quando utilizamos funções tidyverse para importar dados para o R, é essa classe que os dados adquirem depois de serem importados. Além da importação de dados, podemos criar um tibble R usando função tibble::tibble(), semelhante ao uso da função data.frame(). Podemos ainda converter um data.frame para um tibble usando função tibble::as_tibble(). Entretanto, em alguns momentos precisaremos da classe data.frame para algumas funções específicas, e podemos converter um tibble para data.frame usando função tibble::as_data_frame().Existem duas diferenças principais uso tibble e data.frame: impressão e subconjunto. Objetos da classe tibbles possuem um método de impressão que mostra contagem número de linhas e colunas, e apenas primeiras 10 linhas e todas colunas que couberem na tela console, além dos modos ou tipos das colunas. Dessa forma, cada coluna ou variável, pode ser modo numbers (int ou dbl), character (chr), logical (lgl), factor (fctr), date + time (dttm) e date (date), além de outras inúmeras possibilidades.Todas funções deste pacote são listadas na página de referência pacote.Para o subconjunto, como vimos Capítulo 4, para selecionar colunas e linhas de objetos bidimensionais podemos utilizar os operadores [] ou [[]], associado com números separados por vírgulas ou o nome da coluna entre aspas, e o operador $ para extrair uma coluna pelo seu nome. Comparando um data.frame um tibble, o último é mais rígido na seleção das colunas: ele nunca faz correspondência parcial e gera um aviso se coluna que você está tentando acessar não existe.Por fim, podemos “espiar” os dados utilizando função tibble::glimpse() para ter uma noção geral de número de linhas, colunas, e conteúdo de todas colunas. Essa é função tidyverse da função R Base str().Para se aprofundar tema, recomendamos leitura Capítulo 10 Tibbles de Wickham & Grolemund (2017).","code":"\n## Tibble - impressão\ntidy_anfibios_locais\n#> # A tibble: 1,163 × 25\n#>    id      reference_number species_number record sampled_habitat active_methods passive_methods complementary_meth… period month_start year_start\n#>    <chr>              <dbl>          <dbl> <chr>  <chr>           <chr>          <chr>           <chr>               <chr>        <dbl>      <dbl>\n#>  1 amp1001             1001             19 ab     fo,ll           as             pt              <NA>                mo,da…           9       2000\n#>  2 amp1002             1002             16 co     fo,la,ll        as             pt              <NA>                mo,da…          12       2007\n#>  3 amp1003             1002             14 co     fo,la,ll        as             pt              <NA>                mo,da…          12       2007\n#>  4 amp1004             1002             13 co     fo,la,ll        as             pt              <NA>                mo,da…          12       2007\n#>  5 amp1005             1003             30 co     fo,ll,br        as             <NA>            <NA>                mo,da…           7       1988\n#>  6 amp1006             1004             42 co     tp,pp,la,ll,is  <NA>           <NA>            <NA>                <NA>            NA         NA\n#>  7 amp1007             1005             23 co     sp              as             <NA>            <NA>                <NA>             4       2007\n#>  8 amp1008             1005             19 co     sp,la,sw        as,sb,tr       <NA>            <NA>                tw,ni            4       2007\n#>  9 amp1009             1005             13 ab     fo              <NA>           pt              <NA>                mo,da…           4       2007\n#> 10 amp1010             1006              1 ab     fo              <NA>           pt              <NA>                mo,da…           5       2011\n#> # … with 1,153 more rows, and 14 more variables: month_finish <dbl>, year_finish <dbl>, effort_months <dbl>, country <chr>, state <chr>,\n#> #   state_abbreviation <chr>, municipality <chr>, site <chr>, latitude <dbl>, longitude <dbl>, coordinate_precision <chr>, altitude <dbl>,\n#> #   temperature <dbl>, precipitation <dbl>\n## Tibble - subconjunto\ntidy_anfibios_locais$ref\n#> Warning: Unknown or uninitialised column: `ref`.\n#> NULL\n## Espiar os dados\ntibble::glimpse(tidy_anfibios_locais[, 1:10])\n#> Rows: 1,163\n#> Columns: 10\n#> $ id                    <chr> \"amp1001\", \"amp1002\", \"amp1003\", \"amp1004\", \"amp1005\", \"amp1006\", \"amp1007\", \"amp1008\", \"amp1009\", \"amp1010\", \"amp…\n#> $ reference_number      <dbl> 1001, 1002, 1002, 1002, 1003, 1004, 1005, 1005, 1005, 1006, 1006, 1006, 1006, 1006, 1006, 1006, 1006, 1006, 1006, …\n#> $ species_number        <dbl> 19, 16, 14, 13, 30, 42, 23, 19, 13, 1, 1, 2, 4, 4, 6, 5, 8, 2, 5, 1, 2, 2, 1, 2, 2, 1, 2, 3, 7, 8, 7, 7, 7, 7, 7, …\n#> $ record                <chr> \"ab\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"ab\", \"ab\", \"ab\", \"ab\", \"ab\", \"ab\", \"ab\", \"ab\", \"ab\", \"ab\", \"ab\", …\n#> $ sampled_habitat       <chr> \"fo,ll\", \"fo,la,ll\", \"fo,la,ll\", \"fo,la,ll\", \"fo,ll,br\", \"tp,pp,la,ll,is\", \"sp\", \"sp,la,sw\", \"fo\", \"fo\", \"fo\", \"fo…\n#> $ active_methods        <chr> \"as\", \"as\", \"as\", \"as\", \"as\", NA, \"as\", \"as,sb,tr\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ passive_methods       <chr> \"pt\", \"pt\", \"pt\", \"pt\", NA, NA, NA, NA, \"pt\", \"pt\", \"pt\", \"pt\", \"pt\", \"pt\", \"pt\", \"pt\", \"pt\", \"pt\", \"pt\", \"pt\", \"p…\n#> $ complementary_methods <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ period                <chr> \"mo,da,tw,ni\", \"mo,da,tw,ni\", \"mo,da,tw,ni\", \"mo,da,tw,ni\", \"mo,da,ni\", NA, NA, \"tw,ni\", \"mo,da,tw,ni\", \"mo,da,tw,…\n#> $ month_start           <dbl> 9, 12, 12, 12, 7, NA, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3,…"},{"path":"cap5.html","id":"magrittr-pipe--","chapter":"Capítulo 5 Tidyverse","heading":"5.6 magrittr (pipe - %>%)","text":"O operador pipe %>% permite o encadeamento de várias funções, eliminando necessidade de criar objetos para armazenar resultados intermediários. Dessa forma, pipes são uma ferramenta poderosa para expressar uma sequência de múltiplas operações.O operador pipe %>% vem pacote magrittr, entretanto, todos os pacotes tidyverse automaticamente tornam o pipe disponível. Essa função torna os códigos em R mais simples, pois podemos realizar múltiplas operações em uma única linha. Ele captura o resultado de uma declaração e o torna primeira entrada da próxima declaração, então podemos pensar como “EM SEGUIDA FAÇA” ao final de cada linha de código.Todas funções deste pacote são listadas na página de referência pacote.principal vantagem uso dos pipes é facilitar depuração (debugging - achar erros) nos códigos, porque seu uso torna linguagem R mais próxima que falamos e pensamos, uma vez que evita o uso de funções dentro de funções (funções compostas, lembra-se fog e gof ensino médio? Evitamos eles aqui também).Digitar %>% é um pouco chato, dessa forma, existe um atalho para sua inserção nos scripts: Ctrl + Shift + M.Para deixar esse tópico menos estranho quem possa ver essa operação pela primeira vez, vamos fazer alguns exemplos.Essas operações ainda estão simples, vamos torná-las mais complexas com várias funções compostas. É nesses casos que propriedade organizacional uso pipe emerge: podemos facilmente ver o encadeamento de operações, onde cada função é disposta numa linha. Apenas um adendo: função set.seed() fixa amostragem de funções que geram valores aleatórios, como é o caso da função rpois().O uso pipe vai se tornar especialmente útil quando seguirmos para os pacotes das próximas duas seções: tidyr e dplyr. Com esses pacotes faremos operações em linhas e colunas de nossos dados tabulares, então podemos encadear uma série de funções para manipulação, limpeza e análise de dados.Há ainda três outras variações pipe que podem ser úteis em alguns momentos, mas que para funcionar precisam que o pacote magrittr esteja carregado:%T>%: retorna o lado esquerdo em vez lado direito da operação%$%: “explode” variáveis em um quadro de dados%<>%: permite atribuição usando pipesPara se aprofundar tema, recomendamos leitura Capítulo 18 Pipes de Wickham & Grolemund (2017). 📝 Importante \npartir da versão R 4.1+ (18/05/2021), o operador pipe se tornou nativo R. Entretanto, o operador foi atualizado para |>, podendo ser inserido com o mesmo atalho Ctrl + Shift + M, mas necessitando uma mudança de opção em Tools > Global Options > Code > [x] Use native pipe operator, |> (requires R 4.1+), requerendo que o RStudio esteja numa versão igual ou superior 1.4.17+.","code":"\n## R Base - sem pipe\nsqrt(sum(1:100))\n#> [1] 71.06335\n\n## Tidyverse - com pipe\n1:100 %>% \n    sum() %>% \n    sqrt()\n#> [1] 71.06335\n## Fixar amostragem\nset.seed(42)\n\n## R Base - sem pipe\nve <- sum(sqrt(sin(log10(rpois(100, 10)))))\nve\n#> [1] 91.27018\n\n## Fixar amostragem\nset.seed(42)\n\n## Tidyverse - com pipe\nve <- rpois(100, 10) %>% \n    log10() %>%\n    sin() %>% \n    sqrt() %>% \n    sum()\nve\n#> [1] 91.27018"},{"path":"cap5.html","id":"tidyr","chapter":"Capítulo 5 Tidyverse","heading":"5.7 tidyr","text":"Um conjunto de dados tidy (organizados) são mais fáceis de manipular, modelar e visualizar. Um conjunto de dados está formato tidy ou não, dependendo de como linhas, colunas e células são combinadas com observações, variáveis e valores. Nos dados tidy, variáveis estão nas colunas, observações estão nas linhas e valores estão nas células, sendo que para esse último, não deve haver mais de um valor por célula (Figura 5.2).Cada variável em uma colunaCada observação em uma linhaCada valor como uma célula\nFigura 5.2: três regras que tornam um conjunto de dados tidy. Adaptado de: Wickham & Grolemund (2017)).\nTodas funções deste pacote são listadas na página de referência pacote.Para realizar diversas transformações nos dados, fim de ajustá-los ao formato tidy existe uma série de funções para: unir colunas, separar colunas, lidar com valores faltantes (NA), transformar base de dados de formato longo para largo (ou vice-e-versa), além de outras funções específicas.unite(): junta dados de múltiplas colunas em uma colunaseparate(): separa caracteres em múltiplas colunasseparate_rows(): separa caracteres em múltiplas colunas e linhasdrop_na(): retira linhas com NA conjunto de dadosreplace_na(): substitui NA conjunto de dadospivot_wider(): transforma um conjunto de dados longo (long) para largo (wide)pivot_longer(): transforma um conjunto de dados largo (wide) para longo (long)","code":""},{"path":"cap5.html","id":"palmerpenguins","chapter":"Capítulo 5 Tidyverse","heading":"5.7.1 palmerpenguins","text":"Para exemplificar o funcionamento dessas funções, usaremos os dados de medidas de pinguins chamados palmerpenguins, disponíveis pacote palmerpenguins.Esses dados foram coletados e disponibilizados pela Dra. Kristen Gorman e pela Palmer Station, Antarctica LTER, membro da Long Term Ecological Research Network.O pacote palmerpenguins contém dois conjuntos de dados. Um é chamado de penguins e é uma versão simplificada dos dados brutos. O segundo conjunto de dados é penguins_raw e contém todas variáveis e nomes originais. Ambos os conjuntos de dados contêm dados para 344 pinguins, de três espécies diferentes, coletados em três ilhas arquipélago de Palmer, na Antártica. Destacamos também versão traduzida desses dados para o português, disponível pacote dados.Vamos utilizar principalmente o conjunto de dados penguins_raw, que é versão dos dados brutos.Podemos ainda verificar os dados, pedindo uma ajuda de cada um dos objetos.","code":"\n## Instalar o pacote\ninstall.packages(\"palmerpenguins\")\n## Carregar o pacote palmerpenguins\nlibrary(palmerpenguins)\n## Ajuda dos dados\n?penguins\n?penguins_raw"},{"path":"cap5.html","id":"glimpse","chapter":"Capítulo 5 Tidyverse","heading":"5.7.2 glimpse()","text":"Primeiramente, vamos observar os dados e utilizar função tibble::glimpse() para ter uma noção geral dos dados.","code":"\n## Visualizar os dados\npenguins_raw\n#> # A tibble: 344 × 17\n#>    studyName `Sample Number` Species             Region Island Stage `Individual ID` `Clutch Comple…` `Date Egg` `Culmen Length…` `Culmen Depth …`\n#>    <chr>               <dbl> <chr>               <chr>  <chr>  <chr> <chr>           <chr>            <date>                <dbl>            <dbl>\n#>  1 PAL0708                 1 Adelie Penguin (Py… Anvers Torge… Adul… N1A1            Yes              2007-11-11             39.1             18.7\n#>  2 PAL0708                 2 Adelie Penguin (Py… Anvers Torge… Adul… N1A2            Yes              2007-11-11             39.5             17.4\n#>  3 PAL0708                 3 Adelie Penguin (Py… Anvers Torge… Adul… N2A1            Yes              2007-11-16             40.3             18  \n#>  4 PAL0708                 4 Adelie Penguin (Py… Anvers Torge… Adul… N2A2            Yes              2007-11-16             NA               NA  \n#>  5 PAL0708                 5 Adelie Penguin (Py… Anvers Torge… Adul… N3A1            Yes              2007-11-16             36.7             19.3\n#>  6 PAL0708                 6 Adelie Penguin (Py… Anvers Torge… Adul… N3A2            Yes              2007-11-16             39.3             20.6\n#>  7 PAL0708                 7 Adelie Penguin (Py… Anvers Torge… Adul… N4A1            No               2007-11-15             38.9             17.8\n#>  8 PAL0708                 8 Adelie Penguin (Py… Anvers Torge… Adul… N4A2            No               2007-11-15             39.2             19.6\n#>  9 PAL0708                 9 Adelie Penguin (Py… Anvers Torge… Adul… N5A1            Yes              2007-11-09             34.1             18.1\n#> 10 PAL0708                10 Adelie Penguin (Py… Anvers Torge… Adul… N5A2            Yes              2007-11-09             42               20.2\n#> # … with 334 more rows, and 6 more variables: `Flipper Length (mm)` <dbl>, `Body Mass (g)` <dbl>, Sex <chr>, `Delta 15 N (o/oo)` <dbl>,\n#> #   `Delta 13 C (o/oo)` <dbl>, Comments <chr>\n\n## Espiar os dados\ndplyr::glimpse(penguins_raw)\n#> Rows: 344\n#> Columns: 17\n#> $ studyName             <chr> \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL…\n#> $ `Sample Number`       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,…\n#> $ Species               <chr> \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie Penguin (Pygoscelis adeliae)…\n#> $ Region                <chr> \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anv…\n#> $ Island                <chr> \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen…\n#> $ Stage                 <chr> \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adu…\n#> $ `Individual ID`       <chr> \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", \"N4A1\", \"N4A2\", \"N5A1\", \"N5A2\", \"N6A1\", \"N6A2\", \"N7A1\", \"N7A2\", \"N…\n#> $ `Clutch Completion`   <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes…\n#> $ `Date Egg`            <date> 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16, 2007-11-16, 2007-11-16, 2007-11-15, 2007-11-15, 2007-11-09, 2007-…\n#> $ `Culmen Length (mm)`  <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41.1, 38.6, 34.6, 36.6, 38.7, 42.5, 34.4, 46…\n#> $ `Culmen Depth (mm)`   <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17.6, 21.2, 21.1, 17.8, 19.0, 20.7, 18.4, 21…\n#> $ `Flipper Length (mm)` <dbl> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198, 185, 195, 197, 184, 194, 174, 180, 189, …\n#> $ `Body Mass (g)`       <dbl> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 3200, 3800, 4400, 3700, 3450, 4500, 3325, 42…\n#> $ Sex                   <chr> \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\", \"FEMALE\", \"MALE\", NA, NA, NA, NA, \"FEMALE\", \"MALE\", \"MALE\", \"FEM…\n#> $ `Delta 15 N (o/oo)`   <dbl> NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18718, 9.46060, NA, 9.13362, 8.63243, NA, NA, NA, 8.55583, NA, 9.185…\n#> $ `Delta 13 C (o/oo)`   <dbl> NA, -24.69454, -25.33302, NA, -25.32426, -25.29805, -25.21799, -24.89958, NA, -25.09368, -25.21315, NA, NA, NA, -2…\n#> $ Comments              <chr> \"Not enough blood for isotopes.\", NA, NA, \"Adult not sampled.\", NA, NA, \"Nest never observed with full clutch.\", \"…"},{"path":"cap5.html","id":"unite","chapter":"Capítulo 5 Tidyverse","heading":"5.7.3 unite()","text":"Primeiramente, vamos exemplificar como juntar e separar colunas. Vamos utilizar função tidyr::unite() para unir colunas. Há diversos parâmetros para alterar como esta função funciona, entretanto, é importante destacar três deles: col nome da coluna que vai receber colunas unidas, sep indicando o caractere separador das colunas unidas, e remove para uma resposta lógica se colunas unidas são removidas ou não. Vamos unir colunas “Region” e “Island” na nova coluna “region_island”.","code":"\n## Unir colunas\npenguins_raw_unir <- tidyr::unite(data = penguins_raw, \n                                  col = \"region_island\",\n                                  Region:Island, \n                                  sep = \", \",\n                                  remove = FALSE)\nhead(penguins_raw_unir[, c(\"Region\", \"Island\", \"region_island\")])\n#> # A tibble: 6 × 3\n#>   Region Island    region_island    \n#>   <chr>  <chr>     <chr>            \n#> 1 Anvers Torgersen Anvers, Torgersen\n#> 2 Anvers Torgersen Anvers, Torgersen\n#> 3 Anvers Torgersen Anvers, Torgersen\n#> 4 Anvers Torgersen Anvers, Torgersen\n#> 5 Anvers Torgersen Anvers, Torgersen\n#> 6 Anvers Torgersen Anvers, Torgersen"},{"path":"cap5.html","id":"separate","chapter":"Capítulo 5 Tidyverse","heading":"5.7.4 separate()","text":"De forma contrária, podemos utilizar funções tidyr::separate() e tidyr::separate_rows() para separar elementos de uma coluna em mais colunas. Respectivamente, primeira função separa uma coluna em novas colunas conforme separação, e segunda função separa uma coluna, distribuindo os elementos nas linhas. Novamente, há diversos parâmetros para mudar o comportamento dessas funções, mas destacaremos aqui quatro deles: col coluna ser separada, os nomes das novas colunas, sep indicando o caractere separador das colunas, e remove para uma resposta lógica se colunas separadas são removidas ou não. Vamos separar coluna “Stage” nas colunas “stage” e “egg_stage”.","code":"\n## Separar colunas\npenguins_raw_separar <- tidyr::separate(data = penguins_raw, \n                                        col = Stage,\n                                        into = c(\"stage\", \"egg_stage\"), \n                                        sep = \", \",\n                                        remove = FALSE)\nhead(penguins_raw_separar[, c(\"Stage\", \"stage\", \"egg_stage\")])\n#> # A tibble: 6 × 3\n#>   Stage              stage egg_stage  \n#>   <chr>              <chr> <chr>      \n#> 1 Adult, 1 Egg Stage Adult 1 Egg Stage\n#> 2 Adult, 1 Egg Stage Adult 1 Egg Stage\n#> 3 Adult, 1 Egg Stage Adult 1 Egg Stage\n#> 4 Adult, 1 Egg Stage Adult 1 Egg Stage\n#> 5 Adult, 1 Egg Stage Adult 1 Egg Stage\n#> 6 Adult, 1 Egg Stage Adult 1 Egg Stage\n\n## Separar colunas em novas linhas\npenguins_raw_separar_linhas <- tidyr::separate_rows(data = penguins_raw,\n                                                    Stage,\n                                                    sep = \", \")\nhead(penguins_raw_separar_linhas[, c(\"studyName\", \"Sample Number\", \"Species\", \n                                     \"Region\", \"Island\", \"Stage\")])\n#> # A tibble: 6 × 6\n#>   studyName `Sample Number` Species                             Region Island    Stage      \n#>   <chr>               <dbl> <chr>                               <chr>  <chr>     <chr>      \n#> 1 PAL0708                 1 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen Adult      \n#> 2 PAL0708                 1 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen 1 Egg Stage\n#> 3 PAL0708                 2 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen Adult      \n#> 4 PAL0708                 2 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen 1 Egg Stage\n#> 5 PAL0708                 3 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen Adult      \n#> 6 PAL0708                 3 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen 1 Egg Stage"},{"path":"cap5.html","id":"drop_na-e-replace_na","chapter":"Capítulo 5 Tidyverse","heading":"5.7.5 drop_na() e replace_na()","text":"Valor faltante (NA) é um tipo especial de elemento que são discutidos Capítulo 4 e são relativamente comuns em conjuntos de dados. Em R Base, vimos algumas formas de lidar com esse tipo de elemento. formato tidyverse, existem também várias formas de lidar com eles, mas aqui focaremos nas funções tidyr::drop_na() e tidyr::replace_na(), para retirar linhas e substituir esses valores, respectivamente.","code":"\n## Remover todas as linhas com NAs\npenguins_raw_todas_na <- tidyr::drop_na(data = penguins_raw)\nhead(penguins_raw_todas_na)\n#> # A tibble: 6 × 17\n#>   studyName `Sample Number` Species      Region Island Stage `Individual ID` `Clutch Comple…` `Date Egg` `Culmen Length…`\n#>   <chr>               <dbl> <chr>        <chr>  <chr>  <chr> <chr>           <chr>            <date>                <dbl>\n#> 1 PAL0708                 7 Adelie Peng… Anvers Torge… Adul… N4A1            No               2007-11-15             38.9\n#> 2 PAL0708                 8 Adelie Peng… Anvers Torge… Adul… N4A2            No               2007-11-15             39.2\n#> 3 PAL0708                29 Adelie Peng… Anvers Biscoe Adul… N18A1           No               2007-11-10             37.9\n#> 4 PAL0708                30 Adelie Peng… Anvers Biscoe Adul… N18A2           No               2007-11-10             40.5\n#> 5 PAL0708                39 Adelie Peng… Anvers Dream  Adul… N25A1           No               2007-11-13             37.6\n#> 6 PAL0809                69 Adelie Peng… Anvers Torge… Adul… N32A1           No               2008-11-11             35.9\n#> # … with 7 more variables: `Culmen Depth (mm)` <dbl>, `Flipper Length (mm)` <dbl>, `Body Mass (g)` <dbl>, Sex <chr>,\n#> #   `Delta 15 N (o/oo)` <dbl>, `Delta 13 C (o/oo)` <dbl>, Comments <chr>\n\n## Remover linhas de colunas específicas com NAs\npenguins_raw_colunas_na <- tidyr::drop_na(data = penguins_raw,\n                                          any_of(\"Comments\"))\nhead(penguins_raw_colunas_na[, \"Comments\"])\n#> # A tibble: 6 × 1\n#>   Comments                             \n#>   <chr>                                \n#> 1 Not enough blood for isotopes.       \n#> 2 Adult not sampled.                   \n#> 3 Nest never observed with full clutch.\n#> 4 Nest never observed with full clutch.\n#> 5 No blood sample obtained.            \n#> 6 No blood sample obtained for sexing.\n\n## Substituir NAs por outro valor\npenguins_raw_subs_na <- tidyr::replace_na(data = penguins_raw,\n                                          list(Comments = \"Unknown\"))\nhead(penguins_raw_subs_na[, \"Comments\"])\n#> # A tibble: 6 × 1\n#>   Comments                      \n#>   <chr>                         \n#> 1 Not enough blood for isotopes.\n#> 2 Unknown                       \n#> 3 Unknown                       \n#> 4 Adult not sampled.            \n#> 5 Unknown                       \n#> 6 Unknown"},{"path":"cap5.html","id":"pivot_longer-e-pivot_wider","chapter":"Capítulo 5 Tidyverse","heading":"5.7.6 pivot_longer() e pivot_wider()","text":"Por fim, trataremos da pivotagem ou remodelagem de dados. Veremos como mudar o formato nosso conjunto de dados de longo (long) para largo (wide) e vice-versa. Primeiramente, vamos ver como partir de um dado longo (long) e criar um dado largo (wide). Essa é uma operação semelhante à “Tabela Dinâmica” das planilhas eletrônicas. Consiste em usar uma coluna para distribuir seus valores em outras colunas, de modo que os valores dos elementos são preenchidos corretamente, reduzindo assim o número de linhas e aumentando o número de colunas. Essa operação é bastante comum em Ecologia de Comunidades, quando queremos transformar uma lista de espécies em uma matriz de comunidades, com várias espécies nas colunas. Para realizar essa operação, usamos função tidyr::pivot_wider(). Dos diversos parâmetros que podem compor essa função, dois deles são fundamentais: names_from que indica coluna de onde os nomes serão usados e values_from que indica que indica coluna com os valores.De modo oposto, podemos partir de um conjunto de dados largo (wide), ou seja, com várias colunas, e queremos que essas colunas preencham uma única coluna, e que os valores antes espalhados nessas várias colunas sejam adicionados um embaixo outro, numa única coluna, formato longo (long). Para essa operação, podemos utilizar função tidyr::pivot_longer(). Novamente, dos diversos parâmetros que podem compor essa função, três deles são fundamentais: cols indicando colunas que serão usadas para serem pivotadas, names_to que indica coluna de onde os nomes serão usados e values_to que indica coluna com os valores.Para se aprofundar tema, recomendamos leitura Capítulo 12 Tidy data de Wickham & Grolemund (2017).","code":"\n## Selecionar colunas\npenguins_raw_sel_col <- penguins_raw[, c(2, 3, 13)]\nhead(penguins_raw_sel_col)\n#> # A tibble: 6 × 3\n#>   `Sample Number` Species                             `Body Mass (g)`\n#>             <dbl> <chr>                                         <dbl>\n#> 1               1 Adelie Penguin (Pygoscelis adeliae)            3750\n#> 2               2 Adelie Penguin (Pygoscelis adeliae)            3800\n#> 3               3 Adelie Penguin (Pygoscelis adeliae)            3250\n#> 4               4 Adelie Penguin (Pygoscelis adeliae)              NA\n#> 5               5 Adelie Penguin (Pygoscelis adeliae)            3450\n#> 6               6 Adelie Penguin (Pygoscelis adeliae)            3650\n\n## Pivotar para largo\npenguins_raw_pivot_wider <- tidyr::pivot_wider(data = penguins_raw_sel_col, \n                                               names_from = Species, \n                                               values_from = `Body Mass (g)`)\nhead(penguins_raw_pivot_wider)\n#> # A tibble: 6 × 4\n#>   `Sample Number` `Adelie Penguin (Pygoscelis adeliae)` `Gentoo penguin (Pygoscelis papua)` `Chinstrap penguin (Pygoscelis antarctica)`\n#>             <dbl>                                 <dbl>                               <dbl>                                       <dbl>\n#> 1               1                                  3750                                4500                                        3500\n#> 2               2                                  3800                                5700                                        3900\n#> 3               3                                  3250                                4450                                        3650\n#> 4               4                                    NA                                5700                                        3525\n#> 5               5                                  3450                                5400                                        3725\n#> 6               6                                  3650                                4550                                        3950\n## Selecionar colunas\npenguins_raw_sel_col <- penguins_raw[, c(2, 3, 10:13)]\nhead(penguins_raw_sel_col)\n#> # A tibble: 6 × 6\n#>   `Sample Number` Species                             `Culmen Length (mm)` `Culmen Depth (mm)` `Flipper Length (mm)` `Body Mass (g)`\n#>             <dbl> <chr>                                              <dbl>               <dbl>                 <dbl>           <dbl>\n#> 1               1 Adelie Penguin (Pygoscelis adeliae)                 39.1                18.7                   181            3750\n#> 2               2 Adelie Penguin (Pygoscelis adeliae)                 39.5                17.4                   186            3800\n#> 3               3 Adelie Penguin (Pygoscelis adeliae)                 40.3                18                     195            3250\n#> 4               4 Adelie Penguin (Pygoscelis adeliae)                 NA                  NA                      NA              NA\n#> 5               5 Adelie Penguin (Pygoscelis adeliae)                 36.7                19.3                   193            3450\n#> 6               6 Adelie Penguin (Pygoscelis adeliae)                 39.3                20.6                   190            3650\n\n## Pivotar para largo\npenguins_raw_pivot_longer <- tidyr::pivot_longer(data = penguins_raw_sel_col, \n                                                 cols = `Culmen Length (mm)`:`Body Mass (g)`,\n                                                 names_to = \"medidas\", \n                                                 values_to = \"valores\")\nhead(penguins_raw_pivot_longer)\n#> # A tibble: 6 × 4\n#>   `Sample Number` Species                             medidas             valores\n#>             <dbl> <chr>                               <chr>                 <dbl>\n#> 1               1 Adelie Penguin (Pygoscelis adeliae) Culmen Length (mm)     39.1\n#> 2               1 Adelie Penguin (Pygoscelis adeliae) Culmen Depth (mm)      18.7\n#> 3               1 Adelie Penguin (Pygoscelis adeliae) Flipper Length (mm)   181  \n#> 4               1 Adelie Penguin (Pygoscelis adeliae) Body Mass (g)        3750  \n#> 5               2 Adelie Penguin (Pygoscelis adeliae) Culmen Length (mm)     39.5\n#> 6               2 Adelie Penguin (Pygoscelis adeliae) Culmen Depth (mm)      17.4"},{"path":"cap5.html","id":"dplyr","chapter":"Capítulo 5 Tidyverse","heading":"5.8 dplyr","text":"O dplyr é um pacote que facilita manipulação de dados, com uma gramática simples e flexível (por exemplo, como filtragem, reordenamento, seleção, entre outras). Ele foi construído com o intuito de obter uma forma mais rápida e expressiva de manipular dados tabulares. O tibble é versão de data frame mais conveniente para se usar com pacote dplyr.Todas funções deste pacote são listadas na página de referência pacote.","code":""},{"path":"cap5.html","id":"gramática","chapter":"Capítulo 5 Tidyverse","heading":"5.8.1 Gramática","text":"Sua gramática simples contém funções verbais para manipulação de dados, baseada em:Verbos: mutate(), select(), filter(), arrange(), summarise(), slice(), rename(), etc.Replicação: across(), if_any(), if_all(), (), starts_with(), ends_with(), contains(), etc.Agrupamento: group_by() e ungroup()Junções: inner_join(), full_join(), left_join(), right_join(), etc.Combinações: bind_rows() e bind_cols()Resumos, contagem e seleção: n(), n_distinct(), first(), last(), nth(), etc.Existe uma série de funções para realizar manipulação dos dados, com diversas finalidades: manipulação de uma tabela, manipulação de duas tabelas, replicação, agrupamento, funções de vetores, além de muitas outras funções específicas.relocate(): muda ordem das colunasrename(): muda o nome das colunasselect(): seleciona colunas pelo nome ou posiçãopull(): seleciona uma coluna como vetormutate(): adiciona novas colunas ou resultados em colunas existentesarrange(): reordena linhas com base nos valores de colunasfilter(): seleciona linhas com base em valores de colunasslice(): seleciona linhas de diferente formasdistinct(): remove linhas com valores repetidos com base nos valores de colunascount(): conta observações para um grupo com base nos valores de colunasgroup_by(): agrupa linhas pelos valores das colunassummarise(): resume os dados através de funções considerando valores das colunas*_join(): funções que juntam dados de duas tabelas através de uma coluna chave","code":""},{"path":"cap5.html","id":"sintaxe","chapter":"Capítulo 5 Tidyverse","heading":"5.8.2 Sintaxe","text":"funções dplyr podem seguir uma mesma sintaxe: o tibble será sempre o primeiro argumento dessas funções, seguido de um operador pipe (%>%) e pelo nome da função que irá fazer manipulação nesses dados. Isso permite o encadeamento de várias operações consecutivas mantendo estrutura dado original e acrescentando mudanças num encadeamento lógico.Sendo assim, funções verbais não precisam modificar necessariamente o tibble original, sendo que operações de manipulações podem e devem ser atribuídas um novo objeto.Além de data.frames e tibbles, manipulação pelo formato dplyr torna o trabalho com outros formatos de classes e dados acessíveis e eficientes como data.table, SQL e Apache Spark, para os quais existem pacotes específicos.dtplyr: manipular conjuntos de dados data.tabledbplyr: manipular conjuntos de dados SQLsparklyr: manipular conjuntos de dados Apache Spark","code":"\n## Sintaxe\ntb_dplyr <- tb %>% \n    funcao_verbal1(argumento1, argumento2, ...) %>% \n    funcao_verbal2(argumento1, argumento2, ...) %>% \n    funcao_verbal3(argumento1, argumento2, ...)"},{"path":"cap5.html","id":"palmerpenguins-1","chapter":"Capítulo 5 Tidyverse","heading":"5.8.3 palmerpenguins","text":"Para nossos exemplos, vamos utilizar novamente os dados de pinguins palmerpenguins. Esses dados estão disponíveis pacote palmerpenguins. Vamos utilizar principalmente o conjunto de dados penguins, que é versão simplificada dos dados brutos penguins_raw.","code":"\n## Carregar o pacote palmerpenguins\nlibrary(palmerpenguins)"},{"path":"cap5.html","id":"relocate","chapter":"Capítulo 5 Tidyverse","heading":"5.8.4 relocate()","text":"Primeiramente, vamos reordenar colunas com função dplyr::relocate(), onde simplesmente listamos colunas que queremos mudar de posição e para onde elas devem ir. Para esse último passo há dois argumentos: .que indica coluna onde coluna realocada deve se mover antes, e o argumento .indicando onde deve se mover depois. Ambos podem ser informados com os nomes ou posições dessas colunas com números.","code":"\n## Reordenar colunas - nome\npenguins_relocate_col <- penguins %>% \n    dplyr::relocate(sex, year, .after = island)\nhead(penguins_relocate_col)\n#> # A tibble: 6 × 8\n#>   species island    sex     year bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#>   <fct>   <fct>     <fct>  <int>          <dbl>         <dbl>             <int>       <int>\n#> 1 Adelie  Torgersen male    2007           39.1          18.7               181        3750\n#> 2 Adelie  Torgersen female  2007           39.5          17.4               186        3800\n#> 3 Adelie  Torgersen female  2007           40.3          18                 195        3250\n#> 4 Adelie  Torgersen <NA>    2007           NA            NA                  NA          NA\n#> 5 Adelie  Torgersen female  2007           36.7          19.3               193        3450\n#> 6 Adelie  Torgersen male    2007           39.3          20.6               190        3650\n\n## Reordenar colunas - posição\npenguins_relocate_ncol <- penguins %>% \n    dplyr::relocate(sex, year, .after = 2)\nhead(penguins_relocate_ncol)\n#> # A tibble: 6 × 8\n#>   species island    sex     year bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#>   <fct>   <fct>     <fct>  <int>          <dbl>         <dbl>             <int>       <int>\n#> 1 Adelie  Torgersen male    2007           39.1          18.7               181        3750\n#> 2 Adelie  Torgersen female  2007           39.5          17.4               186        3800\n#> 3 Adelie  Torgersen female  2007           40.3          18                 195        3250\n#> 4 Adelie  Torgersen <NA>    2007           NA            NA                  NA          NA\n#> 5 Adelie  Torgersen female  2007           36.7          19.3               193        3450\n#> 6 Adelie  Torgersen male    2007           39.3          20.6               190        3650"},{"path":"cap5.html","id":"rename","chapter":"Capítulo 5 Tidyverse","heading":"5.8.5 rename()","text":"Podemos renomear colunas facilmente com função dplyr::rename(), onde primeiramente informamos o nome que queremos que coluna tenha, seguido operador = e coluna nosso dado (“nova_coluna = antiga_coluna”). Também podemos utilizar função dplyr::rename_with(), que faz mudança nome em múltiplas colunas, que pode depender ou não de resultados booleanos.","code":"\n## Renomear as colunas\npenguins_rename <- penguins %>% \n    dplyr::rename(bill_length = bill_length_mm,\n                  bill_depth = bill_depth_mm,\n                  flipper_length = flipper_length_mm,\n                  body_mass = body_mass_g)\nhead(penguins_rename)\n#> # A tibble: 6 × 8\n#>   species island    bill_length bill_depth flipper_length body_mass sex     year\n#>   <fct>   <fct>           <dbl>      <dbl>          <int>     <int> <fct>  <int>\n#> 1 Adelie  Torgersen        39.1       18.7            181      3750 male    2007\n#> 2 Adelie  Torgersen        39.5       17.4            186      3800 female  2007\n#> 3 Adelie  Torgersen        40.3       18              195      3250 female  2007\n#> 4 Adelie  Torgersen        NA         NA               NA        NA <NA>    2007\n#> 5 Adelie  Torgersen        36.7       19.3            193      3450 female  2007\n#> 6 Adelie  Torgersen        39.3       20.6            190      3650 male    2007\n\n## mudar o nome de todas as colunas\npenguins_rename_with <- penguins %>% \n    dplyr::rename_with(toupper)\nhead(penguins_rename_with)\n#> # A tibble: 6 × 8\n#>   SPECIES ISLAND    BILL_LENGTH_MM BILL_DEPTH_MM FLIPPER_LENGTH_MM BODY_MASS_G SEX     YEAR\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 4 Adelie  Torgersen           NA            NA                  NA          NA <NA>    2007\n#> 5 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 6 Adelie  Torgersen           39.3          20.6               190        3650 male    2007"},{"path":"cap5.html","id":"select","chapter":"Capítulo 5 Tidyverse","heading":"5.8.6 select()","text":"Outra operação bastante usual dentro da manipulação de dados tabulares é seleção de colunas. Podemos fazer essa operação com função dplyr::select(), que seleciona colunas pelo nome ou pela sua posição. Aqui há uma série de possibilidades de seleção de colunas, desde utilizar operadores como : para selecionar intervalos de colunas, ! para tomar o complemento (todas menos listadas), além de funções como dplyr::starts_with(), dplyr::ends_with(), dplyr::contains() para procurar colunas com um padrão de texto nome da coluna.","code":"\n## Selecionar colunas por posição\npenguins_select_position <- penguins %>% \n    dplyr::select(3:6)\nhead(penguins_select_position)\n#> # A tibble: 6 × 4\n#>   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#>            <dbl>         <dbl>             <int>       <int>\n#> 1           39.1          18.7               181        3750\n#> 2           39.5          17.4               186        3800\n#> 3           40.3          18                 195        3250\n#> 4           NA            NA                  NA          NA\n#> 5           36.7          19.3               193        3450\n#> 6           39.3          20.6               190        3650\n\n## Selecionar colunas por nomes\npenguins_select_names <- penguins %>% \n    dplyr::select(bill_length_mm:body_mass_g)\nhead(penguins_select_names)\n#> # A tibble: 6 × 4\n#>   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#>            <dbl>         <dbl>             <int>       <int>\n#> 1           39.1          18.7               181        3750\n#> 2           39.5          17.4               186        3800\n#> 3           40.3          18                 195        3250\n#> 4           NA            NA                  NA          NA\n#> 5           36.7          19.3               193        3450\n#> 6           39.3          20.6               190        3650\n\n## Selecionar colunas por padrão\npenguins_select_contains <- penguins %>% \n    dplyr::select(contains(\"_mm\"))\nhead(penguins_select_contains)\n#> # A tibble: 6 × 3\n#>   bill_length_mm bill_depth_mm flipper_length_mm\n#>            <dbl>         <dbl>             <int>\n#> 1           39.1          18.7               181\n#> 2           39.5          17.4               186\n#> 3           40.3          18                 195\n#> 4           NA            NA                  NA\n#> 5           36.7          19.3               193\n#> 6           39.3          20.6               190"},{"path":"cap5.html","id":"pull","chapter":"Capítulo 5 Tidyverse","heading":"5.8.7 pull()","text":"Quando usamos função dplyr::select(), mesmo que para apenas uma coluna, o retorno da função é sempre um tibble. Caso precisemos que essa coluna se torne um vetor dentro encadeamento dos pipes, usamos função dplyr::pull(), que extrai uma única coluna como vetor.","code":"\n## Coluna como vetor\npenguins_select_pull <- penguins %>% \n    dplyr::pull(bill_length_mm)\nhead(penguins_select_pull, 15)\n#>  [1] 39.1 39.5 40.3   NA 36.7 39.3 38.9 39.2 34.1 42.0 37.8 37.8 41.1 38.6 34.6"},{"path":"cap5.html","id":"mutate","chapter":"Capítulo 5 Tidyverse","heading":"5.8.8 mutate()","text":"Uma das operações mais úteis dentre operações para colunas é adicionar ou atualizar os valores de colunas. Para essa operação, usaremos função dplyr::mutate(). Podemos ainda usar os argumentos .e .para indicar onde nova coluna deve ficar, além parâmetro .keep com diversas possibilidades de manter colunas depois de usar função dplyr::mutate(). Por fim, é fundamental destacar o uso das funções de replicação: dplyr::across(), dplyr::if_any() e dplyr::if_all(), para os quais função fará alterações em múltiplas colunas de uma vez, dependendo de resultados booleanos.","code":"\n## Adicionar colunas\npenguins_mutate <- penguins %>% \n    dplyr::mutate(body_mass_kg = body_mass_g/1e3, .before = sex)\nhead(penguins_mutate)\n#> # A tibble: 6 × 9\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g body_mass_kg sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>        <dbl> <fct>  <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750         3.75 male    2007\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800         3.8  female  2007\n#> 3 Adelie  Torgersen           40.3          18                 195        3250         3.25 female  2007\n#> 4 Adelie  Torgersen           NA            NA                  NA          NA        NA    <NA>    2007\n#> 5 Adelie  Torgersen           36.7          19.3               193        3450         3.45 female  2007\n#> 6 Adelie  Torgersen           39.3          20.6               190        3650         3.65 male    2007\n\n## Modificar várias colunas\npenguins_mutate_across <- penguins %>% \n    dplyr::mutate(across(where(is.factor), as.character))\nhead(penguins_mutate_across)\n#> # A tibble: 6 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <chr>   <chr>              <dbl>         <dbl>             <int>       <int> <chr>  <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 4 Adelie  Torgersen           NA            NA                  NA          NA <NA>    2007\n#> 5 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 6 Adelie  Torgersen           39.3          20.6               190        3650 male    2007"},{"path":"cap5.html","id":"arrange","chapter":"Capítulo 5 Tidyverse","heading":"5.8.9 arrange()","text":"Além de operações em colunas, podemos fazer operações em linhas. Vamos começar com reordenação das linhas com base nos valores das colunas. Para essa operação, usamos função dplyr::arrange(). Podemos reordenar por uma ou mais colunas de forma crescente ou decrescente usando função desc() ou o operador - antes da coluna de interesse. Da mesma forma que na função dplyr::mutate(), podemos usar funções de replicação para ordenar linhas para várias colunas de uma vez, dependendo de resultados booleanos.","code":"\n## Reordenar linhas - crescente\npenguins_arrange <- penguins %>% \n    dplyr::arrange(body_mass_g)\nhead(penguins_arrange)\n#> # A tibble: 6 × 8\n#>   species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>     <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Chinstrap Dream               46.9          16.6               192        2700 female  2008\n#> 2 Adelie    Biscoe              36.5          16.6               181        2850 female  2008\n#> 3 Adelie    Biscoe              36.4          17.1               184        2850 female  2008\n#> 4 Adelie    Biscoe              34.5          18.1               187        2900 female  2008\n#> 5 Adelie    Dream               33.1          16.1               178        2900 female  2008\n#> 6 Adelie    Torgersen           38.6          17                 188        2900 female  2009\n\n## Reordenar linhas - decrescente\npenguins_arrange_desc <- penguins %>% \n    dplyr::arrange(desc(body_mass_g))\nhead(penguins_arrange_desc)\n#> # A tibble: 6 × 8\n#>   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n#>   <fct>   <fct>           <dbl>         <dbl>             <int>       <int> <fct> <int>\n#> 1 Gentoo  Biscoe           49.2          15.2               221        6300 male   2007\n#> 2 Gentoo  Biscoe           59.6          17                 230        6050 male   2007\n#> 3 Gentoo  Biscoe           51.1          16.3               220        6000 male   2008\n#> 4 Gentoo  Biscoe           48.8          16.2               222        6000 male   2009\n#> 5 Gentoo  Biscoe           45.2          16.4               223        5950 male   2008\n#> 6 Gentoo  Biscoe           49.8          15.9               229        5950 male   2009\n\n## Reordenar linhas - decrescente\npenguins_arrange_desc_m <- penguins %>% \n    dplyr::arrange(-body_mass_g)\nhead(penguins_arrange_desc_m)\n#> # A tibble: 6 × 8\n#>   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n#>   <fct>   <fct>           <dbl>         <dbl>             <int>       <int> <fct> <int>\n#> 1 Gentoo  Biscoe           49.2          15.2               221        6300 male   2007\n#> 2 Gentoo  Biscoe           59.6          17                 230        6050 male   2007\n#> 3 Gentoo  Biscoe           51.1          16.3               220        6000 male   2008\n#> 4 Gentoo  Biscoe           48.8          16.2               222        6000 male   2009\n#> 5 Gentoo  Biscoe           45.2          16.4               223        5950 male   2008\n#> 6 Gentoo  Biscoe           49.8          15.9               229        5950 male   2009\n\n## Reordenar linhas - multiplas colunas\npenguins_arrange_across <- penguins %>% \n    dplyr::arrange(across(where(is.numeric)))\nhead(penguins_arrange_across)\n#> # A tibble: 6 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Dream               32.1          15.5               188        3050 female  2009\n#> 2 Adelie  Dream               33.1          16.1               178        2900 female  2008\n#> 3 Adelie  Torgersen           33.5          19                 190        3600 female  2008\n#> 4 Adelie  Dream               34            17.1               185        3400 female  2008\n#> 5 Adelie  Torgersen           34.1          18.1               193        3475 <NA>    2007\n#> 6 Adelie  Torgersen           34.4          18.4               184        3325 female  2007"},{"path":"cap5.html","id":"filter","chapter":"Capítulo 5 Tidyverse","heading":"5.8.10 filter()","text":"Uma das principais e mais usuais operações que podemos realizar em linhas é seleção de linhas através filtro por valores de uma ou mais colunas, utilizando função dplyr::filter(). Para realizar os filtros utilizaremos grande parte dos operadores relacionais e lógicos que listamos na Tabela 4.1 Capítulo 4, especialmente os lógicos para combinações de filtros em mais de uma coluna. Além desses operadores, podemos utilizar função .na() para filtros em elementos faltantes, e funções dplyr::() e dplyr::near() para filtros entre valores, e para valores próximos com certa tolerância, respectivamente. Por fim, podemos usar funções de replicação para filtro das linhas para mais de uma coluna, dependendo de resultados booleanos.","code":"\n## Filtrar linhas\npenguins_filter <- penguins %>% \n    dplyr::filter(species == \"Adelie\")\nhead(penguins_filter)\n#> # A tibble: 6 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 4 Adelie  Torgersen           NA            NA                  NA          NA <NA>    2007\n#> 5 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 6 Adelie  Torgersen           39.3          20.6               190        3650 male    2007\n\n## Filtrar linhas\npenguins_filter_two <- penguins %>% \n    dplyr::filter(species == \"Adelie\" & sex == \"female\")\nhead(penguins_filter_two)\n#> # A tibble: 6 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 2 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 3 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 4 Adelie  Torgersen           38.9          17.8               181        3625 female  2007\n#> 5 Adelie  Torgersen           41.1          17.6               182        3200 female  2007\n#> 6 Adelie  Torgersen           36.6          17.8               185        3700 female  2007\n\n## Filtrar linhas\npenguins_filter_in <- penguins %>% \n    dplyr::filter(species %in% c(\"Adelie\", \"Gentoo\"),\n                  sex == \"female\")\nhead(penguins_filter_in)\n#> # A tibble: 6 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 2 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 3 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 4 Adelie  Torgersen           38.9          17.8               181        3625 female  2007\n#> 5 Adelie  Torgersen           41.1          17.6               182        3200 female  2007\n#> 6 Adelie  Torgersen           36.6          17.8               185        3700 female  2007\n\n## Filtrar linhas - NA\npenguins_filter_na <- penguins %>% \n    dplyr::filter(!is.na(sex) == TRUE)\nhead(penguins_filter_na)\n#> # A tibble: 6 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 4 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 5 Adelie  Torgersen           39.3          20.6               190        3650 male    2007\n#> 6 Adelie  Torgersen           38.9          17.8               181        3625 female  2007\n\n## Filtrar linhas - intervalos\npenguins_filter_between <- penguins %>% \n    dplyr::filter(between(body_mass_g, 3000, 4000))\nhead(penguins_filter_between)\n#> # A tibble: 6 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 4 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 5 Adelie  Torgersen           39.3          20.6               190        3650 male    2007\n#> 6 Adelie  Torgersen           38.9          17.8               181        3625 female  2007\n\n## Filtrar linhas por várias colunas\npenguins_filter_if <- penguins %>% \n    dplyr::filter(if_all(where(is.integer), ~ . > 200))\nhead(penguins_filter_if)\n#> # A tibble: 6 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Dream               35.7          18                 202        3550 female  2008\n#> 2 Adelie  Dream               41.1          18.1               205        4300 male    2008\n#> 3 Adelie  Dream               40.8          18.9               208        4300 male    2008\n#> 4 Adelie  Biscoe              41            20                 203        4725 male    2009\n#> 5 Adelie  Torgersen           41.4          18.5               202        3875 male    2009\n#> 6 Adelie  Torgersen           44.1          18                 210        4000 male    2009"},{"path":"cap5.html","id":"slice","chapter":"Capítulo 5 Tidyverse","heading":"5.8.11 slice()","text":"Além da seleção de linhas por filtros, podemos fazer seleção das linhas por intervalos, indicando quais linhas desejamos, usando função dplyr::slice(), e informando o argumento n para o número da linha ou intervalo das linhas. Essa função possui variações sufixo muito interessantes: dplyr::slice_head() e dplyr::slice_tail() seleciona primeiras e últimas linhas, dplyr::slice_min() e dplyr::slice_max() seleciona linhas com os maiores e menores valores de uma coluna, e dplyr::slice_sample() seleciona linhas aleatoriamente.","code":"\n## Seleciona linhas\npenguins_slice <- penguins %>% \n    dplyr::slice(n = c(1, 3, 300:n()))\nhead(penguins_slice)\n#> # A tibble: 6 × 8\n#>   species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>     <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie    Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 Adelie    Torgersen           40.3          18                 195        3250 female  2007\n#> 3 Chinstrap Dream               50.6          19.4               193        3800 male    2007\n#> 4 Chinstrap Dream               46.7          17.9               195        3300 female  2007\n#> 5 Chinstrap Dream               52            19                 197        4150 male    2007\n#> 6 Chinstrap Dream               50.5          18.4               200        3400 female  2008\n\n## Seleciona linhas - head\npenguins_slice_head <- penguins %>% \n    dplyr::slice_head(n = 5)\nhead(penguins_slice_head)\n#> # A tibble: 5 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 4 Adelie  Torgersen           NA            NA                  NA          NA <NA>    2007\n#> 5 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n\n## Seleciona linhas - max\npenguins_slice_max <- penguins %>% \n    dplyr::slice_max(body_mass_g, n = 5)\nhead(penguins_slice_max)\n#> # A tibble: 6 × 8\n#>   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n#>   <fct>   <fct>           <dbl>         <dbl>             <int>       <int> <fct> <int>\n#> 1 Gentoo  Biscoe           49.2          15.2               221        6300 male   2007\n#> 2 Gentoo  Biscoe           59.6          17                 230        6050 male   2007\n#> 3 Gentoo  Biscoe           51.1          16.3               220        6000 male   2008\n#> 4 Gentoo  Biscoe           48.8          16.2               222        6000 male   2009\n#> 5 Gentoo  Biscoe           45.2          16.4               223        5950 male   2008\n#> 6 Gentoo  Biscoe           49.8          15.9               229        5950 male   2009\n\n## Seleciona linhas - sample\npenguins_slice_sample <- penguins %>% \n    dplyr::slice_sample(n = 30)\nhead(penguins_slice_sample)\n#> # A tibble: 6 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Biscoe              41.3          21.1               195        4400 male    2008\n#> 2 Gentoo  Biscoe              44.5          15.7               217        4875 <NA>    2009\n#> 3 Adelie  Torgersen           41.4          18.5               202        3875 male    2009\n#> 4 Adelie  Biscoe              37.6          17                 185        3600 female  2008\n#> 5 Adelie  Dream               36            17.9               190        3450 female  2007\n#> 6 Adelie  Biscoe              35.7          16.9               185        3150 female  2008"},{"path":"cap5.html","id":"distinct","chapter":"Capítulo 5 Tidyverse","heading":"5.8.12 distinct()","text":"última operação que apresentaremos para linhas é retirada de linhas com valores repetidos com base nos valores de uma ou mais colunas, utilizando função dplyr::distinct(). Essa função por padrão retorna apenas (s) coluna(s) utilizada(s) para retirar linhas com valores repetidos, sendo necessário acrescentar o argumento .keep_all = TRUE para retornar todas colunas. Por fim, podemos usar funções de replicação para retirar linhas com valores repetidos para mais de uma coluna, dependendo de resultados booleanos.","code":"\n## Retirar linhas com valores repetidos\npenguins_distinct <- penguins %>% \n    dplyr::distinct(body_mass_g)\nhead(penguins_distinct)\n#> # A tibble: 6 × 1\n#>   body_mass_g\n#>         <int>\n#> 1        3750\n#> 2        3800\n#> 3        3250\n#> 4          NA\n#> 5        3450\n#> 6        3650\n\n## Retirar linhas com valores repetidos - manter as outras colunas\npenguins_distinct_keep_all <- penguins %>% \n    dplyr::distinct(body_mass_g, .keep_all = TRUE)\nhead(penguins_distinct_keep_all)\n#> # A tibble: 6 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 4 Adelie  Torgersen           NA            NA                  NA          NA <NA>    2007\n#> 5 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 6 Adelie  Torgersen           39.3          20.6               190        3650 male    2007\n\n## Retirar linhas com valores repetidos para várias colunas\npenguins_distinct_keep_all_across <- penguins %>% \n    dplyr::distinct(across(where(is.integer)), .keep_all = TRUE)\nhead(penguins_distinct_keep_all_across)\n#> # A tibble: 6 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 4 Adelie  Torgersen           NA            NA                  NA          NA <NA>    2007\n#> 5 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 6 Adelie  Torgersen           39.3          20.6               190        3650 male    2007"},{"path":"cap5.html","id":"count","chapter":"Capítulo 5 Tidyverse","heading":"5.8.13 count()","text":"Agora entraremos assunto de resumo das observações. Podemos fazer contagens resumos dos nossos dados, utilizando para isso função dplyr::count(). Essa função contará valores de uma ou mais colunas, geralmente para variáveis categóricas, semelhante à função R Base table(), mas num contexto tidyverse.","code":"\n## Contagens de valores para uma coluna\npenguins_count <- penguins %>% \n    dplyr::count(species)\npenguins_count\n#> # A tibble: 3 × 2\n#>   species       n\n#>   <fct>     <int>\n#> 1 Adelie      152\n#> 2 Chinstrap    68\n#> 3 Gentoo      124\n\n## Contagens de valores para mais de uma coluna\npenguins_count_two <- penguins %>% \n    dplyr::count(species, island)\npenguins_count_two\n#> # A tibble: 5 × 3\n#>   species   island        n\n#>   <fct>     <fct>     <int>\n#> 1 Adelie    Biscoe       44\n#> 2 Adelie    Dream        56\n#> 3 Adelie    Torgersen    52\n#> 4 Chinstrap Dream        68\n#> 5 Gentoo    Biscoe      124"},{"path":"cap5.html","id":"group_by","chapter":"Capítulo 5 Tidyverse","heading":"5.8.14 group_by()","text":"Uma grande parte das operações feitas nos dados são realizadas em grupos definidos por valores de colunas com dados categóricas. função dplyr::group_by() transforma um tibble em um tibble grouped, onde operações são realizadas “por grupo”. Essa função é utilizada geralmente junto com função dplyr::summarise(), que veremos logo em seguida. O agrupamento não altera aparência dos dados (além de informar como estão agrupados). função dplyr::ungroup() remove o agrupamento. Podemos ainda usar funções de replicação para fazer os agrupamentos para mais de uma coluna, dependendo de resultados booleanos.","code":"\n## Agrupamento\npenguins_group_by <- penguins %>% \n    dplyr::group_by(species)\nhead(penguins_group_by)\n#> # A tibble: 6 × 8\n#> # Groups:   species [1]\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 4 Adelie  Torgersen           NA            NA                  NA          NA <NA>    2007\n#> 5 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 6 Adelie  Torgersen           39.3          20.6               190        3650 male    2007\n\n## Agrupamento de várias colunas\npenguins_group_by_across <- penguins %>% \n    dplyr::group_by(across(where(is.factor)))\nhead(penguins_group_by_across)\n#> # A tibble: 6 × 8\n#> # Groups:   species, island, sex [3]\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 4 Adelie  Torgersen           NA            NA                  NA          NA <NA>    2007\n#> 5 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 6 Adelie  Torgersen           39.3          20.6               190        3650 male    2007"},{"path":"cap5.html","id":"summarise","chapter":"Capítulo 5 Tidyverse","heading":"5.8.15 summarise()","text":"Como dissemos, muitas vezes queremos resumir nossos dados, principalmente para ter uma noção geral das variáveis (colunas) ou mesmo começar análise exploratória resumindo variáveis contínuas por grupos de variáveis categóricas. Dessa forma, ao utilizar função dplyr::summarise() teremos um novo tibble com os dados resumidos, que é agregação ou resumo dos dados através de funções. Da mesma forma que outras funções, podemos usar funções de replicação para resumir valores para mais de uma coluna, dependendo de resultados booleanos.","code":"\n## Resumo\npenguins_summarise <- penguins %>% \n    dplyr::group_by(species) %>% \n    dplyr::summarize(body_mass_g_mean = mean(body_mass_g, na.rm = TRUE),\n                     body_mass_g_sd = sd(body_mass_g, na.rm = TRUE))\npenguins_summarise\n#> # A tibble: 3 × 3\n#>   species   body_mass_g_mean body_mass_g_sd\n#>   <fct>                <dbl>          <dbl>\n#> 1 Adelie               3701.           459.\n#> 2 Chinstrap            3733.           384.\n#> 3 Gentoo               5076.           504.\n\n## Resumo para várias colunas\npenguins_summarise_across <- penguins %>% \n    dplyr::group_by(species) %>% \n    dplyr::summarize(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)))\npenguins_summarise_across\n#> # A tibble: 3 × 6\n#>   species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n#>   <fct>              <dbl>         <dbl>             <dbl>       <dbl> <dbl>\n#> 1 Adelie              38.8          18.3              190.       3701. 2008.\n#> 2 Chinstrap           48.8          18.4              196.       3733. 2008.\n#> 3 Gentoo              47.5          15.0              217.       5076. 2008."},{"path":"cap5.html","id":"bind_rows-e-bind_cols","chapter":"Capítulo 5 Tidyverse","heading":"5.8.16 bind_rows() e bind_cols()","text":"Muitas vezes teremos de combinar duas ou mais tabelas de dados. Podemos utilizar funções R Base rbind() e cbind(), como vimos Capítulo 4. Entretanto, pode ser interessante avançar para funções dplyr::bind_rows() e dplyr::bind_cols() formato tidyverse. ideia é muito semelhante: primeira função combina dados por linhas e segunda por colunas. Entretanto, há algumas vantagens uso dessas funções, como identificação das linhas pelo argumento .id para primeira função, e conferência nome das colunas pelo argumento .name_repair para segunda função.","code":"\n## Selecionar as linhas para dois tibbles\npenguins_01 <- dplyr::slice(penguins, 1:5)\npenguins_02 <- dplyr::slice(penguins, 51:55)\n\n## Combinar as linhas\npenguins_bind_rows <- dplyr::bind_rows(penguins_01, penguins_02, .id = \"id\")\nhead(penguins_bind_rows)\n#> # A tibble: 6 × 9\n#>   id    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n#>   <chr> <fct>   <fct>              <dbl>         <dbl>             <int>       <int> <fct>  <int>\n#> 1 1     Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n#> 2 1     Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n#> 3 1     Adelie  Torgersen           40.3          18                 195        3250 female  2007\n#> 4 1     Adelie  Torgersen           NA            NA                  NA          NA <NA>    2007\n#> 5 1     Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n#> 6 2     Adelie  Biscoe              39.6          17.7               186        3500 female  2008\n\n## Combinar as colunas\npenguins_bind_cols <- dplyr::bind_cols(penguins_01, penguins_02, .name_repair = \"unique\")\nhead(penguins_bind_cols)\n#> # A tibble: 5 × 16\n#>   species...1 island...2 bill_length_mm...3 bill_depth_mm...4 flipper_length_mm...5 body_mass_g...6 sex...7 year...8 species...9 island...10\n#>   <fct>       <fct>                   <dbl>             <dbl>                 <int>           <int> <fct>      <int> <fct>       <fct>      \n#> 1 Adelie      Torgersen                39.1              18.7                   181            3750 male        2007 Adelie      Biscoe     \n#> 2 Adelie      Torgersen                39.5              17.4                   186            3800 female      2007 Adelie      Biscoe     \n#> 3 Adelie      Torgersen                40.3              18                     195            3250 female      2007 Adelie      Biscoe     \n#> 4 Adelie      Torgersen                NA                NA                      NA              NA <NA>        2007 Adelie      Biscoe     \n#> 5 Adelie      Torgersen                36.7              19.3                   193            3450 female      2007 Adelie      Biscoe     \n#> # … with 6 more variables: bill_length_mm...11 <dbl>, bill_depth_mm...12 <dbl>, flipper_length_mm...13 <int>, body_mass_g...14 <int>,\n#> #   sex...15 <fct>, year...16 <int>"},{"path":"cap5.html","id":"join","chapter":"Capítulo 5 Tidyverse","heading":"5.8.17 *_join()","text":"Finalmente, veremos o último conjunto de funções pacote dplyr, junção de tabelas. Nessa operação, fazemos combinação de pares de conjunto de dados tabulares por uma ou mais colunas chaves. Há dois tipos de junções: junção de modificação e junção de filtragem. junção de modificação primeiro combina observações por suas chaves e, em seguida, copia variáveis (colunas) de uma tabela para outra. É fundamental destacar importância da coluna chave, que é indicada pelo argumento . Essa coluna deve conter elementos que sejam comuns às duas tabelas para que haja combinação dos elementos.Existem quatro tipos de junções de modificações, que são realizadas pelas funções: dplyr::inner_join(), dplyr::left_join(), dplyr::full_join() e dplyr::right_join(), e que podem ser representadas na Figura 5.3.\nFigura 5.3: Diferentes tipos de joins, representados com um diagrama de Venn. Adaptado de: Wickham & Grolemund (2017).\nConsiderando nomenclatura de duas tabelas de dados por x e y, temos:inner_join(x, y): mantém apenas observações em x e em yleft_join(x, y): mantém todas observações em xright_join(x, y): mantém todas observações em yfull_join(x, y): mantém todas observações em x e em yAqui, vamos demostrar apenas função dplyr::left_join(), combinando um tibble de coordenadas geográficas das ilhas com o conjunto de dados penguins.Já o segundo tipo de junção, junção de filtragem combina observações da mesma maneira que junções de modificação, mas afetam observações (linhas), não variáveis (colunas). Existem dois tipos.semi_join(x, y): mantém todas observações em x que têm uma correspondência em yanti_join(x, y): elimina todas observações em x que têm uma correspondência em yDe forma geral, semi-joins são úteis para corresponder tabelas de resumo filtradas de volta às linhas originais, removendo linhas que não estavam antes join. Já anti-joins são úteis para diagnosticar incompatibilidades de junção, por exemplo, ao verificar os elementos que não combinam entre duas tabelas de dados.","code":"\n## Adicionar uma coluna chave de ids\npenguin_islands <- tibble(\n    island = c(\"Torgersen\", \"Biscoe\", \"Dream\", \"Alpha\"),\n    longitude = c(-64.083333, -63.775636, -64.233333, -63),\n    latitude = c(-64.766667, -64.818569, -64.733333, -64.316667))\n\n## Junção - left\npenguins_left_join <- dplyr::left_join(penguins, penguin_islands, by = \"island\")\nhead(penguins_left_join)\n#> # A tibble: 6 × 10\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year longitude latitude\n#>   <fct>   <chr>              <dbl>         <dbl>             <int>       <int> <fct>  <int>     <dbl>    <dbl>\n#> 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007     -64.1    -64.8\n#> 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007     -64.1    -64.8\n#> 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007     -64.1    -64.8\n#> 4 Adelie  Torgersen           NA            NA                  NA          NA <NA>    2007     -64.1    -64.8\n#> 5 Adelie  Torgersen           36.7          19.3               193        3450 female  2007     -64.1    -64.8\n#> 6 Adelie  Torgersen           39.3          20.6               190        3650 male    2007     -64.1    -64.8"},{"path":"cap5.html","id":"operações-de-conjuntos-e-comparação-de-dados","chapter":"Capítulo 5 Tidyverse","heading":"5.8.18 Operações de conjuntos e comparação de dados","text":"Temos ainda operações de conjuntos e comparação de dados.union(x, y): retorna todas linhas que aparecem em x, y ou mais dos conjuntos de dadosinteresect(x, y): retorna apenas linhas que aparecem em x e em ysetdiff(x, y): retorna linhas que aparecem x, mas não em ysetequal(x, y): retorna se x e y são iguais e quais suas diferençasPara se aprofundar tema, recomendamos leitura Capítulo 13 Relational data de Wickham & Grolemund (2017).","code":""},{"path":"cap5.html","id":"stringr","chapter":"Capítulo 5 Tidyverse","heading":"5.9 stringr","text":"O pacote stringr fornece um conjunto de funções para manipulação de caracteres ou strings. O pacote concentra-se nas funções de manipulação mais importantes e comumente usadas. Para funções mais específicas, recomenda-se usar o pacote stringi, que fornece um conjunto mais abrangente de funções. funções stringr podem ser agrupadas em algumas operações para tarefas específicas como: ) correspondência de padrões, ii) retirar e acrescentar espaços em branco, iii) mudar maiúsculas e minúsculas, além de muitas outras operações com caracteres.Todas funções deste pacote são listadas na página de referência pacote.Demonstraremos algumas funções para algumas operações mais comuns, utilizando um vetor de um elemento, com o string “penguins”.Podemos explorar o comprimento de strings com função stringr::str_length().Extrair um string por sua posição usando função stringr::str_sub() ou por um padrão com stringr::str_extract().Substituir strings por outros strings com stringr::str_replace().Separar strings por um padrão com função stringr::str_split().Inserir espaços em brancos pela esquerda, direita ou ambos com função stringr::str_pad().Também podemos remover espaços em branco da esquerda, direita ou ambos, utilizando stringr::str_trim().Podemos também alterar minúsculas e maiúsculas em diferentes posições string, com várias funções.Podemos ainda ordenar os elementos de um vetor por ordem alfabética de forma crescente ou decrescente, usando stringr::str_sort().Podemos ainda utilizar essas funções em complemento com o pacote dplyr, para alterar os strings de colunas ou nome das colunas.Para se aprofundar tema, recomendamos leitura Capítulo 14 Strings de Wickham & Grolemund (2017).","code":"\n## Comprimento\nstringr::str_length(string = \"penguins\")\n#> [1] 8\n## Extrair pela posição\nstringr::str_sub(string = \"penguins\", end = 3)\n#> [1] \"pen\"\n\n## Extrair por padrão\nstringr::str_extract(string = \"penguins\", pattern = \"p\")\n#> [1] \"p\"\n## Substituir\nstringr::str_replace(string = \"penguins\", pattern = \"i\", replacement = \"y\")\n#> [1] \"penguyns\"\n## Separar\nstringr::str_split(string = \"p-e-n-g-u-i-n-s\", pattern = \"-\", simplify = TRUE)\n#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n#> [1,] \"p\"  \"e\"  \"n\"  \"g\"  \"u\"  \"i\"  \"n\"  \"s\"\n## Inserir espacos em branco\nstringr::str_pad(string = \"penguins\", width = 10, side = \"left\")\n#> [1] \"  penguins\"\nstringr::str_pad(string = \"penguins\", width = 10, side = \"right\")\n#> [1] \"penguins  \"\nstringr::str_pad(string = \"penguins\", width = 10, side = \"both\")\n#> [1] \" penguins \"\n## Remover espacos em branco\nstringr::str_trim(string = \" penguins \", side = \"left\")\n#> [1] \"penguins \"\nstringr::str_trim(string = \" penguins \", side = \"right\")\n#> [1] \" penguins\"\nstringr::str_trim(string = \" penguins \", side = \"both\")\n#> [1] \"penguins\"\n## Alterar minúsculas e maiúsculas\nstringr::str_to_lower(string = \"Penguins\")\n#> [1] \"penguins\"\nstringr::str_to_upper(string = \"penguins\")\n#> [1] \"PENGUINS\"\nstringr::str_to_sentence(string = \"penGuins\")\n#> [1] \"Penguins\"\nstringr::str_to_title(string = \"penGuins\")\n#> [1] \"Penguins\"\n## Ordenar\nstringr::str_sort(x = letters)\n#>  [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\nstringr::str_sort(x = letters, dec = TRUE)\n#>  [1] \"z\" \"y\" \"x\" \"w\" \"v\" \"u\" \"t\" \"s\" \"r\" \"q\" \"p\" \"o\" \"n\" \"m\" \"l\" \"k\" \"j\" \"i\" \"h\" \"g\" \"f\" \"e\" \"d\" \"c\" \"b\" \"a\"\n## Alterar valores das colunas\npenguins_stringr_valores <- penguins %>% \n    dplyr::mutate(species = stringr::str_to_lower(species))\n\n## Alterar nome das colunas\npenguins_stringr_nomes <- penguins %>% \n    dplyr::rename_with(stringr::str_to_title)"},{"path":"cap5.html","id":"forcats","chapter":"Capítulo 5 Tidyverse","heading":"5.10 forcats","text":"O pacote forcats fornece um conjunto de ferramentas úteis para facilitar manipulação de fatores. Como dito Capítulo 4, usamos fatores geralmente quando temos dados categóricos, que são variáveis que possuem um conjunto de valores fixos e conhecidos. funções são utilizadas principalmente para: ) mudar ordem dos níveis, ii) mudar os valores dos níveis, iii) adicionar e remover níveis, iv) combinar múltiplos níveis, além de outras operações.Todas funções deste pacote são listadas na página de referência pacote.Vamos utilizar ainda os dados penguins e penguins_raw para exemplificar o uso pacote forcats.Primeiramente, vamos converter dados de string para fator, utilizando função forcats::as_factor().Podemos facilmente mudar o nome dos níveis utilizando função forcats::fct_recode().Para inverter os níveis, usamos função forcats::fct_rev().Uma operação muito comum com fatores é mudar ordem dos níveis. Quando precisamos especificar ordem dos níveis, podemos fazer essa operação manualmente com função forcats::fct_relevel().Como vimos, reordenação dos níveis pode ser feita manualmente. Mas existem outras formas automáticas de reordenação seguindo algumas regras, para quais existem funções específicas.forcats::fct_inorder(): pela ordem em que aparecem pela primeira vezforcats::fct_infreq(): por número de observações com cada nível (decrescente, .e., o maior primeiro)forcats::fct_inseq(): pelo valor numérico nívelPor fim, podemos fazer agregação de níveis raros em um nível utilizando função forcats::fct_lump().Podemos ainda utilizar essas funções em complemento com o pacote dplyr para fazer manipulações de fatores nas colunas de tibbles.Para se aprofundar tema, recomendamos leitura Capítulo 15 Factors de Wickham & Grolemund (2017).","code":"\n## Carregar o pacote palmerpenguins\nlibrary(palmerpenguins)\n## String\nforcats::as_factor(penguins_raw$Species) %>% head()\n#> [1] Adelie Penguin (Pygoscelis adeliae) Adelie Penguin (Pygoscelis adeliae) Adelie Penguin (Pygoscelis adeliae)\n#> [4] Adelie Penguin (Pygoscelis adeliae) Adelie Penguin (Pygoscelis adeliae) Adelie Penguin (Pygoscelis adeliae)\n#> Levels: Adelie Penguin (Pygoscelis adeliae) Gentoo penguin (Pygoscelis papua) Chinstrap penguin (Pygoscelis antarctica)\n## Mudar o nome dos níveis\nforcats::fct_recode(penguins$species, a = \"Adelie\", c = \"Chinstrap\", g = \"Gentoo\") %>% head()\n#> [1] a a a a a a\n#> Levels: a c g\n## Inverter os níveis\nforcats::fct_rev(penguins$species) %>% head()\n#> [1] Adelie Adelie Adelie Adelie Adelie Adelie\n#> Levels: Gentoo Chinstrap Adelie\n## Especificar a ordem dos níveis\nforcats::fct_relevel(penguins$species, \"Chinstrap\", \"Gentoo\", \"Adelie\") %>% head()\n#> [1] Adelie Adelie Adelie Adelie Adelie Adelie\n#> Levels: Chinstrap Gentoo Adelie\n## Níveis pela ordem em que aparecem\nforcats::fct_inorder(penguins$species) %>% head()\n#> [1] Adelie Adelie Adelie Adelie Adelie Adelie\n#> Levels: Adelie Gentoo Chinstrap\n\n## Ordem (decrescente) de frequência\nforcats::fct_infreq(penguins$species) %>% head()\n#> [1] Adelie Adelie Adelie Adelie Adelie Adelie\n#> Levels: Adelie Gentoo Chinstrap\n## Agregação de níveis raros em um nível\nforcats::fct_lump(penguins$species) %>% head()\n#> [1] Adelie Adelie Adelie Adelie Adelie Adelie\n#> Levels: Adelie Gentoo Other\n## Transformar várias colunas em fator\npenguins_raw_multi_factor <- penguins_raw %>% \n    dplyr::mutate(across(where(is.character), forcats::as_factor))"},{"path":"cap5.html","id":"lubridate","chapter":"Capítulo 5 Tidyverse","heading":"5.11 lubridate","text":"O pacote lubridate fornece um conjunto de funções para manipulação de dados de data e horário. Dessa forma, esse pacote facilita manipulação dessa classe de dado R, pois geralmente esses dados não são intuitivos e mudam dependendo tipo de objeto de data e horário. Além disso, os métodos que usam datas e horários devem levar em consideração fusos horários, anos bissextos, horários de verão, além de outras particularidades. Existem diversas funções nesse pacote, sendo mesmas focadas em: ) transformações de data/horário, ii) componentes, iii) arredondamentos, iv) durações, v) períodos, vi) intervalos, além de muitas outras funções específicas.Todas funções deste pacote são listadas na página de referência pacote.Apesar de estar inserido escopo tidyverse, este pacote não é carregado com os demais, requisitando seu carregamento solo.Existem três tipos de dados data/horário:Data: tempo em dias, meses e anos <date>Horário: tempo dentro de um dia <time>Data-horário: tempo em um instante (data mais tempo) <dttm>Para trabalhar exclusivamente com horários, podemos utilizar o pacote hms.É fundamental também destacar que algumas letras terão um significado temporal, sendo abreviações de diferentes períodos em inglês: year (ano), month (mês), weak (semana), day (dia), hour (hora), minute (minuto), e second (segundo).Para acessar informação da data e horários atuais podemos utilizar funções lubridate::today() e lubridate::now().Além dessas informações instantâneas, existem três maneiras de criar um dado de data/horário.De um stringDe componentes individuais de data e horárioDe um objeto de data/horário existenteOs dados de data/horário geralmente estão formato de strings. Podemos transformar os dados especificando ordem dos seus componentes, ou seja, ordem em que ano, mês e dia aparecem string, usando letras y (ano), m (mês) e d (dia) na mesma ordem, por exemplo, lubridate::dmy().Essas funções também aceitam números sem aspas, além de serem muito versáteis e funcionarem em outros diversos formatos.Além da data, podemos especificar horários atrelados essas datas. Para criar uma data com horário adicionamos um underscore (_) e h (hora), m (minuto) e s (segundo) ao nome da função, além argumento tz para especificar o fuso horário (tema tratado mais adiante nessa seção).Podemos ainda ter componentes individuais de data/horário em múltiplas colunas. Para realizar essa transformação, podemos usar funções lubridate::make_date() e lubridate::make_datetime().Por fim, podemos criar datas modificando entre data/horário e data, utilizando funções lubridate::as_datetime() e lubridate::as_date().Uma vez que entendemos como podemos criar dados de data/horário, podemos explorar funções para acessar e definir componentes individuais. Para essa tarefa existe uma grande quantidade de funções para acessar partes específicas de datas e horários.year(): acessa o anomonth(): acessa o mêsmonth(): acessa o diayday(): acessa o dia anomday(): acessa o dia mêswday(): acessa o dia da semanahour(): acessa horasminute(): acessa os minutossecond(): acessa os segundosAlém de acessar componentes de datas e horários, podemos usar essas funções para fazer inclusão de informações de datas e horários.Mais convenientemente, podemos utilizar função update() para alterar vários valores de uma vez.Muitas vezes precisamos fazer operações com datas, como: adição, subtração, multiplicação e divisão. Para tanto, é preciso entender três classes importantes que representam intervalos de tempo.Durações: representam um número exato de segundosPeríodos: representam unidades humanas como semanas e mesesIntervalos: representam um ponto inicial e finalQuando fazemos uma subtração de datas, criamos um objeto da classe difftime. Essa classe pode ser um pouco complicada de trabalhar, então dentro lubridate usamos funções que convertem essa classe em duração, da classe Duration. durações sempre registram o intervalo de tempo em segundos, com alguma unidade de tempo maior entre parênteses. Há uma série de funções para tratar dessa classe.duration(): cria data em duraçãoas.duration(): converte datas em duraçãodyears(): duração de anosdmonths(): duração de mesesdweeks(): duração de semanasddays(): duração de diasdhours(): duração de horasdminutes(): duração de minutosdseconds(): duração de segundosPodemos ainda utilizar durações para fazer operações aritméticas com datas como adição, subtração e multiplicação.Além das durações, podemos usar períodos, que são extensões de tempo não fixados em segundos como durações, mas flexíveis, com o tempo em dias, semanas, meses ou anos, permitindo uma interpretação mais intuitiva das datas. Novamente, há uma série de funções para realizar essas operações.period(): cria data em períodoas.period(): converte datas em períodoseconds(): período em segundosminutes(): período em minutoshours(): período em horasdays(): período em diasweeks(): período em semanasmonths(): período em mesesyears(): período em anosAlém disso, podemos fazer operações com os períodos, somando e subtraindo.Por fim, intervalos são períodos de tempo limitados por duas datas, possuindo uma duração com um ponto de partida, que o faz preciso para determinar uma duração. Intervalos são objetos da classe Interval. Da mesma forma que para duração e períodos, há uma série de funções para realizar essas operações.interval(): cria data em intervalo%--%: cria data em intervaloas.interval(): converte datas em intervaloint_start(): acessa ou atribui data inicial de um intervaloint_end(): acessa ou atribui data final de um intervaloint_length(): comprimento de um intervalo em segundosint_flip(): inverte ordem da data de início e da data de término em um intervaloint_shift(): desloca datas de início e término de um intervaloint_aligns(): testa se dois intervalos compartilham um ponto finalint_standardize(): garante que todos os intervalos sejam positivosint_diff(): retorna os intervalos que ocorrem entre os elementos de data/horárioint_overlaps(): testa se dois intervalos se sobrepõem%within%: testa se o primeiro intervalo está contido segundoUma operação de destaque é verificar sobreposição entre dois intervalos.Podemos ainda calcular quantos períodos existem dentro de um intervalo, utilizando operações de / e %/%.Ainda podemos fazer transformações dos dados para períodos e ter todas unidades de data e tempo que o intervalo compreende.Por fim, fusos horários tendem ser um fator complicador quando precisamos analisar informações instantâneas de tempo (horário) de outras partes planeta, ou mesmo fazer conversões dos horários. lubridate há funções para ajudar nesse sentido. Para isso, podemos utilizar função lubridate::with_tz() e argumento tzone informar o fuso horário para transformação horário.Podemos descobrir o fuso horário que o R está considerando com função Sys.timezone().R há uma listagem dos nomes dos fusos horários que podemos utilizar argumento tzone para diferentes fusos horários.Podemos nos perguntar que horas são em outra parte globo ou fazer conversões facilmente lubridate.Para se aprofundar tema, recomendamos leitura Capítulo 16 Dates times de Wickham & Grolemund (2017).","code":"\n## Carregar\nlibrary(lubridate)\n## Extrair a data nesse instante\nlubridate::today()\n#> [1] \"2022-06-08\"\n\n## Extrair a data e tempo nesse instante\nlubridate::now()\n#> [1] \"2022-06-08 00:04:44 -03\"\n## Strings e números para datas\nlubridate::dmy(\"03-03-2021\")\n#> [1] \"2021-03-03\"\n## Strings e números para datas\nlubridate::dmy(\"03-Mar-2021\")\nlubridate::dmy(03032021)\nlubridate::dmy(\"03032021\")\nlubridate::dmy(\"03/03/2021\")\nlubridate::dmy(\"03.03.2021\")\n## Especificar horários e fuso horário\nlubridate::dmy_h(\"03-03-2021 13\")\n#> [1] \"2021-03-03 13:00:00 UTC\"\nlubridate::dmy_hm(\"03-03-2021 13:32\")\n#> [1] \"2021-03-03 13:32:00 UTC\"\nlubridate::dmy_hms(\"03-03-2021 13:32:01\")\n#> [1] \"2021-03-03 13:32:01 UTC\"\nlubridate::dmy_hms(\"03-03-2021 13:32:01\", tz = \"America/Sao_Paulo\")\n#> [1] \"2021-03-03 13:32:01 -03\"\n## Dados com componentes individuais\ndados <- tibble::tibble(\n    ano = c(2021, 2021, 2021),\n    mes = c(1, 2, 3),\n    dia = c(12, 20, 31),\n    hora = c(2, 14, 18), \n    minuto = c(2, 44, 55))\n\n## Data de componentes individuais\ndados %>% \n    dplyr::mutate(data = lubridate::make_datetime(ano, mes, dia, hora, minuto))\n#> # A tibble: 3 × 6\n#>     ano   mes   dia  hora minuto data               \n#>   <dbl> <dbl> <dbl> <dbl>  <dbl> <dttm>             \n#> 1  2021     1    12     2      2 2021-01-12 02:02:00\n#> 2  2021     2    20    14     44 2021-02-20 14:44:00\n#> 3  2021     3    31    18     55 2021-03-31 18:55:00\n## Data para data-horário\nlubridate::as_datetime(today())\n#> [1] \"2022-06-08 UTC\"\n\n## Data-horário para data\nlubridate::as_date(now())\n#> [1] \"2022-06-08\"\n## Extrair\nlubridate::year(now())\n#> [1] 2022\nlubridate::month(now())\n#> [1] 6\nlubridate::month(now(), label = TRUE)\n#> [1] Jun\n#> Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < Oct < Nov < Dec\nlubridate::day(now())\n#> [1] 8\nlubridate::wday(now())\n#> [1] 4\nlubridate::wday(now(), label = TRUE)\n#> [1] Wed\n#> Levels: Sun < Mon < Tue < Wed < Thu < Fri < Sat\nlubridate::second(now())\n#> [1] 44.33378\n## Data\ndata <- dmy_hms(\"04-03-2021 01:04:56\")\n\n## Incluir\nlubridate::year(data) <- 2020\nlubridate::month(data) <- 01\nlubridate::hour(data) <- 13\n## Incluir vários valores\nupdate(data, year = 2020, month = 1, mday = 1, hour = 1)\n#> [1] \"2020-01-01 01:04:56 UTC\"\n## Subtração de datas\ntempo_estudando_r <- lubridate::today() - lubridate::dmy(\"30-11-2011\")\n\n## Conversão para duração\ntempo_estudando_r_dur <- lubridate::as.duration(tempo_estudando_r)\n\n## Criando durações\nlubridate::duration(90, \"seconds\")\n#> [1] \"90s (~1.5 minutes)\"\nlubridate::duration(1.5, \"minutes\")\n#> [1] \"90s (~1.5 minutes)\"\nlubridate::duration(1, \"days\")\n#> [1] \"86400s (~1 days)\"\n\n## Transformação da duração\nlubridate::dseconds(100)\n#> [1] \"100s (~1.67 minutes)\"\nlubridate::dminutes(100)\n#> [1] \"6000s (~1.67 hours)\"\nlubridate::dhours(100)\n#> [1] \"360000s (~4.17 days)\"\nlubridate::ddays(100)\n#> [1] \"8640000s (~14.29 weeks)\"\nlubridate::dweeks(100)\n#> [1] \"60480000s (~1.92 years)\"\nlubridate::dyears(100)\n#> [1] \"3155760000s (~100 years)\"\n## Somando durações a datas\nlubridate::today() + lubridate::ddays(1)\n#> [1] \"2022-06-09\"\n\n## Subtraindo durações de datas\nlubridate::today() - lubridate::dyears(1)\n#> [1] \"2021-06-07 18:00:00 UTC\"\n\n## Multiplicando durações\n2 * dyears(2)\n#> [1] \"126230400s (~4 years)\"\n## Criando períodos\nperiod(c(90, 5), c(\"second\", \"minute\"))\n#> [1] \"5M 90S\"\nperiod(c(3, 1, 2, 13, 1), c(\"second\", \"minute\", \"hour\", \"day\", \"week\"))\n#> [1] \"20d 2H 1M 3S\"\n\n## Transformação de períodos\nlubridate::seconds(100)\n#> [1] \"100S\"\nlubridate::minutes(100)\n#> [1] \"100M 0S\"\nlubridate::hours(100)\n#> [1] \"100H 0M 0S\"\nlubridate::days(100)\n#> [1] \"100d 0H 0M 0S\"\nlubridate::weeks(100)\n#> [1] \"700d 0H 0M 0S\"\nlubridate::years(100)\n#> [1] \"100y 0m 0d 0H 0M 0S\"\n## Somando datas\nlubridate::today() + lubridate::weeks(10)\n#> [1] \"2022-08-17\"\n\n## Subtraindo datas\nlubridate::today() - lubridate::weeks(10)\n#> [1] \"2022-03-30\"\n\n## Criando datas recorrentes\nlubridate::today() + lubridate::weeks(0:10)\n#>  [1] \"2022-06-08\" \"2022-06-15\" \"2022-06-22\" \"2022-06-29\" \"2022-07-06\" \"2022-07-13\" \"2022-07-20\" \"2022-07-27\" \"2022-08-03\" \"2022-08-10\"\n#> [11] \"2022-08-17\"\n## Criando duas datas - início de estudos do R e nascimento do meu filho\nr_inicio <- lubridate::dmy(\"30-11-2011\")\nfilho_nascimento <- lubridate::dmy(\"26-09-2013\")\nr_hoje <- lubridate::today()\n\n## Criando intervalos - interval\nr_intervalo <- lubridate::interval(r_inicio, r_hoje)\n\n## Criando intervalos - interval %--%\nfilho_intervalo <- filho_nascimento %--% lubridate::today()\n\n## Operações com intervalos\nlubridate::int_start(r_intervalo)\n#> [1] \"2011-11-30 UTC\"\nlubridate::int_end(r_intervalo)\n#> [1] \"2022-06-08 UTC\"\nlubridate::int_length(r_intervalo)\n#> [1] 332035200\nlubridate::int_flip(r_intervalo)\n#> [1] 2022-06-08 UTC--2011-11-30 UTC\nlubridate::int_shift(r_intervalo, duration(days = 30))\n#> [1] 2011-12-30 UTC--2022-07-08 UTC\n## Verificar sobreposição - int_overlaps\nlubridate::int_overlaps(r_intervalo, filho_intervalo)\n#> [1] TRUE\n\n## Verificar se intervalo está contido\nr_intervalo %within% filho_intervalo\n#> [1] FALSE\nfilho_intervalo %within% r_intervalo\n#> [1] TRUE\n## Períodos dentro de um intervalo - anos\nr_intervalo / lubridate::years()\n#> [1] 10.52055\nr_intervalo %/% lubridate::years()\n#> [1] 10\n\n## Períodos dentro de um intervalo - dias e semandas\nfilho_intervalo / lubridate::days()\n#> [1] 3177\nfilho_intervalo / lubridate::weeks()\n#> [1] 453.8571\n## Tempo total estudando R\nlubridate::as.period(r_intervalo)\n#> [1] \"10y 6m 9d 0H 0M 0S\"\n\n## Idade do meu filho\nlubridate::as.period(filho_intervalo)\n#> [1] \"8y 8m 13d 0H 0M 0S\"\n## Fuso horário no R\nSys.timezone()\n#> [1] \"America/Sao_Paulo\"\n## Verificar os fuso horários\nlength(OlsonNames())\n#> [1] 595\nhead(OlsonNames())\n#> [1] \"Africa/Abidjan\"     \"Africa/Accra\"       \"Africa/Addis_Ababa\" \"Africa/Algiers\"     \"Africa/Asmara\"      \"Africa/Asmera\"\n## Que horas são em...\nlubridate::with_tz(lubridate::now(), tzone = \"America/Sao_Paulo\")\n#> [1] \"2022-06-08 00:04:44 -03\"\nlubridate::with_tz(lubridate::now(), tzone = \"GMT\")\n#> [1] \"2022-06-08 03:04:44 GMT\"\nlubridate::with_tz(lubridate::now(), tzone = \"Europe/Berlin\")\n#> [1] \"2022-06-08 05:04:44 CEST\"\n\n## Altera o fuso sem mudar a hora\nlubridate::force_tz(lubridate::now(), tzone = \"GMT\")\n#> [1] \"2022-06-08 00:04:44 GMT\""},{"path":"cap5.html","id":"purrr","chapter":"Capítulo 5 Tidyverse","heading":"5.12 purrr","text":"O pacote purrr implementa Programação Funcional R, fornecendo um conjunto completo e consistente de ferramentas para trabalhar com funções e vetores. programação funcional é um assunto bastante extenso, sendo mais conhecido R pela família de funções purrr::map(), que permite substituir muitos loops por um código mais sucinto e fácil de ler. Não focaremos aqui nas outras funções, pois esse é um assunto extremamente extenso.Todas funções deste pacote são listadas na página de referência pacote.Um loop pode ser entendido como uma iteração: um bloco de códigos é repetido mudando um contador de uma lista de possibilidades. Vamos exemplificar com uma iteração bem simples, onde imprimiremos console os valores de 1 10, utilizando função (), um contador em um vetor de dez números 1:10 que será iterado, bloco de códigos definido entre {}, usando função print() para imprimir os valores.ideia é bastante simples: função () vai atribuir o primeiro valor da lista ao contador , esse contador será utilizado em todo o bloco de códigos. Quando o bloco terminar, o segundo valor é atribuído ao contador e entra bloco de códigos, repetindo esse processo até que todos os elementos da lista tenham sido atribuídos ao contador.Com essa ideia em mente, programação funcional faz mesma operação utilizando função purrr::map(). O mesmo loop ficaria dessa forma.Nessa estrutura, temos:map(.x, .f).x: um vetor, lista ou data frame.f: uma funçãoNum outro exemplo, aplicaremos função sum() para somar os valores de vários elementos de uma lista.Há diferente tipos de retornos da família purrr::map().map(): retorna uma listamap_chr(): retorna um vetor de stringsmap_dbl(): retorna um vetor numérico (double)map_int(): retorna um vetor numérico (integer)map_lgl(): retorna um vetor lógicomap_dfr(): retorna um data frame (por linhas)map_dfc(): retorna um data frame (por colunas)Essas funcionalidades já eram conhecidas R Base pelas funções da família apply(): apply(), lapply(), sapply(), vapply(), mapply(), rapply() e tapply(). Essas funções formam base de combinações mais complexas e ajudam realizar operações com poucas linhas de código, para diferentes retornos.Temos ainda duas variantes da função map(): purrr::map2() e purrr::pmap(), para duas ou mais listas, respectivamente. Como vimos para primeira função, existem várias variações sufixo para modificar o retorno da função.Essas funções podem ser usadas em conjunto para implementar rotinas de manipulação e análise de dados com poucas linhas de código, mas que não exploraremos em sua completude aqui. Listamos dois exemplos simples.Para se aprofundar tema, recomendamos leitura Capítulo 21 Iteration de Wickham & Grolemund (2017).","code":"\n## Loop for\nfor(i in 1:10){\n    print(i)\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5\n#> [1] 6\n#> [1] 7\n#> [1] 8\n#> [1] 9\n#> [1] 10\n## Loop for com map\npurrr::map(.x = 1:10, .f = print)\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5\n#> [1] 6\n#> [1] 7\n#> [1] 8\n#> [1] 9\n#> [1] 10\n#> [[1]]\n#> [1] 1\n#> \n#> [[2]]\n#> [1] 2\n#> \n#> [[3]]\n#> [1] 3\n#> \n#> [[4]]\n#> [1] 4\n#> \n#> [[5]]\n#> [1] 5\n#> \n#> [[6]]\n#> [1] 6\n#> \n#> [[7]]\n#> [1] 7\n#> \n#> [[8]]\n#> [1] 8\n#> \n#> [[9]]\n#> [1] 9\n#> \n#> [[10]]\n#> [1] 10\n## Função map\nx <- list(1:5, c(4, 5, 7), c(1, 1, 1), c(2, 2, 2, 2, 2))\npurrr::map(x, sum)\n#> [[1]]\n#> [1] 15\n#> \n#> [[2]]\n#> [1] 16\n#> \n#> [[3]]\n#> [1] 3\n#> \n#> [[4]]\n#> [1] 10\n## Variações da função map\npurrr::map_dbl(x, sum)\n#> [1] 15 16  3 10\npurrr::map_chr(x, paste, collapse = \" \")\n#> [1] \"1 2 3 4 5\" \"4 5 7\"     \"1 1 1\"     \"2 2 2 2 2\"\n## Listas\nx <- list(3, 5, 0, 1)\ny <- list(3, 5, 0, 1)\nz <- list(3, 5, 0, 1)\n\n## Função map2\npurrr::map2_dbl(x, y, prod)\n#> [1]  9 25  0  1\n\n## Função pmap\npurrr::pmap_dbl(list(x, y, z), prod)\n#> [1]  27 125   0   1\n## Resumo dos dados\npenguins %>% \n    dplyr::select(where(is.numeric)) %>% \n    tidyr::drop_na() %>% \n    purrr::map_dbl(mean)\n#>    bill_length_mm     bill_depth_mm flipper_length_mm       body_mass_g              year \n#>          43.92193          17.15117         200.91520        4201.75439        2008.02924\n## Análise dos dados\npenguins %>%\n    dplyr::group_split(island, species) %>% \n    purrr::map(~ lm(bill_depth_mm ~ bill_length_mm, data = .x)) %>% \n    purrr::map(summary) %>% \n    purrr::map(\"r.squared\")\n#> [[1]]\n#> [1] 0.2192052\n#> \n#> [[2]]\n#> [1] 0.4139429\n#> \n#> [[3]]\n#> [1] 0.2579242\n#> \n#> [[4]]\n#> [1] 0.4271096\n#> \n#> [[5]]\n#> [1] 0.06198376"},{"path":"cap5.html","id":"para-se-aprofundar-1","chapter":"Capítulo 5 Tidyverse","heading":"5.13 Para se aprofundar","text":"Listamos seguir livros que recomendamos para seguir com sua aprendizagem em R e tidyverse.","code":""},{"path":"cap5.html","id":"livros-1","chapter":"Capítulo 5 Tidyverse","heading":"5.13.1 Livros","text":"Recomendamos aos (às) interessados() os livros: ) Oliveira e colaboradores (2018) Ciência de dados com R, ii) Grolemund (2018) Essentials Data Science: Knowledge Discovery Using R,iii) Holmes e Huber (2019) Modern Statistics Modern Biology, iv) Ismay e Kim (2020) Statistical Inference via Data Science: ModernDive R Tidyverse, v) Wickham e Grolemund (2017) R Data Science: Import, Tidy, Transform, Visualize, Model Data, vi) Zumel e Mount (2014) Practical Data Science R Paperback, vii) Irizarry (2017) Introduction Data Science: Data Analysis Prediction Algorithms R, e viii) Irizarry (2019) Introduction Data Science.","code":""},{"path":"cap5.html","id":"links-1","chapter":"Capítulo 5 Tidyverse","heading":"5.13.2 Links","text":"Ciência de Dados em RtidyverseSTHDA - Statistical tools high-throughput data analysisEstatística é com R! - tidyverseTidyverse Skills Data Science RGetting Started TidyverseTidyverse BasicsManipulando Dados com dplyr e tidyr","code":""},{"path":"cap5.html","id":"exercícios-1","chapter":"Capítulo 5 Tidyverse","heading":"5.14 Exercícios","text":"5.1\nReescreva operações abaixo utilizando pipes %>%.log10(cumsum(1:100))sum(sqrt(abs(rnorm(100))))sum(sort(sample(1:10, 10000, rep = TRUE)))5.2\nUse função download.file() e unzip() para baixar e extrair o arquivo data paper de médios e grandes mamíferos: ATLANTIC MAMMALS. Em seguida, importe para o R, usando função readr::read_csv().5.3\nUse função tibble::glimpse() para ter uma noção geral dos dados importados item anterior.5.4\nCompare os dados de penguins (palmerpenguins::penguins_raw e palmerpenguins::penguins). Monte uma série de funções dos pacotes tidyr e dplyr para limpar os dados e fazer com que o primeiro dado seja igual ao segundo.5.5\nUsando os dados de penguins (palmerpenguins::penguins), calcule correlação de Pearson entre comprimento e profundidade bico para cada espécie e para todas espécies. Compare os índices de correlação para exemplificar o Paradoxo de Simpson.5.6\nOficialmente pandemia de COVID-19 começou Brasil com o primeiro caso dia 26 de fevereiro de 2020. Calcule quantos anos, meses e dias se passou desde então. Calcule também quanto tempo se passou até você ser vacinado.Soluções dos exercícios.","code":""},{"path":"cap6.html","id":"cap6","chapter":"Capítulo 6 Visualização de dados","heading":"Capítulo 6 Visualização de dados","text":"","code":""},{"path":"cap6.html","id":"pré-requisitos-do-capítulo-2","chapter":"Capítulo 6 Visualização de dados","heading":"Pré-requisitos do capítulo","text":"Pacotes que serão utilizados nesse capítulo.","code":"\n## Pacotes\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(datasauRus)\nlibrary(gridExtra)\n\n## Dados\npenguins <- palmerpenguins::penguins\n\n## Edição dos nomes das colunas para português \ncolnames(penguins) <- c(\"especies\", \"ilha\", \"comprimento_bico\", \n                        \"profundidade_bico\", \"comprimento_nadadeira\", \n                        \"massa_corporal\", \"sexo\", \"ano\")"},{"path":"cap6.html","id":"contextualização-2","chapter":"Capítulo 6 Visualização de dados","heading":"6.1 Contextualização","text":"visualização de dados através de gráficos frequentemente é melhor forma de apresentar e interpretar informações ecológicas pois sintetiza os dados e facilita o entendimento de padrões. Geralmente, os gráficos são necessários em quase todas análises estatísticas, além de enriquecer argumentação e discussão de hipóteses levantadas para publicações, trabalhos de consultoria, TCCs, dissertações, teses, entre outros.Existem vários tipos de gráficos para representar os padrões em seus dados para diferentes finalidades. Esses diferentes tipos de gráficos podem até mesmo ser usados para representar o mesmo tipo de dado. Nesta seção, focaremos nos gráficos mais simples para representar uma ou duas variáveis (.e., gráficos bidimensionais). Dependendo tipo de variável (categórica ou contínua – veja os tipos de variáveis na Figura 2.2 Capítulo 2), os gráficos mais indicados para representar os dados mudam. De forma simplificada, os gráficos são representações dos nossos dados tabulares, de modo que os eixos representam colunas e feições (pontos, linhas, barras, caixas, etc.) representam linhas.Comumente nos gráficos são representados uma ou duas colunas, quando muito três, em gráficos de três dimensões. Para mais colunas, partimos para dados agregados que podem ser vistos Capítulo 9 de análises multivariadas. Além disso, utilização de mais de duas colunas pode estar relacionado com outras partes estéticas (e.g. aes() pacote ggplot2) gráfico como cor, forma ou tamanho de pontos, linhas ou outras feições.Dessa forma, dedicamos esse capítulo inteiramente apresentar os principais conceitos, como gramática de gráficos, e uma apresentação geral que pode funcionar como “um guia de bolso” de gráficos, uma vez que apresentamos os principais tipos de gráficos para análises ecológicas e estatísticas. Além disso, último tópico focamos na finalização (ajustes finos) de gráficos para publicações. Este capítulo fornece base conceitual necessária para entender visualização gráfica de dados apresentada ao longo livro.Existe uma ampla gama de pacotes para fazer gráficos R, sendo esse um ponto muito forte dessa linguagem. Além disso, essa grande disponibilidade de pacotes e funções permitem visualização dos mais diferentes tipos de dados, o que torna linguagem R detentora de alta praticidade, uma vez que maior parte dos pacotes possui uma sintaxe relativamente simples para apresentação de gráficos excelentes e de ótima qualidade. Mais adiante Capítulo 15, ampliamos discussão da visualização gráfica com ferramentas para visualização de dados geoespaciais R.Este capítulo foi organizado em quatro partes: ) principais pacotes, ii) gramática dos gráficos, iii) tipos de gráficos (um guia de bolso para visualização de vários gráficos R), e iv) edição de gráficos com alta qualidade para publicação. Apesar de apresentarmos diferentes pacotes com grande potencial para visualização gráfica, focaremos neste capítulo pacote ggplot2, talvez o pacote mais utilizado e com maior gama de possibilidades de criação de excelentes gráficos. 📝 Importante \nNeste capítulo, vamos ensinar gradativamente os comandos para construir gráficos e como editar suas diferentes camadas. Por este motivo, o leitor não verá uma padronização nos temas dos gráficos (fonte, cor, fundo painel, etc.), uma vez que os gráficos sem edição (padrão ggplot2) possuem temas pré-definidos. Desse modo, para fazer um gráfico com maior qualidade (chamamos aqui de versão “com ajustes finos”) será necessário adicionar comandos extras que são exatamente o foco deste capítulo. Em alguns comandos básicos o leitor vai perceber que devido aos dados utilizados, os eixos vão possuir palavras em inglês misturadas com palavras em português. Optamos por não traduzir esses nomes para preservar o padrão que será visualizado nos primeiros comandos que cada leitor irá fazer em seu computador.Usaremos os dados disponíveis pacote palmerpenguins para exemplificar funções ggplot2, descrito Capítulo 5.","code":""},{"path":"cap6.html","id":"principais-pacotes","chapter":"Capítulo 6 Visualização de dados","heading":"6.2 Principais pacotes","text":"seguir, apresentamos uma listagem dos principais pacotes para visualização de dados R, além das principais funções desses pacotes.graphics: é o pacote padrão (default) R para produzir gráficos simples, porém útil para visualizações rápidas de quase todos classes de objetos. Possui funções como: plot(), hist(), barplot(), boxplot(), abline(), points(), lines() e polygon(). Destacamos que função plot() pode estar presente em diversos pacotesgraphics: é o pacote padrão (default) R para produzir gráficos simples, porém útil para visualizações rápidas de quase todos classes de objetos. Possui funções como: plot(), hist(), barplot(), boxplot(), abline(), points(), lines() e polygon(). Destacamos que função plot() pode estar presente em diversos pacotesggplot2: pacote integrado ao tidyverse (Capítulo 5), possui uma sintaxe própria baseada na gramática de gráficos por camadas (layers), necessitando de funções específicas para objetos de classes diferentes, demandando geralmente mais tempo para construção dos códigos. Possui funções como ggplot(), aes(), geom_*(), facet_*(), stats_*(), coord_*() e theme_*(), que são conectadas pelo operador +ggplot2: pacote integrado ao tidyverse (Capítulo 5), possui uma sintaxe própria baseada na gramática de gráficos por camadas (layers), necessitando de funções específicas para objetos de classes diferentes, demandando geralmente mais tempo para construção dos códigos. Possui funções como ggplot(), aes(), geom_*(), facet_*(), stats_*(), coord_*() e theme_*(), que são conectadas pelo operador +ggplot2 extentions: conjunto de pacotes que adicionam diversas expansões ao pacote ggplot2. Exemplos: gganimate, GGally, patchwork e esquisseggplot2 extentions: conjunto de pacotes que adicionam diversas expansões ao pacote ggplot2. Exemplos: gganimate, GGally, patchwork e esquissevisdat: cria visualizações preliminares de dados exploratórios de um conjunto de dados inteiro para identificar problemas ou recursos inesperados usando ‘ggplot2’. Possui diversas funções específicas: vis_dat() para visão geral dos dados, vis_miss() para visão de dados faltantes (NA) e vis_compare() para visualizar diferença entre dadosvisdat: cria visualizações preliminares de dados exploratórios de um conjunto de dados inteiro para identificar problemas ou recursos inesperados usando ‘ggplot2’. Possui diversas funções específicas: vis_dat() para visão geral dos dados, vis_miss() para visão de dados faltantes (NA) e vis_compare() para visualizar diferença entre dadosggpubr: pacote que fornece funções simplificadas para criar e personalizar gráficos para publicação, baseados ggplot2. Possui funções específicas: gghistogram(), ggdensity(), ggboxplot(), ggviolin(), ggbarplot() e ggscatter()ggpubr: pacote que fornece funções simplificadas para criar e personalizar gráficos para publicação, baseados ggplot2. Possui funções específicas: gghistogram(), ggdensity(), ggboxplot(), ggviolin(), ggbarplot() e ggscatter()lattice: pacote para visualização de dados inspirado nos gráficos treliça (inglês Trellis, geralmente para dados com muitas variáveis que geram uma matriz retangular de gráficos). Também possui funções específicas: xyplot(), histogram(), densityplot(), barchart(), bwplot() e dotplot(). O pacote latticeExtra disponibiliza algumas possibilidades mais para esse pacotelattice: pacote para visualização de dados inspirado nos gráficos treliça (inglês Trellis, geralmente para dados com muitas variáveis que geram uma matriz retangular de gráficos). Também possui funções específicas: xyplot(), histogram(), densityplot(), barchart(), bwplot() e dotplot(). O pacote latticeExtra disponibiliza algumas possibilidades mais para esse pacoteplotly: pacote para criar gráficos interativos da web por meio da biblioteca gráfica de JavaScript de código aberto plotly.js. Também possui funções específicas: plot_ly(), add_histogram(), add_bars(), add_boxplot(), add_markers(), add_paths(), add_lines() e add_polygons()plotly: pacote para criar gráficos interativos da web por meio da biblioteca gráfica de JavaScript de código aberto plotly.js. Também possui funções específicas: plot_ly(), add_histogram(), add_bars(), add_boxplot(), add_markers(), add_paths(), add_lines() e add_polygons()","code":""},{"path":"cap6.html","id":"grámatica-dos-gráficos","chapter":"Capítulo 6 Visualização de dados","heading":"6.3 Grámatica dos gráficos","text":"O livro Grammar Graphics (Wilkinson Wills 2005) utiliza uma analogia da linguística para criar uma “gramática” para visualização gráfica. Segundo ele, língua se torna expressiva pelo fato da gramática criar um sistema de regras que tornam declarações com significado conhecido. De maneira semelhante, ideia da gramática dos gráficos cria regras para representação gráfica dos dados partir de atributos estéticos (inglês aesthetic) como cor, forma e tamanho que definem geometria dos objetos, como pontos, linhas e barras (Wickham 2016). Além disso, esta gramática reconhece que tais elementos podem ser organizados em camadas, tal como construímos um mapa com diferentes camadas como elevação, hidrografia, rodovias, limites políticos, etc.Inspirado pela gramática dos gráficos proposta por Wilkinson & Wills (2005), Hadley Wickham criou o pacote ggplot2, onde “gg” representa contração de Grammar Graphics (Wickham 2016). camadas nesta gramática (layered-grammar) são organizadas da seguinte forma:Camada 1 - dados ggplot(): são informações formato data.frame que serão usadas nas diferentes camadas nas funções aes(), geom_*(), stat_*(), facet_*() e scale_*()Camada 2 - mapeamento aes(): atributos estéticos, determina que colunas data.frame serão usadas para representações geométricas, assim como tamanho, forma, cor, preenchimento e transparênciaCamada 3 - definição da geometria geom_*(): define o tipo de gráfico, como pontos, boxplots, violino, linhas, polígonos, entre outrosCamada 4 - transformações estatísticas stat_*(): modificam, quando necessário, os dados que serão incluídos gráfico, além de produzir estatísticas como regressõesCamada 5 - sistema de coordenadas coords_*(): descreve como coordenadas dos dados são mapeadas para o plano gráficoCamada 6 - facetas facets_*(): especifica como visualização dos elementos aes() são divididos em diferentes “janelas gráficas”Camada 7 - escala scale_*(): permite o controle das características visuais (cor, forma e tamanho) dos elementos declarados em aes()Camada 8 - temas theme*(): controla aparência visual dos elementos gráfico, como fontes, cores e legendaEstruturalmente podemos observar estrutura da Figura 6.1.\nFigura 6.1: Esquema gráfico ilustrando camadas que definem strutura de organização aditiva da gramática dos gráficos (ggplot2). exemplo, partir de um banco de dados, o mapeamento de quais colunas representam o eixo Y e X e de um atributo gráfico (pontos) é possível construir um gráfico de dispersão que ilustra relação quantitativa entre variável Y e X.\nEm resumo, o mapeamento gráfico ggplot2 segue seguinte estrutura (Wickham Grolemund 2017):","code":"ggplot(data = <DATA>) + \n    <GEOM_FUNCTION>(\n        mapping = aes(<MAPPINGS>),\n        stat = <STAT>, \n        position = <POSITION>\n        ) +\n    <COORDINATE_FUNCTION> +\n    <FACET_FUNCTION> +\n    <SCALE_FUNCTION> +\n    <THEME_FUNCTION>"},{"path":"cap6.html","id":"tipos-de-gráficos","chapter":"Capítulo 6 Visualização de dados","heading":"6.4 Tipos de gráficos","text":"Nesta seção, listamos os principais tipos de gráficos e fazemos uma descrição de quantas colunas e o tipo de variável que eles representam.Histograma (histogram): distribuição de frequência de uma coluna para dados contínuos (cores diferentes podem representar espécies, populações ou grupos distintos)Gráfico de densidade (density plot): distribuição da densidade de uma coluna para dados contínuos (assim como histograma, cores diferentes podem ser utilizadas para representar espécies, populações ou grupos distintos)Gráfico de dispersão (scatter plot) e gráfico de linha: relação entre valores de duas colunas para dados contínuos (X e Y)Diagrama de pontos (dot plot): distribuição da quantidade de valores agrupados de uma coluna para dados contínuosGráfico de setores (pie chart e donut chart): representação da quantidade de valores de uma coluna para dados categóricos, geralmente em proporção ou porcentagemGráfico de barras (bar plot): representação da quantidade de valores de uma ou mais colunas para dados categóricosGráfico de caixa (box plot e violin plot): distribuição de valores contínuos de uma coluna (Y) para dois ou mais fatores categóricos de outra coluna (X) formato de caixas e também formato de “violinos” (considerando variação e distribuição)Gráfico pareado (pairs plot): relação entre valores de duas colunas para dados contínuos (X e Y), para colunas par--parPara facilitar compreensão das regras da gramática dos gráficos, cada tipo de gráfico segue mesma estrutura de organização, que respeita camadas de informação descritas na seção anterior. Podemos perceber, portanto, que algumas camadas não são necessárias dependendo tipo de gráfico ou conjunto de dados que pretendemos analisar.Nos exemplos seguir, versão padrão se refere à representação determinada default das funções pacote ggplot2. Desse modo, somente informamos variáveis que serão utilizadas dentro de cada camada e forma geométrica (.e., tipo de gráfico) desejada. Porém, para cada tipo gráfico apresentamos funções e argumentos para ajustes finos e personalizados.","code":""},{"path":"cap6.html","id":"histograma-histogram","chapter":"Capítulo 6 Visualização de dados","heading":"6.4.1 Histograma (histogram)","text":"O histograma é um gráfico extremamente popular, sendo bastante útil para visualizar distribuição de variáveis contínuas. É bem provável que você já tenha visto um histograma quando aprendeu pela primeira vez famosa distribuição normal (Figura 6.2).\nFigura 6.2: Histograma de uma distribuição normal com 10000 observações.\nNeste histograma é possível entender que maioria dos valores da variável x objeto dist_normal estão próximos ao valor da média, .e., zero. Em Ecologia, os histogramas são utilizados para visualizar, por exemplo, variação morfológica entre espécies (subespécies, gênero, famílias, etc.), variação de parâmetros populacionais entre diferentes espécies ou dentro da mesma espécie em diferentes localidades.Versão padrãoVamos utilizar o conjunto de dados palmerpenguins para construir um histograma da distribuição da variável comprimento_nadadeira com função geom_hitogram(). Esta função utiliza uma variável contínua eixo X e frequência de cada categoria de intervalo de valores eixo Y. O gráfico seguir representa frequência de uma variável (neste caso, medida de todos os pinguins, independente da espécie) (Figura 6.3).\nFigura 6.3: Histograma da variável comprimento_nadadeira.\nDefinindo o número de classesVamos utilizar o argumento bins para definir em quantas classes variável x deve ser dividida (Figura 6.4).\nFigura 6.4: Histograma da variável comprimento_nadadeira para diferentes classes de divisão.\nComparando múltiplas categoriasSe quisermos comparar distribuição de uma variável contínua entre diferentes categorias, podemos utilizar o argumento fill para colorir o gráfico. exemplo abaixo, utilizamos cores diferentes para ilustrar distribuição da variável X (x = comprimento_nadadeira) entre espécies diferentes (fill = especies) (Figura 6.5).\nFigura 6.5: Histograma da variável comprimento_nadadeira para diferentes espécies com e sem sobreposição.\nAjustes finos (versão personalizada)\nFigura 6.6: Histograma da variável comprimento_nadadeira com ajustes finos.\nPrincipais camadas utilizadas na função geom_histogram()aes()Eixo X (x): variável contínua (comprimento_nadadeira)Preenchimento (fill): variável categórica (especies) que define cores tendo como base o número de níveis dentro desta categoriageom()geom_histogram(): para que variável contínua seja plotada como histogramaTransparência das linhas e preenchimentos (alpha): 0.5 (varia de 0, transparência máxima, 1, sem transparência)Posição das barras: o argumento position define se barras devem ser inseridas de maneira sobreposta (position = \"identity\") ou não (position = \"dodge\")scale()scale_fill_manual(): para definir manualmente corestheme()theme_bw(): para selecionar o tema com fundo brancolabs(): para personalizar os títulos dos eixos X e Y, e da legenda","code":"\n## Dados\ndist_normal <- data.frame(x = rnorm(10000, mean = 0, sd = 1))\n\n## Histograma de uma variável contínua\nggplot(data = dist_normal, aes(x = x)) +\n    geom_histogram()\n## Histograma da coluna flipper_length_mm\nggplot(data = penguins, aes(x = comprimento_nadadeira)) +\n    geom_histogram()\n## Histograma com 10 classes\nggplot(data = penguins, aes(x = comprimento_nadadeira)) +\n    geom_histogram(bins = 10) +\n    labs(title = \"10 classes\")\n\n## Histograma com 30 classes\nggplot(data = penguins, aes(x = comprimento_nadadeira)) +\n    geom_histogram(bins = 30) +\n    labs(title = \"30 classes\")\n## Histograma com cores para diferentes categorias com sobreposição\nggplot(data = penguins, aes(x = comprimento_nadadeira, fill = especies)) +\n    geom_histogram(alpha = .4) +\n    labs(title = \"Com sobreposiçao\")\n\n## Histograma com cores para diferentes categorias sem sobreposição\nggplot(data = penguins, aes(x = comprimento_nadadeira, fill = especies)) +\n    geom_histogram(position = \"dodge\") +\n    labs(title = \"Sem sobreposiçao\")\n## Histograma exemplo\nggplot(data = penguins, aes(x = comprimento_nadadeira, fill = especies)) +\n    geom_histogram(alpha = .4, position = \"identity\") +\n    scale_fill_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    theme_bw(base_size = 14) +\n    labs(x = \"Comprimento da nadadeira (mm)\", \n         y = \"Frequência absoluta\", fill = \"Espécies\")"},{"path":"cap6.html","id":"gráfico-de-densidade-density-plot","chapter":"Capítulo 6 Visualização de dados","heading":"6.4.2 Gráfico de densidade (density plot)","text":"Nesta seção aprenderemos criar um gráfico de densidade R utilizando o ggplot2. Assim como o histograma, o gráfico de densidade é utilizado para visualizar distribuição de uma variável contínua em intervalos. Esse gráfico é uma variação histograma que utiliza Kernel Smoother e, além de ser muito útil para visualizar distribuições, pode ser usado para testar várias hipóteses ecológicas, como descrito Capítulo 14.Versão padrãoVamos utilizar o conjunto de dados palmerpenguins para plotar distribuição da variável comprimento_nadadeira em um gráfico de densidade. Utilizaremos função geom_density() para plotar uma variável eixo X (Figura 6.7).\nFigura 6.7: Gráfico de densidade da variável comprimento_nadadeira.\nAlém da versão de densidade em linha, é possível utilizar o argumento fill para definir cor de preenchimento gráfico e o argumento alpha para definir transparência preenchimento. Utilizamos ainda o argumento color para definir cor da linha (Figura 6.8).\nFigura 6.8: Gráfico de densidade da variável comprimento_nadadeira com cor e preenchimento.\nComparando múltiplas categoriasEm algumas situações queremos comparar distribuição de uma variável contínua entre diferentes categorias. Dessa forma, podemos utilizar o argumento fill para colorir o gráfico. exemplo abaixo, utilizamos cores diferentes para ilustrar distribuição da variável x entre espécies diferentes (fill = especies) (Figura 6.9).\nFigura 6.9: Gráfico de densidade da variável comprimento_nadadeira para diferentes espécies com e sem transparência.\nAjustes finos (versão personalizada)\nFigura 6.10: Gráfico de densidade da variável comprimento_nadadeira com ajustes finos.\nPrincipais camadas utilizadas na função geom_density()aes()Eixo X (x): variável contínua (comprimento_nadadeira)Preenchimento (fill): variável categórica (especies) que define cores tendo como base o número de níveis dentro desta categoriageom()geom_density(): para que variável contínua seja plotada como densidadeTransparência das linhas e preenchimentos (alpha): 0.5 (varia de 0, transparência máxima, 1, sem transparência)scale()scale_fill_manual(): para definir manualmente cores de preferência usuárioscale_x_continuous() e scale_y_continuous(): determinam os limites (valores mínimos e máximos) para os dois eixos e, além disso, os intervalos entre os valores (breaks)theme()theme_bw(): para selecionar o tema com fundo brancolabs(): para personalizar os títulos dos eixos X e Y, e da legenda","code":"\n## Gráfico de densidade\nggplot(data = penguins, aes(x = comprimento_nadadeira)) +\n    geom_density()\n## Argumento fill\nggplot(data = penguins, aes(x = comprimento_nadadeira)) +\n    geom_density(fill = \"cyan4\")\n\n## Argumento fill, color e alpha\nggplot(data = penguins, aes(x = comprimento_nadadeira)) +\n    geom_density(fill = \"cyan4\", color = \"black\", alpha = .4)\n## O argumento fill preenche cada nível da coluna \"especies\" (sem transparência: alpha = 1)\nggplot(data = penguins, aes(x = comprimento_nadadeira, fill = especies)) +\n    geom_density() +\n    scale_fill_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    labs(title = \"Sem transparência\")\n\n## Gráfico de densidade com cores para diferentes categorias com sobreposição\nggplot(data = penguins, aes(x = comprimento_nadadeira, fill = especies)) +\n    geom_density(alpha = .4) +\n    scale_fill_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    labs(title = \"Com transparência\")\n## Gráfico de densidade exemplo\nggplot(data = penguins, aes(x = comprimento_nadadeira, fill = especies)) +\n    geom_density(alpha = .4) +\n    scale_fill_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    scale_x_continuous(breaks = seq(from = 160, to = 240, by = 10), limits = c(160, 240)) +\n    scale_y_continuous(breaks = seq(from = 0, to = .07, by = .01)) +\n    theme_bw(base_size = 14) +\n    labs(x = \"Comprimento da nadadeira (mm)\", y = \"Densidade\", fill = \"Espécies\")"},{"path":"cap6.html","id":"diagrama-de-pontos-dot-plot","chapter":"Capítulo 6 Visualização de dados","heading":"6.4.3 Diagrama de pontos (dot plot)","text":"Uma alternativa ao histograma e ao gráfico de densidade é o diagrama de pontos (Dot plot, apesar de ser relativamente menos usado em Ecologia.Versão padrãoVamos utilizar o conjunto de dados palmerpenguins para visualizar distribuição da variável comprimento_nadadeira com o diagrama de pontos com função geom_dotplot() (Figura 6.11).\nFigura 6.11: Diagrama de pontos da variável comprimento_nadadeira.\nComparando múltiplas categoriasAssim como nas funções geom_histogram() e geom_density(), é possível comparar categorias na função geom_dotplot() utilizando o argumento fill, bem como os argumentos color, alpha e dotsize (Figura 6.12).\nFigura 6.12: Diagrama de pontos da variável comprimento_nadadeira para diferentes espécies com e sem transparência.\nAjustes finos (versão personalizada)\nFigura 6.13: Diagrama de pontos da variável comprimento_nadadeira com ajustes finos.\n 📝 Importante \nUma das limitações diagrama de pontos é que sobreposição dos pontos pode não permitir visualização apropriada desses valores sobrepostos entre diferentes grupos quando comparados.Principais camadas utilizadas na função geom_dotplot()aes()Eixo X (x): variável contínua (comprimento_nadadeira)Preenchimento (fill): variável categórica (especies) que define cores tendo como base o número de níveis dentro desta categoriageom()geom_dotplot(): para que variável contínua seja plotada como diagrama de pontosTransparência dos pontos (alpha): 0.5 (varia de 0, transparência máxima, 1, sem transparência)Cor da borda ponto (color): valor padrão (se não especificado) é blackTamanho dos pontos (dotsize): valor padrão (se não especificado) é 1Posição dos pontos: o argumento position define se os pontos devem ser inseridos de maneira sobreposta (position = \"identity\") ou não (position = \"dodge\")scale()scale_fill_manual(): para definir manualmente cores de preferência usuárioscale_x_continuous(): e scale_y_continuous() determinam os limites (valor mínimo e máximo) para os dois eixos e, além disso, os intervalos entre os valores (breaks)theme()theme_bw(): para selecionar o tema com fundo brancolabs(): para personalizar os títulos dos eixos X e Y, e da legenda","code":"\n## Gráfico de pontos\nggplot(data = penguins, aes(x = comprimento_nadadeira)) +\n    geom_dotplot(dotsize = .6)\n## O argumento fill preenche cada nível da coluna \"especies\" (sem transparência: alpha = 1)\nggplot(data = penguins, aes(x = comprimento_nadadeira, fill = especies)) +\n    geom_dotplot(dotsize = .9)\n\n## Diagrama de pontos com cores para diferentes categorias com sobreposição\nggplot(data = penguins, aes(x = comprimento_nadadeira, fill = especies)) +\n    geom_dotplot(dotsize = .7, color = \"black\", alpha = .4)\n## Diagrama de pontos exemplo\nggplot(data = penguins, aes(x = comprimento_nadadeira, fill = especies)) +\n    geom_dotplot(color = \"black\", alpha = .7, position = \"dodge\") +\n    scale_fill_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    scale_x_continuous(breaks = seq(from = 170, to = 240, by = 10), limits = c(170, 240)) +\n    scale_y_continuous(breaks = seq(from = 0, to = 1.4, by = .2), limits = c(0, 1.4)) +\n    theme_bw(base_size = 14) +\n    labs(x = \"Comprimento da nadadeira (mm)\", y = \"Frequência\", fill = \"Espécies\")"},{"path":"cap6.html","id":"gráfico-de-barras-bar-plot","chapter":"Capítulo 6 Visualização de dados","heading":"6.4.4 Gráfico de barras (bar plot)","text":"O gráfico de barras é um dos gráficos mais usados em artigos e livros de Ecologia, uma vez que permite comparar valores absolutos ou médios (combinados com alguma medida de variação, como desvio padrão) de uma variável contínua entre diferentes níveis de uma variável categórica.Versão padrãoO gráfico de barras utiliza retângulos para representar uma variável contínua ou contagem de uma variável categórica, sendo que o comprimento dos retângulos é proporcional ao valor que ele representa. Por exemplo, é possível comparar qual quantidade de indivíduos medidos para cada espécie de pinguim (Figura 6.14).\nFigura 6.14: Gráfico de barras indicando quantidade de indivíduos medidos de cada espécie de pinguim.\nAlém disso, é possível alterar cores (color) e o preenchimento (fill) das barras, bem como sua transparência (alpha) e largura (width), como demonstrado nos próximos quatro gráficos (Figura 6.15).\nFigura 6.15: Gráficos de barras indicando quantidade de indivíduos medidos de cada espécie de pinguim, modificando cor, preenchimento, transparência e largura das barras.\nOutra possibilidade para representação gráfico de barras é inverter direção das barras com função coord_flip() (Figura 6.16).\nFigura 6.16: Gráficos de barras indicando quantidade de indivíduos medidos de cada espécie de pinguim, invertendo direção das barras.\nÉ possível utilizar variáveis categóricas para definir cores e preenchimento e ilustrar, por exemplo, tratamentos ou espécies diferentes com os argumentos fill e color (Figura 6.17).\nFigura 6.17: Gráfico de barras indicando quantidade de indivíduos medidos de cada espécie de pinguim, para diferentes espécies.\nAdicionando medidas de variaçãoEm algumas comparações, utilizar somente os valores absolutos pode não ser visualização mais apropriada, como, por exemplo, em desenhos de ANOVA (Capítulo 7). Desse modo, ao invés valor máximo da barra representar o valor absoluto (e.g., número de indivíduos de uma espécie), ele vai representar o valor médio. Além disso, linhas adicionais (chamadas barras de erro) vão representar alguma medida de variação como desvio padrão, erro padrão, intervalo de confiança, entre outros (Figura 6.18).\nFigura 6.18: Gráfico de barras indicando os valores médios e o desvio padrão da variável comprimento_nadadeira para cada espécie de pinguim.\nAjustes finos (versão personalizada)\nFigura 6.19: Gráfico de barras indicando quantidade de indivíduos medidos de cada espécie de pinguim, com ajustes finos.\n","code":"\n## Número de indivíduos coletados\npenguins_count <- penguins %>%\n    dplyr::count(especies)\npenguins_count\n#> # A tibble: 3 × 2\n#>   especies      n\n#>   <fct>     <int>\n#> 1 Adelie      152\n#> 2 Chinstrap    68\n#> 3 Gentoo      124\n\n## Gráfico de barras\nggplot(data = penguins_count, aes(x = especies, y = n)) + \n    geom_bar(stat = \"identity\")\n## Modificando o preenchimento\nggplot(data = penguins_count, aes(x = especies, y = n)) + \n    geom_bar(stat = \"identity\", fill = \"cyan4\")\n\n## Modificando a cor e o preenchimento\nggplot(data = penguins_count, aes(x = especies, y = n)) + \n    geom_bar(stat = \"identity\", color = \"cyan4\", fill = \"white\")\n\n## Modificando a largura da barra = .75\nggplot(data = penguins_count, aes(x = especies, y = n)) +\n    geom_bar(stat = \"identity\", width = .75) +\n    labs(title = \"Largura = .75\")\n\n## Modificando a largura da barra = .25\nggplot(data = penguins_count, aes(x = especies, y = n)) +\n    geom_bar(stat = \"identity\", width = .25) +\n    labs(title = \"Largura = .25\") \n## Barras vertical\nggplot(data = penguins_count, aes(x = especies, y = n)) +\n    geom_bar(stat = \"identity\", width = .6)\n\n## Barras horizontal\nggplot(data = penguins_count, aes(x = especies, y = n)) +\n    geom_bar(stat = \"identity\", width = .6) + \n    coord_flip()\n## Gráfico de barras com preenchimento colorido\nggplot(data = penguins_count, aes(x = especies, y = n, fill = especies)) +\n    geom_bar(stat = \"identity\")\n## Calcular o desvio padrão por espécie\npenguins_media <- penguins %>% \n    dplyr::group_by(especies) %>% \n    dplyr::summarise(media = mean(comprimento_nadadeira, na.rm = TRUE),\n                     desvio = sd(comprimento_nadadeira, na.rm = TRUE))\n\n## Gráfico de barras com desvio padrão\nggplot(data = penguins_media, aes(x = especies, y = media, fill = especies)) +\n    geom_bar(stat = \"identity\", alpha = .4) +\n    geom_errorbar(aes(ymin = media-desvio, ymax = media+desvio), width = .1) + \n    geom_point()\n## Gráfico de barra exemplo\nggplot(data = penguins_count, aes(x = especies, y = n, fill = especies)) +\n    geom_bar(stat = \"identity\") +\n    geom_label(aes(label = n), fill = \"white\") +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw(base_size = 14) +\n    labs(x = \"Espécies\", y = \"Número de indivíduos\", fill = \"Espécies\")"},{"path":"cap6.html","id":"gráfico-de-setores-pie-chart-e-donut-chart","chapter":"Capítulo 6 Visualização de dados","heading":"6.4.5 Gráfico de setores (pie chart e donut chart)","text":"Além gráfico de barras, o gráfico de setores representa uma alternativa para comparar proporção entre categorias. Tais gráficos podem ser representados como pie charts ou donut charts, como demonstrado abaixo. exemplo abaixo, utilizamos mesma comparação realizada item acima. Porém, os valores de contagem (número de indivíduos por espécie) devem ser transformados previamente em proporção.Gráfico de setores (pie chart)Gráfico de setores tipo pie (Figura 6.20).\nFigura 6.20: Gráfico de setores indicando proporção de indivíduos medidos de cada espécie de pinguim.\nGráfico de setores (donut chart)Gráfico de setores tipo donuts (Figura 6.21).\nFigura 6.21: Gráfico de setores tipo donuts indicando proporção de indivíduos medidos de cada espécie de pinguim.\nComparando gráficos de setores com gráfico de barrasO mesmo conjunto de dados pode ser visualizado de diferentes formas. Não diferente, comparação da proporção de ocorrências de diferentes categorias pode ser feita de várias maneiras. Abaixo, apresentamos comparação da proporção de indivíduos de cada uma das três espécies banco de dados penguins (Figura 6.22).\nFigura 6.22: Comparação da proporção de indivíduos por cada uma das três espécies dos dados feita através de gráfico de barras, setores tipo pie e setores tipo donuts.\nPrincipais camadas utilizadas gráfico de barras e de setores: geom_bar()aes()Eixo X (x): variável categórica (especies)Eixo Y (y): variável contínua (comprimento_nadadeira)Preenchimento (fill): variável categórica (especies) define cor preenchimento e os níveis dentro desta categoria determinam o número de cores que devem ser indicadas scale_fill_manual()geom()geom_bar(): para que variáveis categóricas sejam plotadas como gráficos de barra ou setoresTransparência das barras (alpha): 0.5 (varia de 0, transparência máxima, 1, sem transparência)stat: é necessário usar o argumento identity quando os valores eixo Y são adicionados pelo usuáriogeom_label(): forma geométrica que adiciona rótulo dos valores absolutos das barras por categoria (especies)geom_errorbar(): ymine ymax delimitam os valores mínimos e máximos, respectivamente, das barras de erro. Tais valores são representados pelo valor da média menos (caso ymin) ou mais (caso ymax) o valor intervalo de confiança, desvio ou erro padrãocoordcoord_polar(): sistema de coordenadas para gerar barras circulares sobrepostas (stacked) que são usadas nos gráficos de setores (pie chart e donut chart)o argumento start = 0 indica o local de início gráfico que, neste caso, começa na “hora” 0 em um “relógio” de 12 horasscale()scale_fill_manual(): para definir manualmente cores de preferência usuáriotheme()theme_bw(): para selecionar o tema com fundo brancolabs(): para personalizar os títulos dos eixos X e Y, e da legenda.","code":"\n## Cálculo da proporção - pie\npenguins_prop <- penguins %>%\n    dplyr::count(especies) %>% \n    dplyr::mutate(prop = round(n/sum(n), 4)*100)\n\n## Gráfico de setores\nggplot(data = penguins_prop, aes(x = \"\", y = prop, fill = especies)) + \n    geom_bar(stat = \"identity\", color = \"white\") +\n    geom_text(aes(label = paste0(prop, \"%\")), color = \"white\", \n              position = position_stack(vjust = .5), size = 6) +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    coord_polar(\"y\", start = 0) +\n    theme_void() +\n    labs(fill = \"Espécies\")\n## Gráfico de setores - donut\nggplot(data = penguins_prop, aes(x = 2, y = prop, fill = especies)) +\n    geom_bar(stat = \"identity\") +\n    geom_text(aes(label = paste0(prop, \"%\")), color = \"white\",\n              position = position_stack(vjust = .5), size = 4) +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    xlim(0, 2.5) +\n    coord_polar(theta = \"y\", start = 0) +\n    theme_void() +\n    theme(legend.position = c(.5, .5),\n          legend.title = element_text(size = 20),\n          legend.text = element_text(size = 15)) +\n    labs(fill = \"Espécies\")\n## Gráfico de barras - vertical  \ng_bar_v <- ggplot(data = penguins_prop, aes(x = especies, y = prop, fill = especies)) +\n    geom_bar(stat = \"identity\") +\n    geom_label(aes(label = prop), fill = \"white\") +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    labs(title = \"Gráfico de Barras (Vertical)\", x = \"Espécies\", \n         y = \"Número de indivíduos\", fill = \"Espécies\")\n\n## Gráfico de barras - horizontal  \ng_bar_h <- ggplot(data = penguins_prop, aes(x = especies, y = prop, fill = especies)) +\n    geom_bar(stat = \"identity\") +\n    geom_label(aes(label = prop), fill = \"white\") +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    coord_flip() +\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    labs(title = \"Gráfico de Barras (Horizonal)\", x = \"Espécies\", \n         y = \"Número de indivíduos\", fill = \"Espécies\")\n\n## Gráfico de setores - pie\ng_pie <- ggplot(data = penguins_prop, aes(x = \"\", y = prop, fill = especies)) + \n    geom_bar(stat = \"identity\", color = \"white\") +\n    geom_text(aes(label = paste0(prop, \"%\")), color = \"white\", \n              position = position_stack(vjust = .5), size = 3) +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    coord_polar(\"y\", start = 0) +\n    theme_void() +\n    labs(title = \"Pie chart\", fill = \"Espécies\")\n\n## Gráfico de setores - donut\ng_donut <- ggplot(data = penguins_prop, aes(x = 2, y = prop, fill = especies)) +\n    geom_bar(stat = \"identity\") +\n    geom_text(aes(label = paste0(prop, \"%\")), color = \"white\",\n              position = position_stack(vjust = .5), size = 2) +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    coord_polar(theta = \"y\", start = 0) +\n    xlim(0, 2.5) +\n    theme_void() +\n    theme(legend.position = \"none\") +\n    labs(title = \"Donut chart\", fill = \"Espécies\")\n\n## Combinação dos gráfigos\ngrid.arrange(g_bar_v, g_bar_h, g_pie, g_donut, nrow = 2)"},{"path":"cap6.html","id":"gráfico-de-caixa-boxplot","chapter":"Capítulo 6 Visualização de dados","heading":"6.4.6 Gráfico de caixa (boxplot)","text":"O gráfico de caixa, também conhecido como boxplot, e amplamente utilizado nos artigos e livros de Ecologia, é uma visualização gráfica que sintetiza informações importantes de dados contínuos como mediana e variação (Figura 6.23) para diferentes níveis de uma variável categórica.\nFigura 6.23: Estrutura e elementos boxplot.\nVersão padrãoVamos plotar uma variável contínua (comprimento_nadadeira) eixo y em função de uma variável categórica eixo x (especies). definição de qual coluna bando de dados é x e qual é y é feita dentro na função aes() (Figura 6.24).\nFigura 6.24: Gráfico de caixa para variável comprimento_nadadeira para cada espécie de pinguim.\nÉ possível destacar, se houver, os pontos referentes aos outliers (valores discrepantes) com o argumento outlier.color. Caso tenha interesse, é possível também remover os outliers gráfico (Figura 6.25).\nFigura 6.25: Gráfico de caixa para variável comprimento_nadadeira para cada espécie de pinguim, destacando e removendo os outliers.\nOutra opção para os gráficos tipo boxplot é utilizar o argumento notch = TRUE para produzir diagramas de caixa entalhados (notched). Estes diagramas são úteis para inferir de forma aproximada se existe diferença significativa entre médias dos grupos (Figura 6.26).\nFigura 6.26: Gráfico de caixa para variável comprimento_nadadeira para cada espécie de penguim, com entalhamentos.\nComparando múltiplas categoriasNo exemplo abaixo, utilizamos cores diferentes para ilustrar espécies diferentes através argumento fill = especies (Figura 6.27).\nFigura 6.27: Gráfico de caixa para variável comprimento_nadadeira para cada espécie de pinguim, com preenchimentos diferentes.\nCombinando boxplot com pontos (jitter)Podemos ainda acrescentar pontos para mostrar distribuição dos dados, fazendo uma “agitação” (jitter) dos pontos sobre caixas (Figura 6.28).\nFigura 6.28: Gráfico de caixa para variável comprimento_nadadeira para cada espécie de penguim, com distribuição dos dados.\nGráfico de violino (violin plot) como alternativa ao boxplotAlém das caixas boxplot, podemos utilizar o formato de “violino” (gráfico de violino) para representar variação dos dados contínuos para categorias. informação adicional ao boxplot que o gráfico de violino permite visualizar é densidade e distribuição dos pontos, assim como apresentamos acima gráfico de densidades geom_density(). diferença é que densidade é espelhada e, desse modo, podemos visualizar os intervalores dos dados com maior ou menor concentração de valores (Figura 6.29).\nFigura 6.29: Gráfico de violino para variável comprimento_nadadeira para cada espécie de penguim, com distribuição dos dados.\nÉ possível também combinar boxplot e gráfico de violino em um único gráfico (Figura 6.30).\nFigura 6.30: Gráfico de caixa e de violino para variável comprimento_nadadeira para cada espécie de penguim.\nAjustes finos (versão personalizada)\nFigura 6.31: Gráfico de caixa combinado ou não com Gráfico de violino e jitter para variável comprimento_nadadeira para cada espécie de penguim, com ajustes finos.\nPrincipais camadas utilizadas geom_boxplot() e geom_violin()aes()Eixo X (x): variável categórica (especies)Eixo Y (y): variável contínua (comprimento_nadadeira)Preenchimento (fill): variável categórica (especies) define cor preenchimento e os níveis dentro desta categoria determinam o número de cores que devem ser indicadas scale_fill_manual().geom()geom_boxplot(): para que variáveis contínuas e categóricas sejam plotadas como gráficos de caixas ou violinoswidth: largura das barras ou dos pontos(valor padrão: width = 1)fill: pode definir uma cor padrão (caso não tenha utilizado o fill dentro argumento aes()) como fill = \"gray\"notch: para utilizar caixa entalhada o argumento deve ser notch = TRUE, escolha padrão da função geom_boxplot() é notch = FALSEgeom_violin(): assim como nas outras formas geométricas, é possível controlar largura, cor, preenchimento e transparências dos violinosgeom_jitter(): esta função basicamente “agita” aleatoriamente os pontos para evitar sobreposição de valores idênticos. Esta função produz mesma representação se usar função geom_point(position = \"jitter\")scale()scale_fill_manual(): para definir manualmente cores de preferência usuáriotheme()theme_bw(): para selecionar o tema com fundo brancolabs(): para personalizar os títulos dos eixos X e Y, e da legenda","code":"\n## Gráfico de caixas das coluna comprimento_nadadeira e espécies\nggplot(data = penguins, aes(y = comprimento_nadadeira, x = especies)) +\n    geom_boxplot()\n## Destaque dos outliers\nggplot(data = penguins, aes(y = comprimento_nadadeira, x = especies)) +\n    geom_boxplot(outlier.color = \"red\") +\n    labs(title = \"Outliers vermelhos\")\n\n## Remoção dos outliers\nggplot(data = penguins, aes(y = comprimento_nadadeira, x = especies)) +\n    geom_boxplot(outlier.shape = NA) +\n    labs(title = \"Outliers removidos\")\n## Gráfico com caixa entalhadas\nggplot(data = penguins, aes(y = comprimento_nadadeira, x = especies)) +\n    geom_boxplot(notch = TRUE) +\n    labs(title = \"Caixas entalhadas\")\n## Modificando o preenchimento\nggplot(data = penguins, \n       aes(y = comprimento_nadadeira, x = especies, fill = especies)) +\n    geom_boxplot()\n## Boxplot com jitters\nggplot(data = penguins, aes(y = comprimento_nadadeira, \n                            x = especies, \n                            fill = especies)) +\n    geom_boxplot() +\n    geom_jitter(size = .5)\n## Gráfico de violino\nggplot(data = penguins, aes(y = comprimento_nadadeira, x = especies, fill = especies)) +\n    geom_violin() +\n    geom_jitter(size = .5)\n## Combinando o gráfico de violino com o de caixas\nggplot(data = penguins, aes(y = comprimento_nadadeira, x = especies, fill = especies)) +\n    geom_violin() +\n    geom_boxplot(width = .1, fill = \"gray\")\n## Gráfico de caixas exemplo\nggplot(data = penguins, aes(x = especies, y = comprimento_nadadeira, fill = especies)) +\n    geom_boxplot(width = .5, show.legend = FALSE) +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw(base_size = 14) +\n    labs(title = \"Boxplot sem pontos\", x = \"Espécies\", y = \"Comprimento da nadadeira (mm)\")\n\n## Gráfico de violino exemplo\nggplot(data = penguins, aes(x = especies, y = comprimento_nadadeira, fill = especies)) +\n    geom_violin(width = .5, show.legend = FALSE) +\n    geom_jitter(alpha = .4, show.legend = FALSE, \n                position = position_jitter(width = .15, seed = 0)) +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw(base_size = 14) +\n    labs(title = \"Gráfico de violino com jitter\", x = \"Espécies\", y = \"Comprimento da nadadeira (mm)\")\n\n## Gráfico de caixas e violino exemplo\nggplot(data = penguins, aes(x = especies, y = comprimento_nadadeira, fill = especies)) +\n    geom_violin(width = .5, show.legend = FALSE) +\n    geom_boxplot(width = .3, fill = \"gray\", show.legend = FALSE) +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw(base_size = 14) +\n    labs(title = \"Gráfico de violino com boxplot\", x = \"Espécies\", y = \"Comprimento da nadadeira (mm)\")"},{"path":"cap6.html","id":"gráfico-de-dispersão-scatter-plot","chapter":"Capítulo 6 Visualização de dados","heading":"6.4.7 Gráfico de dispersão (scatter plot)","text":"O gráfico de dispersão (em inglês, scatter plot) é famoso na Ecologia por ser visualização preferida para representar relação entre área e riqueza de espécies. Neste gráfico, os eixos X e Y são representados por variáveis contínuas. Em especial, os gráficos de dispersão são usados para representar os resultados testados por análises estatísticas como regressão linear, ANCOVA, mantel, PCA, PCoA, nMDS entre outros (para mais detalhes, consultar os Capítulos 7, 8 e 9).Versão padrãoVamos utilizar o conjunto de dados palmerpenguins para construir um gráfico de dispersão relacionando variáveis comprimento_bico e comprimento_nadadeira com função geom_point(). Esta função utiliza variável contínuas eixo X e Y. O gráfico mostra os pares ordenados das medidas das duas variáveis (Figura 6.32).\nFigura 6.32: Gráfico de dispersão relacionando variáveis comprimento_bico e comprimento_nadadeira.\nDefinindo cor, tamanho, forma e preenchimento dos pontosDa mesma forma que para os outros gráficos, podemos alterar cor, tamanho, forma e preenchimento dos pontos (Figura 6.33).\nFigura 6.33: Gráfico de dispersão relacionando variáveis comprimento_bico e comprimento_nadadeira para cada espécie de penguim, com alterações na cor, tamanho e preenchimento dos pontos.\nforma dos pontos permite dois controles importantes: forma em si (símbolos como círculo, quadrado, etc.) e possibilidade de preenchimento da forma. figura seguir discrimina esses símbolos e o valor que deve ser utilizado para desenhar forma preferida. É importante notar que os símbolos 21 25 possuem dois argumentos: () color, que, na verdade é cor da borda símbolo e (ii) fill, cor de preenchimento símbolo. O tipo de símbolo é definido pelo argumento shape (Figura 6.34).\nFigura 6.34: Tipos de símbolos disponíveis para representar pontos.\nAssim, é possível controlar cores, formas e preenchimento combinado os argumentos shape, fill e color com função scale_manual(). É importante notar que para os símbolos entre 15 e 20 só podemos controlar o argumento cor, enquanto os símbolos entre 21 e 25 podemos controlar cor e o preenchimento (Figura 6.35).\nFigura 6.35: Gráfico de dispersão relacionando variáveis comprimento_bico e comprimento_nadadeira para cada espécie de pinguim, com variações na cor e preenchimento dos pontos.\nDefinindo linhas de ajusteQuando usamos modelos estatísticos como, por exemplo, lm(), glm(), gam(), entre outros (veja mais nos Capítulos 7 e 8), podemos utilizar os valores preditos para demonstrar relação entre variáveis X e Y. ggplot2, função geom_smooth() faz esse ajuste com certa simplicidade (Figura 6.36).\nFigura 6.36: Gráfico de dispersão relacionando variáveis comprimento_bico e comprimento_nadadeira para cada espécie de penguim, com linhas de ajustes estatísticos.\nAjustes finos (versão personalizada)\nFigura 6.37: Gráfico de dispersão relacionando variáveis comprimento_bico e comprimento_nadadeira para cada espécie de penguim, com ajustes finos.\nAlém disso, podemos relacionar dados não tão usuais. Recomendamos leitura artigo de Matejka & Fitzmaurice (2017) que apresenta armadilhas típicas que dados podem gerar quando evitamos de visualizá-los previamente (Figura 6.38).\nFigura 6.38: Gráfico de dispersão relacionando variáveis x e y formando um dinossauro.\n","code":"\n## Gráfico de dispersão das coluna comprimento_nadadeira e comprimento_bico\nggplot(data = penguins, aes(x = comprimento_bico, y = comprimento_nadadeira)) +\n    geom_point()\n## Cor e tamanho dos pontos \nggplot(data = penguins, aes(x = comprimento_bico, y = comprimento_nadadeira)) +\n    geom_point(color = \"cyan4\", size = 4) + \n    labs(title = \"Sem transparência\")\n\n## Cor, tamanho dos pontos e transparência\nggplot(data = penguins, aes(x = comprimento_bico, y = comprimento_nadadeira)) +\n    geom_point(color = \"cyan4\", size = 4, alpha = .4) +\n    labs(title = \"Com transparência\")\n## Formato e tamanho\nggplot(data = penguins, aes(x = comprimento_bico, y = comprimento_nadadeira)) +\n    geom_point(shape = 1, size = 4)\n\n## Formato e tamanho para espécies\nggplot(data = penguins, aes(x = comprimento_bico, y = comprimento_nadadeira, color = especies)) +\n    geom_point(shape = 19, size = 4)\n\n## Formato e tamanho e cor\nggplot(data = penguins, aes(x = comprimento_bico, y = comprimento_nadadeira, fill = especies)) +\n    geom_point(shape = 21, size = 4, color = \"black\")\n## Linha de ajuste\nggplot(data = penguins, aes(x = comprimento_bico, y = comprimento_nadadeira)) +\n    geom_point(shape = 20, size = 4, color = \"black\") +\n    geom_smooth(method = lm)\n## Gráfico de dispersão exemplo\nggplot(data = penguins, aes(x = comprimento_bico, y = comprimento_nadadeira,\n                            color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    scale_shape_manual(values = c(19, 15, 17)) +\n    scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw(base_size = 16) +\n    labs(x = \"Comprimento do bico (mm)\", y = \"Comprimento da nadadeira (mm)\", \n         color = \"Espécies\", shape = \"Espécies\")\n## Gráfico do dinossauro\ndatasaurus_dozen %>% \n    dplyr::filter(dataset == \"dino\") %>% \n    ggplot() +\n    aes(x = x, y = y) +\n    geom_point(colour = \"black\", fill = \"black\", \n               size = 4, alpha = .7, shape = 21) +\n    theme_bw() +\n    theme(axis.title = element_text(size = 24),\n          axis.text.x = element_text(size = 20),\n          axis.text.y = element_text(size = 20))"},{"path":"cap6.html","id":"visualização-de-múltiplos-gráficos-pareados","chapter":"Capítulo 6 Visualização de dados","heading":"6.4.8 Visualização de múltiplos gráficos pareados","text":"Muitas vezes precisamos plotar relação de mais de uma variável, e muitas vezes essas variáveis são de mais de um tipo (contínua, categórica, etc.). O gráfico mais indicado nesses casos é o gráfico pareado (Emerson et al. 2013) que nos auxilia à ter uma visão geral conjunto de dados e de suas interrelações. Esse gráfico também é chamado de pairs plot ou correlograma.Gráfico pareado com variáveis contínuasA função ggpairs() pacote GGally permite criar múltiplos gráficos pareados comparando variáveis contínuas seu conjunto de dados. Além de demonstrar gráficos de dispersão de cada par de variáveis, ela apresenta gráficos de densidade de cada variável individualmente e, além disso, os valores de correlação entre os pares analisados com ou sem uma potencial variável categórica (neste caso, especies) (Figura 6.39).\nFigura 6.39: Gráfico pareado relacionando várias variáveis contínuas para espécies de penguins.\nGráfico pareado com vários tipos de variáveisComo alternativa, função ggpairs() permite também incluir variáveis categóricas nas comparações. Neste caso, ela reconhece o tipo de gráfico (boxplot, dispersão, etc.) partir dos modos das variáveis (Figura 6.40).\nFigura 6.40: Gráfico pareado relacionando várias variáveis contínuas e categóricas para espécies de penguins.\n","code":"\n## Gráfico pareado com variáveis contínuas\npenguins %>%\n    dplyr::select(massa_corporal, comprimento_bico, profundidade_bico, comprimento_nadadeira) %>%\n    GGally::ggpairs(aes(color = penguins$especies)) +\n    scale_colour_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw()\n## Gráfico pareado com variáveis contínuas e categóricas\npenguins %>%\n    dplyr::select(sexo, massa_corporal, comprimento_bico, profundidade_bico, comprimento_nadadeira) %>%\n    GGally::ggpairs(aes(color = sexo)) +\n    scale_colour_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    scale_fill_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw()"},{"path":"cap6.html","id":"erros-comuns-dos-usuários-do-ggplot2-e-como-evitá-los","chapter":"Capítulo 6 Visualização de dados","heading":"6.4.9 Erros comuns dos usuários do ggplot2 e como evitá-los","text":"Abaixo, apresentamos uma lista não exaustiva dos erros mais comuns que cometemos (e vimos muitos usuários cometerem) ao fazer gráficos ggplot2.Utilizar ajuste manual nas funções scale_shape_manual(), scale_color_manual() ou scale_fill_manual() sem indicar argumento aes() variáveis que devem definir cada um desses elementos gráficosUtilizar ajuste manual nas funções scale_shape_manual(), scale_color_manual() ou scale_fill_manual() sem indicar argumento aes() variáveis que devem definir cada um desses elementos gráficosUtilizar ajuste manual na função scale_size_manual() indicando uma variável categórica em vez de numéricaUtilizar ajuste manual na função scale_size_manual() indicando uma variável categórica em vez de numéricaNúmero de cores indicadas como valores scale_fill_manual() ou scale_color_manual(): ao definir cores de maneira personalizada (ou seja, não usando o padrão da função) é muito comum utilizarmos o número de cores usados por algum tutorial ou livro. Com frequência, o exemplo seguido e seus dados não possuem o mesmo número de cores dos nossos dados. Deste modo, você pode usar códigos R para ajudar quantificar o número de cores necessárias. Por exemplo, para os dados penguins, o código seguir indica o número de cores necessárias: length(levels(penguins\\$species)). Assim, será necessário indicar três cores diferentes dentro da função scale_*().Número de cores indicadas como valores scale_fill_manual() ou scale_color_manual(): ao definir cores de maneira personalizada (ou seja, não usando o padrão da função) é muito comum utilizarmos o número de cores usados por algum tutorial ou livro. Com frequência, o exemplo seguido e seus dados não possuem o mesmo número de cores dos nossos dados. Deste modo, você pode usar códigos R para ajudar quantificar o número de cores necessárias. Por exemplo, para os dados penguins, o código seguir indica o número de cores necessárias: length(levels(penguins\\$species)). Assim, será necessário indicar três cores diferentes dentro da função scale_*().Função geom_smooth(): como explicado acima, função geom_smooth() é muito útil (e simples) para gerar linhas de ajuste (best fit) típicas de modelos lineares e não lineares. Porém, fique alerta ao usar, por exemplo, geom_smooth(method = lm), o modelo linear utilizado para testar sua predição foi o lm(). Se tiver utilizado glm()ou gam() o ajuste deve ser produzido partir desses modelos.Função geom_smooth(): como explicado acima, função geom_smooth() é muito útil (e simples) para gerar linhas de ajuste (best fit) típicas de modelos lineares e não lineares. Porém, fique alerta ao usar, por exemplo, geom_smooth(method = lm), o modelo linear utilizado para testar sua predição foi o lm(). Se tiver utilizado glm()ou gam() o ajuste deve ser produzido partir desses modelos.Uso incorreto modo das variáveis: neste caso, o usuário utilizar uma variável numérica (por exemplo, 1, 2 e 3) como variável categórica. Neste caso, é preciso transformar variável numérica em variável categóricas (antes de fazer o ggplot2 ou dentro aes()).Uso incorreto modo das variáveis: neste caso, o usuário utilizar uma variável numérica (por exemplo, 1, 2 e 3) como variável categórica. Neste caso, é preciso transformar variável numérica em variável categóricas (antes de fazer o ggplot2 ou dentro aes()).Veja exemplos (Figura 6.41).\nFigura 6.41: Gráficos mostrando erros comuns na criação de gráficos pacote ggplot2.\n","code":"\n## Figura incorreta, sem a transformação da variável ano\npenguins %>%\n    ggplot(aes(x = ano, y = comprimento_bico)) +\n    geom_boxplot() + \n    theme_bw() +\n    labs(title = \"Figura incorreta\")\n\n## Figura correta, com transformação interna da variável ano\npenguins %>%\n    ggplot(aes(x = factor(ano), y = comprimento_bico)) +\n    geom_boxplot() + \n    theme_bw() +\n    labs(title = \"Figura correta com transformação interna\")\n\n## Figura correta, com transformação prévia da variável ano\npenguins %>%\n    mutate(ano_f = as.factor(ano)) %>% \n    ggplot(aes(x = ano_f, y = comprimento_bico)) +\n    geom_boxplot() + \n    theme_bw() +\n    labs(title = \"Figura correta com transformação prévia\")"},{"path":"cap6.html","id":"finalização-de-gráficos-para-publicação","chapter":"Capítulo 6 Visualização de dados","heading":"6.5 Finalização de gráficos para publicação","text":"","code":""},{"path":"cap6.html","id":"posição-cores-e-fonte-da-legenda","chapter":"Capítulo 6 Visualização de dados","heading":"6.5.1 Posição, cores e fonte da legenda","text":"É possível controlar posição, cores e fonte da legenda em diversos locais com alguns argumentos dentro da função theme():legend.position: controla posição na área gráfico: top, right, bottom, left ou none. Além disso, é possível inserir legenda internamente gráfico indicando posições nos eixos X e Ylegend.position: controla posição na área gráfico: top, right, bottom, left ou none. Além disso, é possível inserir legenda internamente gráfico indicando posições nos eixos X e Ylegend.box: determina características retângulo onde legenda é inserida: legend.box.background (combinado com element_rect()) e legend.box.margin (combinado com margin())legend.box: determina características retângulo onde legenda é inserida: legend.box.background (combinado com element_rect()) e legend.box.margin (combinado com margin())legend.text: controla cor e tamanho da legenda (duas informações devem ser inseridas dentro da função element_text())legend.text: controla cor e tamanho da legenda (duas informações devem ser inseridas dentro da função element_text())legend.title: personaliza cor e tamanho da legenda também dentro da função element_text()legend.title: personaliza cor e tamanho da legenda também dentro da função element_text()Vejamos alguns exemplos (Figura 6.42).\nFigura 6.42: Gráficos mostrando finalização de gráficos para publicação em relação à posição, cores e fonte da legenda pacote ggplot2.\n","code":"\n## Legenda acima\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    scale_shape_manual(values = c(19, 15, 17)) +\n    scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw(base_size = 14) +\n    theme(legend.position = \"top\") +\n    labs(title = \"Legenda acima do gráfico\", x = \"Comprimento do bico (mm)\", \n         y = \"Profundidade do bico (mm)\", color = \"Espécies\", shape = \"Espécies\")\n\n## Leganda abaixo\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    scale_shape_manual(values = c(19, 15, 17)) +\n    scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw(base_size = 14) +\n    theme(legend.position = \"bottom\") +\n    labs(title = \"Legenda abaixo do gráfico\", x = \"Comprimento do bico (mm)\", \n         y = \"Profundidade do bico (mm)\", color = \"Espécies\", shape = \"Espécies\")\n\n## Sem legenda\nggplot(data = penguins, \n       aes(x = comprimento_bico,  y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    scale_shape_manual(values = c(19, 15, 17)) +\n    scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw(base_size = 14) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Sem legenda\", x = \"Comprimento do bico (mm)\", \n         y = \"Profundidade do bico (mm)\", color = \"Espécies\", shape = \"Espécies\")\n\n## Legenda personalizada\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    scale_shape_manual(values = c(19, 15, 17)) +\n    scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw(base_size = 14) +\n    theme(legend.position = \"right\",\n          legend.text = element_text(size = 14, colour = \"red\"),\n          legend.title = element_text(face = \"bold\"),\n          legend.box.background = element_rect(color=\"red\", size=2),\n          legend.margin = margin(6, 6, 6, 6)) +\n    labs(title = \"Legenda personalizada\", x = \"Comprimento do bico (mm)\", \n         y = \"Profundidade do bico (mm)\", color = \"Espécies\", shape = \"Espécies\")\n\n## Legenda interna\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    scale_shape_manual(values = c(19, 15, 17)) +\n    scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw(base_size = 16) +\n    theme(legend.position = c(.2, .8),\n          legend.title = element_blank(),\n          legend.key =  element_blank(),\n          legend.background = element_blank(),\n          legend.text = element_text(size = 12, face = \"bold\")) +\n    labs(title = \"Legenda interna\", x = \"Comprimento do bico (mm)\", \n         y = \"Profundidade do bico (mm)\", color = \"Espécies\", shape = \"Espécies\")"},{"path":"cap6.html","id":"elementos-gráficos-eixo-fonte-gride","chapter":"Capítulo 6 Visualização de dados","heading":"6.5.2 Elementos gráficos: eixo, fonte, gride","text":"O gráfico padronizado (sem edição extra) geralmente não traz elementos mínimos para publicação em revistas, livros ou periódicos. Além controle da posição, cor e tamanho da legenda, é fundamental personalizar os seguintes elementos: eixo, fonte e gride.EixosVariação: define limites mínimos e máximos para os eixos X (xlim()) e Y (ylim())Intervalo: define o valor intervalo entre os números dos eixos X e YVejamos mais alguns exemplos (Figura 6.43).\nFigura 6.43: Gráficos mostrando finalização de gráficos para publicação em relação aos elementos gráficos de eixo, fonte e gride pacote ggplot2.\nFonte dos eixos X e YTipoTamanhoCorFace (itálico, negrito, etc.)ÂnguloVejamos outros exemplos com mudanças nos eixos (Figura 6.44).\nFigura 6.44: Gráficos mostrando finalização de gráficos para publicação em relação aos eixos pacote ggplot2.\nGrideLinhas de grade principais (panel.grid.major)Linhas de grade secundárias (panel.grid.minor)Borda gráfico (panel.border)Vejamos outros exemplos com mudanças ao gride (Figura 6.45).\nFigura 6.45: Gráficos mostrando finalização de gráficos para publicação em relação ao gride pacote ggplot2.\n","code":"\n## Nome dos eixos\nggplot(data = penguins,  \n       aes(x = comprimento_bico, y = comprimento_nadadeira, \n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .4) +\n    ylim(170, 230) +\n    xlim(30, 60) +\n    labs(title = \"Nome dos eixos\", x = \"Eixo X\", y = \"Eixo Y\")\n\n## Intervalo dos eixos\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .4) +\n    scale_x_continuous(limits = c(30, 60), breaks = seq(30, 60, 2)) +\n    labs(title = \"Intervalo dos eixos\", x = \"Eixo X\", y = \"Eixo Y\")\n## Cor e fonte dos eixos\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .4) +\n    theme(axis.title.x = element_text(face = \"bold\", size = 20, colour = \"cyan4\"),\n          axis.text.x = element_text(size = 14),\n          axis.title.y = element_text(face = \"bold\", size = 20, colour = \"cyan4\"),\n          axis.text.y = element_text(size = 14)) +\n    labs(title = \"Cor e fonte dos eixos\", x = \"Eixo X\", y = \"Eixo Y\")\n\n## Intervalo e ângulos do texto dos eixos\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .4) +\n    scale_x_continuous(limits = c(20, 60), breaks = seq(20, 60, 2)) +\n    theme(axis.title.x = element_text(face = \"bold\", size = 20, colour = \"cyan4\"),\n          axis.text.x = element_text(size = 14, angle = 45),\n          axis.title.y = element_text(face = \"bold\", size = 20, colour = \"cyan4\"),\n          axis.text.y = element_text(size = 14)) +\n    labs(title = \"Intervalo e ângulos do texto dos eixos\", x = \"Eixo X\", y = \"Eixo Y\")\n## Linhas de grade principais\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .4) +\n    scale_x_continuous(limits = c(30, 60), breaks = seq(30, 60, 5)) +\n    theme(axis.title.x = element_text(face = \"bold\", size = 16),\n          axis.text.x = element_text(size = 12),\n          axis.title.y = element_text(face = \"bold\", size = 16),\n          axis.text.y = element_text(size = 12),\n          panel.grid.minor = element_blank()) +\n    labs(title = \"Linhas de grade principais\", x = \"Eixo X\", y = \"Eixo Y\")\n\n## Retirar linhas de grade\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .4) +\n    scale_x_continuous(limits = c(30, 60), breaks = seq(30, 60, 5)) +\n    theme(axis.title.x = element_text(face = \"bold\", size = 16),\n          axis.text.x = element_text(size = 12),\n          axis.title.y = element_text(face = \"bold\", size = 16),\n          axis.text.y = element_text(size = 12),\n          panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank()) +\n    labs(title = \"Retirar linhas de grade\", x = \"Eixo X\", y = \"Eixo Y\")\n\n## Borda do gráfico\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = 0.5) +\n    scale_x_continuous(limits = c(30, 60), breaks = seq(30, 60, 5)) +\n    theme(axis.title.x = element_text(face = \"bold\", size = 16),\n          axis.text.x = element_text(size = 12),\n          axis.title.y = element_text(face = \"bold\", size = 16),\n          axis.text.y = element_text(size = 12),\n          panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank(),\n          panel.border = element_rect(size = 2, colour = \"black\", fill = NA)) +\n    labs(title = \"Borda do gráfico\", x = \"Eixo X\", y = \"Eixo Y\")\n\n## Borda do gráfico\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .4) +\n    scale_x_continuous(limits = c(30, 60), breaks = seq(30, 60, 5)) +\n    theme(axis.title.x = element_text(face = \"bold\", size = 16),\n          axis.text.x = element_text(size = 12),\n          axis.title.y = element_text(face = \"bold\", size = 16),\n          axis.text.y = element_text(size = 12),\n          panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank(),\n          axis.line = element_line(size = 1)) +\n    labs(title = \"Borda do gráfico\", x = \"Eixo X\", y = \"Eixo Y\")"},{"path":"cap6.html","id":"temas-personalizados-ggtheme","chapter":"Capítulo 6 Visualização de dados","heading":"6.5.3 Temas personalizados ggtheme()","text":"Existem vários temas criados dentro universo ggtheme() que podem facilitar escolha de um modelo com ótima qualidade para publicação. Abaixo, demonstramos os modelos mais utilizados (Figura 6.46).\nFigura 6.46: Gráficos mostrando finalização de gráficos para publicação em relação ao ggtheme() pacote ggplot2.\n","code":"\n## theme_gray\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    scale_shape_manual(values = c(19, 15, 17)) +\n    scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_gray(base_size = 16) +\n    labs(title = \"theme_gray()\", x = \"Comprimento do bico (mm)\", \n         y = \"Profundidade do bico (mm)\", color = \"Espécies\", shape = \"Espécies\") \n## theme_bw()\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    scale_shape_manual(values = c(19, 15, 17)) +\n    scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_bw(base_size = 16) +\n    labs(title = \"theme_bw()\", x = \"Comprimento do bico (mm)\",\n         y = \"Profundidade do bico (mm)\", color = \"Espécies\", shape = \"Espécies\") \n## theme_classic()\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    scale_shape_manual(values = c(19, 15, 17)) +\n    scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme_classic(base_size = 16) +\n    labs(title = \"theme_classic()\", x = \"Comprimento do bico (mm)\", \n         y = \"Profundidade do bico (mm)\", color = \"Espécies\", shape = \"Espécies\")"},{"path":"cap6.html","id":"criando-seu-próprio-theme_custom","chapter":"Capítulo 6 Visualização de dados","heading":"6.5.4 Criando seu próprio theme_custom()","text":"Por fim, é possível criar um tema personalizado como uma função. Assim, o usuário pode controlar todos os elementos gráficos em um único código. O maior benefício de personalizar uma função é que não será necessário fazer os ajustes finos em todos os gráficos que tiver construindo, o que pode representar grande economia de tempo e linhas de código (Figura 6.47).\nFigura 6.47: Gráficos mostrando finalização de gráficos para publicação em relação à criação de um tema usando o theme_custom() pacote ggplot2.\n","code":"\n## Criar uma função com os ajustes finos\ntema_personalizado <- function(){\n    \n    # Defina uma fonte\n    font <- \"Times\" # Digite names(pdfFonts()) no console do R para ver a lista de fontes disponíveis\n    \n    theme(\n        \n        # Defina elementos do gride\n        panel.grid.major = element_line(colour = \"#d3d3d3\"),  \n        panel.grid.minor = element_blank(),\n        axis.ticks = element_blank(), \n        panel.border = element_rect(colour = \"black\", fill = NA, size = .5),\n        \n        # Defina elementos textuais\n        # Título\n        plot.title = element_text(             \n            family = font,            # Fonte\n            size = 20,                # Tamanho da fonte\n            face = 'bold',            # Tipo de fonte\n            hjust = 0,                # Alinhamento horizontal\n            vjust = 2),               # Alinhamento vertical\n        \n        # Subtítulo\n        plot.subtitle = element_text(          \n            family = font,            # Fonte\n            size = 14),               # Tamanho da fonte\n        \n        # Rúbrica\n        plot.caption = element_text(           \n            family = font,            # Fonte\n            size = 10,                # Tamanho da fonte\n            hjust = 1),               # Alinhamento horizontal\n        \n        # Título dos eixos\n        axis.title = element_text(             \n            family = font,            # Fonte\n            size = 14),               # Tamanho da fonte\n        \n        # Texto dos eixos\n        axis.text = element_text(              \n            family = font,            # Fonte\n            size = 14)              # Tamanho da fonte\n        \n    )}\n\n## Gráfico usando a função de tema criada\nggplot(data = penguins, \n       aes(x = comprimento_bico, y = comprimento_nadadeira,\n           color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    scale_shape_manual(values = c(19, 15, 17)) +\n    scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    tema_personalizado() +\n    labs(title = \"Tema personalizado\", x = \"Comprimento do bico (mm)\", \n         y = \"Profundidade do bico (mm)\", color = \"Espécies\", \n         shape = \"Espécies\", caption = \"Fonte = palmerpinguins\")"},{"path":"cap6.html","id":"exportando-gráficos-com-alta-qualidade-com-a-função-ggsave","chapter":"Capítulo 6 Visualização de dados","heading":"6.5.5 Exportando gráficos com alta qualidade com a função ggsave()","text":"O último passo para construir gráficos com qualidade de publicação é exportar em um formato específico, como .png, .pdf ou .svg (entre outros). função ggsave() não só permite que você tenha o controle sobre o formato, mas também sobre qualidade e tamanho desejados com os seguintes argumentos:width: largura gráficoheight: altura gráficounits: unidade de medida (cm, mm) gráfico para definir largura e tamanhodpi: resolução ou qualidade da imagem, medida em pontos por polegada (dots per inch) (padrão = 300)Vejamos um último exemplo (Figura 6.48).\nFigura 6.48: Gráfico criado para exportação pacote ggplot2.\nPodemos exportar esse gráfico de diferentes formas.","code":"\n## Gráfico\ng1 <- ggplot(data = penguins, \n             aes(x = comprimento_bico, y = comprimento_nadadeira,\n                 color = especies, shape = especies)) +\n    geom_point(size = 4, alpha = .7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    scale_shape_manual(values = c(19, 15, 17)) +\n    scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n    theme(legend.position = c(.1, .1),\n          legend.title = element_blank(),\n          legend.key =  element_blank(),\n          legend.background = element_blank()) +\n    tema_personalizado() +\n    labs(x = \"Comprimento do bico (mm)\", y = \"Profundidade do bico (mm)\", \n         color = \"Espécies\", shape = \"Espécies\")\ng1\n## Exportar no formato PDF\nggsave(filename = \"g1.pdf\", \n       plot = g1, \n       width = 15, \n       height = 15, \n       dpi =300,\n       units = \"cm\")\n\n## Exportar no formato PNG\nggsave(filename = \"g1.png\", \n       plot = g1, \n       width = 15, \n       height = 15, \n       dpi =300,\n       units = \"cm\")\n\n## Exportar no formato SVG\nggsave(filename = \"g1.svg\", \n       plot = g1, \n       width = 15, \n       height = 15, \n       dpi =300,\n       units = \"cm\")"},{"path":"cap6.html","id":"para-se-aprofundar-2","chapter":"Capítulo 6 Visualização de dados","heading":"6.6 Para se aprofundar","text":"Listamos abaixo livros e links que recomendamos para seguir com sua aprendizagem em R e gráficos.","code":""},{"path":"cap6.html","id":"livros-2","chapter":"Capítulo 6 Visualização de dados","heading":"6.6.1 Livros","text":"Recomendamos aos interessados os livros: ) Chang (2018) R Graphics Cookbook, ii) Healy (2018) Data Visualization: practical introduction, iii) Rahlf (2019) Data Visualisation R: 111 Examples, iv) Sievert (2020) Interactive web-based data visualization R, plotly, shiny, v) Wickham (2016) ggplot2: elegant graphics data analysis, vi) Wilke (2019) Fundamentals Data Visualization, vii) Wilkinson e colaboradores (2005) Grammar Graphics.","code":""},{"path":"cap6.html","id":"links-2","chapter":"Capítulo 6 Visualização de dados","heading":"6.6.2 Links","text":"Existem centenas de ferramentas online para aprender visualização de dados e ggplot2. Tais ferramentas são bastante variadas, incluindo desde escolha de tipos de gráficos à seleção de cores. Dentre elas, indicamos os seguintes links em inglês:Visualização\n- Data Visualization R - Rob Kabacoff\n- Data Visualisation Catalogue\n- R Graph Gallery\n- Data Viz\n- Data Viz Project\n- QuickChart\n- Chart.js\n- drawdata.xyzSeleção de cores (paletas)\n- Color Brewer: color advice\n- Wes Anderson Palletes\n- Choosing colors data visualization","code":""},{"path":"cap6.html","id":"exercícios-2","chapter":"Capítulo 6 Visualização de dados","heading":"6.7 Exercícios","text":"6.1\nUtilizando o banco de dados penguins compare o comprimento bico entre diferentes espécies de penguins. Utilize um gráfico de caixa (boxplot) para ilustrar variação intraespecífica e possíveis outliers nos dados. Para melhorar o seu gráfico, lembre-se de nomear os dois eixos corretamente, definir um tema e posicionar legenda.6.2\nUtilizando o banco de dados penguins faça um histograma com distribuição da massa corporal para cada uma das espécies. Utilize uma cor de preenchimento para cada espécie.6.3\nUtilizando o banco de dados penguins, faça três gráficos com o mesmo eixo Y e eixo X. Coloque o comprimento das nadadeiras eixo Y e espécies de pinguins eixo X. primeiro gráfico, utilize o geom_jitter() para plotar os dados brutos. segundo gráfico, utilize o geom_violin() para mostrar distribuição de densidade dos dados. terceiro gráfico, utilize o geom_boxplot() para destacar mediana e os quartis.6.4\nSe você conseguiu resolver o exercício 6.3, agora dê um passo mais e compare os três gráficos lado lado utilizando função grid.arrange(). Lembre-se de colocar um título informativo em cada um dos gráficos antes de juntá-los em uma prancha única. Ao comparar os três tipos de gráficos, qual você considera mais informativo? Experimente combinar mais de um “geom” (camadas) e produzir gráficos ainda mais interessantes.6.5\nUtilize o conjunto de dados ecodados::anova_dois_fatores para construir um gráfico de barras com média e o erro padrão Tempo (Tempo para eliminar droga corpo) eixo Y em função da variável Pessoas (XX, ou XY) e Idade (jovem ou idoso). Antes de fazer o gráfico leia com atenção descrição mesmo através comando ?ecodados::anova_dois_fatores. Uma dica, utilize fill dentro aes() para representar um dos fatores (ex. Pessoas). O outro fator você pode representar eixo X. Veja se consegue, se não conseguir pode olhar cola com solução para aprender como é feito. Outra dica, pesquise sobre função stat_summary() ela irá te ajudar calcular média e o erro padrão dentro comando que gera o gráfico.6.6\nUtilize o banco de dados penguins para criar um gráfico de dispersão entre o tamanho da nadadeira (eixo Y) e massa corporal (eixo X). Utilize o argumento fill para ilustrar com cores diferenças entre os sexos e utilize função facte_grid() para criar um gráfico separado para cada espécie de pinguim. Se você não conhece essa função, dê uma olhada help ?facet_grid(). Você também pode utilizar função drop_na() para excluir os dados faltantes da coluna sexo.Soluções dos exercícios.","code":""},{"path":"cap7.html","id":"cap7","chapter":"Capítulo 7 Modelos lineares","heading":"Capítulo 7 Modelos lineares","text":"","code":""},{"path":"cap7.html","id":"pré-requisitos-do-capítulo-3","chapter":"Capítulo 7 Modelos lineares","heading":"Pré-requisitos do capítulo","text":"Pacotes e dados que serão utilizados neste capítulo. 📝 Importante \nEstatísticas frequentistas como que serão abordadas neste capítulo são baseadas em testes estatísticos (e.g., F, t, 𝛘2, etc.), que são resultados númericos teste e possuem um valor de probabilidade (valor de P) associado com este teste (Gotelli Ellison 2012). O valor de P mede probabilidade que os valores observados ou mais extremos seriam encontrados caso hipótese nula seja verdadeira (veja Capítulo 2). Ao longo livro usaremos o critério convencional de rejeitar hipótese nula quando P < 0.05. Contudo, sugerimos leitura destes artigos (White et al. 2014; Barber Ogle 2014; K. P. Burnham Anderson 2014; Murtaugh 2014; Halsey 2019; Muff et al. 2021) que discutem limitações e problemas associados ao valor de P.","code":"\n## Pacotes\nlibrary(ecodados)\nlibrary(car)\nlibrary(ggpubr)\nlibrary(ggforce)\nlibrary(lsmeans)\nlibrary(lmtest)\nlibrary(sjPlot)\nlibrary(nlme)\nlibrary(ape)\nlibrary(fields)\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(rdist)\n\n## Dados\nCRC_PN_macho <- ecodados::teste_t_var_igual\nCRC_LP_femea <- ecodados::teste_t_var_diferente\nPareado <- ecodados::teste_t_pareado\ncorrelacao_arbustos <- ecodados::correlacao\ndados_regressao <- ecodados::regressoes\ndados_regressao_mul <- ecodados::regressoes\ndados_anova_simples <- ecodados::anova_simples\ndados_dois_fatores <- ecodados::anova_dois_fatores\ndados_dois_fatores_interacao <- ecodados::anova_dois_fatores\ndados_dois_fatores_interacao2 <- ecodados::anova_dois_fatores_interacao2\ndados_bloco <- ecodados::anova_bloco\ndados_ancova <- ecodados::ancova\ndata(\"mite\")\ndata(\"mite.xy\")\ncoords <- mite.xy\ncolnames(coords) <- c(\"long\", \"lat\")\ndata(\"mite.env\")"},{"path":"cap7.html","id":"teste-t-de-student-para-duas-amostras-independentes","chapter":"Capítulo 7 Modelos lineares","heading":"7.1 Teste T (de Student) para duas amostras independentes","text":"Uma das perguntas mais comuns em estatística é saber se há diferença entre médias de dois grupos ou tratamentos. Para responder esta pergunta, William Sealy Gosset, químico da cervejaria Guinness, desenvolveu em 1908 o Teste T que é uma estatística que segue uma distribuição t de Student para rejeitar ou não uma hipótese nula de médias iguais entre dois grupos.\\[t = \\frac{(\\bar{X}_1 - \\bar{X}_2)}{\\sqrt{\\frac{2S^2_p}{n}}}\\]Onde:\\(\\bar{X}\\)1 - \\(\\bar{X}\\)2 = diferença entre médias de duas amostrasS2p = desvio padrão das amostrasn = tamanho das amostrasPremissas Teste tAs amostras devem ser independentesAs unidades amostrais são selecionadas aleatoriamenteDistribuição normal (gaussiana) dos resíduosHomogeneidade da variância 📝 Importante Zar (2010) indica que o Teste T é robusto mesmo com moderada violação da normalidade, principalmente se o tamanho amostral alto.Zar (2010) indica que o Teste T é robusto mesmo com moderada violação da normalidade, principalmente se o tamanho amostral alto.Caso variâncias não sejam homogêneas, isso deve ser informado na linha de comando, pois o denominador da fórmula acima será corrigido.Caso variâncias não sejam homogêneas, isso deve ser informado na linha de comando, pois o denominador da fórmula acima será corrigido.Avaliação das premissasUma das maneiras de avaliarmos premissas de normalidade e homogeneidade da variância relacionadas às análises teste T, ANOVA e regressões lineares simples e múltiplas é o uso da inspeção gráfica da distribuição dos resíduos (Figura 7.1) (. F. Zuur, Ieno, Elphick 2010).premissa de homogeneidade da variância pode ser avaliada através gráfico de dispersão dos resíduos (eixo X) pelos valores preditos da variável resposta (eixo Y) (Figura 7.1A). distribuição dos resíduos será homogênea se não observarmos nenhum padrão na distribuição dos pontos (.e. forma em V, U ou funil).premissa de homogeneidade da variância pode ser avaliada através gráfico de dispersão dos resíduos (eixo X) pelos valores preditos da variável resposta (eixo Y) (Figura 7.1A). distribuição dos resíduos será homogênea se não observarmos nenhum padrão na distribuição dos pontos (.e. forma em V, U ou funil).premissa de normalidade dos resíduos pode ser avaliada através gráfico de quantis-quantis (QQ-plots). distribuição dos resíduos será normal se os pontos estiverem próximos à reta (Figura 7.1B).premissa de normalidade dos resíduos pode ser avaliada através gráfico de quantis-quantis (QQ-plots). distribuição dos resíduos será normal se os pontos estiverem próximos à reta (Figura 7.1B).\nFigura 7.1: Inspeção gráfica da homogeneidade da variância () e normalidade dos resíduos (B). Os símbolos verdes indicam que os gráficos em que os resíduos apresentam distribuição homogênea e normal, enquanto os símbolos vermelhos indicam os gráficos em que os resíduos violam premissas teste.\nExemplo prático 1 - Teste T para duas amostras com variâncias iguaisExplicação dos dadosNeste exemplo, avaliaremos o comprimento rostro-cloacal (CRC em milímetros) de machos de Physalaemus nattereri (Anura:Leptodactylidae) amostrados em diferentes estações ano com armadilhas de interceptação e queda na Região Noroeste Estado de São Paulo (da Silva Rossa-Feres 2010).PerguntaO CRC dos machos de P. nattereri é maior na estação chuvosa que na estação seca?PrediçõesO CRC dos machos será maior na estação chuvosa porque há uma vantagem seletiva para os indivíduos maiores durante atividade reprodutivaVariáveisData frame com os indivíduos (unidade amostral) nas linhas e CRC (mm - variável resposta contínua) e estação (variável preditora categórica) como colunasChecklistVerificar se o seu data frame está com unidades amostrais nas linhas e variáveis preditoras e respostas nas colunasAnáliseVamos olhar os dados usando função head().Vamos verificar normalidade dos resíduos usando o QQ-plot (Figura 7.2).\nFigura 7.2: Normalidade dos resíduos usando o QQ-plot.\nOs pontos estão próximos à reta, indicando que distribuição dos resíduos é normal (veja Figura 7.1).Outra possibilidade é usar os testes de Shapiro-Wilk e Levene para verificar normalidade e homogeneidade da variância, respectivamente. 📝 Importante \nHipótese Nula (H0) destes testes é que distribuição é normal ou homogênea:Valor de p < 0.05 significa que os dados não apresentam distribuição normal ou homogêneaValor de p > 0.05 significa que os dados apresentam distribuição normal ou homogêneaTeste de Shapiro-Wilk para normalidade dos resíduos.Teste de Levene para homogeneidade de variância dos resíduos.Percebam que distribuição dos resíduos foi normal e homogênea na inspeção gráfica, assim como nos testes de Shapiro e Levene, respectivamente. Agora podemos realizar análise sabendo que os dados seguem premissas requeridas pelo Teste T.Vamos para os códigos da análise Teste T para amostragens independentes e variâncias iguais.Quatro valores devem ser apresentados aos leitores: ) estatística teste - representada por t = 4,15, ii) valor de significância - representado por p-value = 0,0001, iii) graus de liberdade - representado por df = 49, e iv) diferença entre médias. Veja abaixo como descrever os resultados seu trabalho.Visualizar os resultados em gráfico (Figura 7.3).\nFigura 7.3: Boxplot da análise Teste T para duas amostras com variâncias iguais.\nInterpretação dos resultadosNeste exemplo, rejeitamos hipótese nula de que médias CRC dos machos entre estações seca e chuvosa são iguais. Os resultados mostram que os machos de P. nattereri coletados na estação chuvosa foram em média 0,43 mm maiores que os machos coletados na estação seca (t49 = 4,15, P < 0,001).Exemplo prático 2 - Teste T para duas amostras independentes com variâncias diferentesExplicação dos dadosNeste exemplo, avaliaremos o comprimento rostro-cloacal (CRC - milímetros) de fêmeas de Leptodactylus podicipinus amostradas em diferentes estações ano com armadilhas de interceptação e queda na Região Noroeste Estado de São Paulo (da Silva Rossa-Feres 2010). 📝 Importante \nOs dados foram alterados em relação publicação original para se enquadrarem exemplo de amostras com variâncias diferentes.PerguntaO CRC das fêmeas de L. podicipinus é maior na estação chuvosa que na estação seca?PrediçõesO CRC das fêmeas será maior na estação chuvosa porque há uma vantagem seletiva para os indivíduos maiores durante atividade reprodutivaVariáveisData frame com os indivíduos (unidade amostral) nas linhas e CRC (mm - variável resposta contínua) e estação (variável preditora categórica) como colunasChecklistVerificar se o seu data frame está com unidades amostrais nas linhas e variáveis preditoras e respostas nas colunasAnáliseOlhar os dados usando função head().Vamos avaliar premissas teste. Começando com o teste de normalidade (Figura 7.4).\nFigura 7.4: Normalidade dos resíduos usando o QQ-plot.\nOs resíduos apresentam distribuição normal. Vamos testar também com o Shapiro-Wilk para normalidade dos resíduos.Agora vamos avaliar homogeneidade da variância.Os resíduos não apresentam homogeneidade da variância. Portanto, vamos realizar o Teste T com variâncias diferentes. Para isso, use o argumento var.equal = FALSE.Neste exemplo, não rejeitamos hipótese nula e consideramos que médias CRC das fêmeas entre estações seca e chuvosa são iguais (t6,49 = 1,76, P = 0,12).Vamos visualizar os resultados em um gráfico (Figura 7.5).\nFigura 7.5: Boxplot da análise Teste T para duas amostras independentes com variâncias diferentes.\nInterpretação dos resultadosOs resultados mostram que fêmeas de L. podicipinus coletadas na estação chuvosa não são maiores que fêmeas coletadas na estação seca, apesar de possuírem maior variância, o que pode ser biologicamente interessante.","code":"\n## Cabeçalho dos dados\nhead(CRC_PN_macho) \n#>    CRC Estacao\n#> 1 3.82 Chuvosa\n#> 2 3.57 Chuvosa\n#> 3 3.67 Chuvosa\n#> 4 3.72 Chuvosa\n#> 5 3.75 Chuvosa\n#> 6 3.83 Chuvosa\n## Teste de normalidade\nresiduos <- lm(CRC ~ Estacao, data = CRC_PN_macho)\nqqPlot(residuos)\n#> [1] 22 26\n## Teste de Shapiro-Wilk\nresiduos_modelo <- residuals(residuos)\nshapiro.test(residuos_modelo)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  residuos_modelo\n#> W = 0.98307, p-value = 0.6746\n## Teste de homogeneidade de variância\nleveneTest(CRC ~ as.factor(Estacao), data = CRC_PN_macho)\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>       Df F value Pr(>F)\n#> group  1  1.1677 0.2852\n#>       49\n## Análise Teste T \nt.test(CRC ~ Estacao, data = CRC_PN_macho, var.equal = TRUE)\n#> \n#>  Two Sample t-test\n#> \n#> data:  CRC by Estacao\n#> t = 4.1524, df = 49, p-value = 0.000131\n#> alternative hypothesis: true difference in means between group Chuvosa and group Seca is not equal to 0\n#> 95 percent confidence interval:\n#>  0.2242132 0.6447619\n#> sample estimates:\n#> mean in group Chuvosa    mean in group Seca \n#>              3.695357              3.260870\n## Gráfico\nggplot(data = CRC_PN_macho, aes(x = Estacao, y = CRC, color = Estacao)) + \n    labs(x = \"Estações\", \n         y = expression(paste(\"CRC (mm) - \", italic(\"P. nattereri\")))) +\n    geom_boxplot(fill = c(\"darkorange\", \"cyan4\"), color = \"black\", \n                 outlier.shape = NA) +\n    geom_jitter(shape = 16, position = position_jitter(0.1), \n                cex = 5, alpha = 0.7) +\n    scale_color_manual(values = c(\"black\", \"black\")) +\n    tema_livro() +\n    theme(legend.position = \"none\")\n## Cabeçalho dos dados\nhead(CRC_LP_femea) \n#>    CRC Estacao\n#> 1 2.72 Chuvosa\n#> 2 2.10 Chuvosa\n#> 3 3.42 Chuvosa\n#> 4 1.50 Chuvosa\n#> 5 3.90 Chuvosa\n#> 6 4.00 Chuvosa\n## Teste de normalidade usando QQ-plot\nresiduos_LP <- lm(CRC ~ Estacao, data = CRC_LP_femea)\nqqPlot(residuos_LP)\n#> [1] 4 6\n## Teste de Shapiro-Wilk\nresiduos_modelo_LP <- residuals(residuos_LP)\nshapiro.test(residuos_modelo_LP)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  residuos_modelo_LP\n#> W = 0.96272, p-value = 0.8219\n## Teste de homogeneidade da variância\nleveneTest(CRC ~ as.factor(Estacao), data = CRC_LP_femea)\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>       Df F value  Pr(>F)  \n#> group  1  9.8527 0.01053 *\n#>       10                  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Teste T\nt.test(CRC ~ Estacao, data = CRC_LP_femea, var.equal = FALSE)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  CRC by Estacao\n#> t = -1.7633, df = 6.4998, p-value = 0.1245\n#> alternative hypothesis: true difference in means between group Chuvosa and group Seca is not equal to 0\n#> 95 percent confidence interval:\n#>  -1.5489301  0.2375016\n#> sample estimates:\n#> mean in group Chuvosa    mean in group Seca \n#>              2.834286              3.490000\n## Gráfico\nggplot(data = CRC_LP_femea, aes(x = Estacao, y = CRC, color = Estacao)) + \n    geom_boxplot(fill = c(\"darkorange\", \"cyan4\"), width = 0.5, \n                 color = \"black\", outlier.shape = NA, alpha = 0.7) +\n    geom_jitter(shape = 20, position = position_jitter(0.2), color = \"black\", cex = 5) +\n    scale_color_manual(values = c(\"darkorange\", \"cyan4\")) +\n    labs(x = \"Estações\", \n         y = expression(paste(\"CRC (mm) - \", italic(\"L. podicipinus\"))), size = 15) +\n    tema_livro() +\n    theme(legend.position = \"none\")"},{"path":"cap7.html","id":"teste-t-para-amostras-pareadas","chapter":"Capítulo 7 Modelos lineares","heading":"7.2 Teste T para amostras pareadas","text":"O Teste T Pareado é uma estatística que usa dados medidos duas vezes na mesma unidade amostral, resultando em pares de observações para cada amostra (amostras pareadas). Ele determina se diferença da média entre duas observações é zero.\\[t = \\frac{\\bar{d}}{S_{\\bar{d}}}\\]Onde:\n- \\(\\bar{d}\\) = média da diferença das medidas pareadas. Observe que o teste não usa medidas originais, e sim, diferença para cada par\n- S\\(\\bar{d}\\) = erro padrão da diferença das medidas pareadasPremissas Teste t para amostras pareadasAs unidades amostrais são selecionadas aleatoriamenteAs observações não são independentesDistribuição normal (gaussiana) dos valores da diferença para cada parExemplo prático 1 - Teste T para amostras pareadasExplicação dos dadosNeste exemplo, avaliaremos diferença na riqueza de espécies de artrópodes registradas em 27 localidades. Todas localidades foram amostradas duas vezes. primeira amostragem foi realizada na localidade antes da perturbação e segunda amostragem foi realizada após localidade ter sofrido uma queimada. Portanto, existe uma dependência temporal, uma vez que amostramos mesma localidade antes e depois da queimada.PerguntaA riqueza de espécies de artrópodes é prejudicada pelas queimadas?PrediçõesA riqueza de espécies de artrópodes será maior antes da queimada devido extinção local das espéciesVariáveisdata frame com localidades nas linhas e riqueza de espécies (variável resposta contínua) e estado (Pré-queimada ou Pós-queimada - variável preditora categórica) da localidade nas colunasChecklistVerificar se o seu data frame está com unidades amostrais nas linhas e variáveis preditoras e respostas nas colunasAnáliseOlhando os dados com função head().Cálculo Teste T com amostras pareadas.Neste exemplo, rejeitamos hipótese nula de que riqueza de espécies de artrópodes é igual antes e depois da queimada (t26 = 7,57, P < 0,001).Podemos visualizar os resultados em gráfico (Figura 7.6).\nFigura 7.6: Boxplot da análise Teste T para duas amostras pareadas.\nInterpretação dos resultadosOs resultados mostram que localidades após queimadas apresentam em média 44,5 espécies de artrópodes menos que antes das queimadas.","code":"\n## Cabeçalho dos dados\nhead(Pareado) \n#>   Areas Riqueza       Estado\n#> 1     1      92 Pre-Queimada\n#> 2     2      74 Pre-Queimada\n#> 3     3      96 Pre-Queimada\n#> 4     4      89 Pre-Queimada\n#> 5     5      76 Pre-Queimada\n#> 6     6      80 Pre-Queimada\n## Análise Teste T Pareado\n\nt.test(Riqueza ~ Estado, paired = TRUE, data = Pareado)\n#> \n#>  Paired t-test\n#> \n#> data:  Riqueza by Estado\n#> t = -7.5788, df = 26, p-value = 4.803e-08\n#> alternative hypothesis: true mean difference is not equal to 0\n#> 95 percent confidence interval:\n#>  -56.63994 -32.47117\n#> sample estimates:\n#> mean difference \n#>       -44.55556\n## Gráfico\nggpaired(Pareado, x = \"Estado\", y = \"Riqueza\",\n         color = \"Estado\", line.color = \"gray\", line.size = 0.8, \n         palette = c(\"darkorange\", \"cyan4\"), width = 0.5, \n         point.size = 4, xlab = \"Estado das localidades\", \n         ylab = \"Riqueza de Espécies\") +\n    expand_limits(y = c(0, 150)) +\n    tema_livro() "},{"path":"cap7.html","id":"correlação-de-pearson","chapter":"Capítulo 7 Modelos lineares","heading":"7.3 Correlação de Pearson","text":"É um teste que mede força relativa da relação linear entre duas variáveis contínuas (X e Y). Importante ressaltar que análise de correlação não assume que variável X influencie variável Y, ou que exista uma relação de causa e efeito entre elas (Zar 2010). análise é definida em termos da variância de X, variância de Y, e covariância de X e Y (.e. como elas variam juntas).\\[r = \\frac{\\sum{XY} - \\frac{\\sum{X} \\sum{Y}}{n}}{\\sqrt{\\left(\\sum{X^2} - \\frac{\\sum{X}^2}{n}\\right)\\left(\\sum{Y^2} - \\frac{\\sum{Y}^2}{n}\\right)}}\\]Onde:r = coeficiente de correlação que indica força da relação linear entre duas variáveis. Seu limite de valores está entre -1 \\(\\leq\\) r \\(\\le\\) 1. correlação positiva indica que o aumento valor de uma das variáveis é acompanhado pelo aumento valor da outra variável. correlação negativa indica que o aumento valor de uma das variáveis é acompanhado pela diminuição valor da outra variável. Se r é igual zero, não existe correlação entre variáveis (Figura 7.7).\nFigura 7.7: Exemplo de correlações negativa (), positiva (B) e nula (C) e variáveis que não apresentam relações lineares entre si (D-E).\nPremissas da Correlação de PersonAs amostras devem ser independentes e pareadas (.e., duas variáveis devem ser medidas na mesma unidade amostral)unidades amostrais são selecionadas aleatoriamenteA relação entre variáveis tem que ser linearExemplo prático 1 - Correlação de PearsonExplicação dos dadosNeste exemplo, avaliaremos correlação entre altura tronco e o tamanho da raiz medidos em 35 indivíduos de uma espécie vegetal arbustiva.PerguntaExiste correlação entre altura tronco e o tamanho da raiz dos arbustos?PrediçõesA altura tronco é positivamente correlacionada com o tamanho da raizVariáveisdata frame com os indivíduos (unidade amostral) nas linhas e altura tronco e tamanho da raiz (duas variáveis têm que ser contínuas) como colunasChecklistVerificar se o seu data frame está com unidades amostrais nas linhas e variáveis preditoras e respostas nas colunasAnáliseVamos olhar os dados com função head().Cálculo Teste da Correlação de Pearson. Para outros testes de correlação como Kendall ou Spearman é só alterar o argumento method e inserir o teste desejado.Neste exemplo, rejeitamos hipótese nula de que variáveis não são correlacionadas (r = 0.89, P < 0,001).Podemos visualizar os resultados em gráfico (Figura 7.8).\nFigura 7.8: Gráfico mostrando relação entre variáveis e uma linha de tendência dos dados.\n 📝 Importante \nlinha de tendência tracejada gráfico é apenas para ilustrar relação positiva entre variáveis. Ela não é gerada pela análise de correlação.Interpretação dos resultadosOs resultados mostram que o aumento na altura dos arbustos é acompanhado pelo aumento tamanho da raiz.","code":"\n## Cabeçalho dos dados\nhead(correlacao_arbustos) \n#>   Tamanho_raiz Tamanho_tronco\n#> 1    10.177049       19.54383\n#> 2     6.622634       17.13558\n#> 3     7.773629       19.50681\n#> 4    11.055257       21.57085\n#> 5     4.487274       13.22763\n#> 6    11.190216       21.62902\n## Correlação de Pearson\ncor.test(correlacao_arbustos$Tamanho_raiz, correlacao_arbustos$Tamanho_tronco, method = \"pearson\")\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  correlacao_arbustos$Tamanho_raiz and correlacao_arbustos$Tamanho_tronco\n#> t = 11.49, df = 33, p-value = 4.474e-13\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.7995083 0.9457816\n#> sample estimates:\n#>       cor \n#> 0.8944449\n\n## Alternativamente\ncor.test(~ Tamanho_tronco + Tamanho_raiz, data = correlacao_arbustos, method = \"pearson\")\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  Tamanho_tronco and Tamanho_raiz\n#> t = 11.49, df = 33, p-value = 4.474e-13\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.7995083 0.9457816\n#> sample estimates:\n#>       cor \n#> 0.8944449\n## Gráfico\nggplot(data = correlacao_arbustos, aes(x = Tamanho_raiz, y = Tamanho_tronco)) + \n    labs(x = \"Tamanho da raiz (m)\", y = \"Altura do tronco (m)\") +\n    geom_point(size = 4, shape = 21, fill = \"darkorange\", alpha = 0.7) +\n    geom_text(x = 14, y = 14, label = \"r = 0.89, P < 0.001\", \n              color = \"black\", size = 5) +\n    geom_smooth(method = lm, se = FALSE, color = \"black\", linetype = \"dashed\") +\n    tema_livro() +\n    theme(legend.position = \"none\")"},{"path":"cap7.html","id":"regressão-linear-simples","chapter":"Capítulo 7 Modelos lineares","heading":"7.4 Regressão Linear Simples","text":"regressão linear simples é usada para analisar relação entre uma variável preditora (plotada eixo-X) e uma variável resposta (plotada eixo-Y). duas variáveis devem ser contínuas. Diferente das correlações, regressão assume uma relação de causa e efeito entre variáveis. O valor da variável preditora (X) causa, direta ou indiretamente, o valor da variável resposta (Y). Assim, Y é uma função linear de X:\\[Y = \\beta_0 + \\beta_{1}X_i + \\epsilon_i\\]Onde:\\(\\beta_0\\) = intercepto (intercept) que representa o valor da função quando X = 0\\(\\beta_{1}\\) = inclinação (slope) que mede mudança na variável Y para cada mudança de unidade da variável X\\(\\epsilon_{1}\\) = erro aleatório referente à variável Y que não pode ser explicado pela variável XPremissas da Regressão Linear SimplesAs amostras devem ser independentesAs unidades amostrais são selecionadas aleatoriamenteDistribuição normal (gaussiana) dos resíduosHomogeneidade da variância dos resíduosExemplo prático 1 - Regressão Linear SimplesExplicação dos dadosNeste exemplo, avaliaremos relação entre o gradiente de temperatura média anual (°C) e o tamanho médio comprimento rostro-cloacal (CRC em mm) de populações de Dendropsophus minutus (Anura:Hylidae) amostradas em 109 localidades Brasil (Boaratti Da Silva 2015).PerguntaA temperatura afeta o tamanho CRC de populações de Dendropsophus minutus?PrediçõesOs CRC das populações serão menores em localidades mais quentes que em localidades mais frias de acordo com Hipótese Balanço de Calor (Olalla-Tárraga Rodríguez 2007)VariáveisData frame com populações (unidade amostral) nas linhas e CRC (variável resposta) médio (mm) e temperatura média anual (variável preditora) como colunasChecklistVerificar se o seu data frame está com unidades amostrais nas linhas e variáveis preditoras e respostas nas colunasAnáliseOlhando os dados com função head().Vamos calcular regressão linear simples.Antes de vermos os resultados, vamos verificar normalidade e homogeneidade das variâncias (Figura 7.9).\nFigura 7.9: Gráficos mostrando premissas da regressão linear simples.\nOs gráficos Residuals vs Fitted, Scale-Location, e Residual vs Leverage estão relacionados com homogeneidade da variância. Nestes gráficos, esperamos ver os pontos dispersos espaço sem padrões com formatos em U ou funil. Neste caso, vemos que linhas vermelhas (que indicam tendência dos dados) estão praticamente retas, seguindo linha pontilhada, sugerindo que não exista heterogeneidade de variância dos resíduos. O gráfico Residual vs Leverage, identifica os valores extremos que estejam mais de uma unidade da distância de Cook (linha pontilhada vermelha). Quando muito discrepantes, esses valores podem influenciar os resultados dos testes estatísticos. Também não temos problemas com esse pressuposto modelo aqui. O gráfico Normal Q-Q (quantile-quantile plot) mede desvios da normalidade. Neste caso, esperamos que os pontos sigam uma linha reta (.e. fiquem muito próximos da linha pontilhada) e não apresentem padrões com formatos de S ou arco. Podemos observar que tanto normalidade como homogeneidade resíduos estão dentro dos padrões esperados.Vamos ver os resultados da regressão simples usando funções anova() e summary(). função anova() retorna uma tabela contendo o grau de liberdade (df), soma dos quadrados, valor de F e o valor de P.função summary() retorna uma tabela contendo o valor intercepto, inclinação da reta (slope) e o coeficiente de determinação (R2) que indica proporção da variação na variável Y que pode ser atribuída à variação na variável X. Percebam que parte final dos resultados apresentados summary() são os mesmo apresentados pela função anova().Vamos visualizar os resultados em gráfico (Figura 7.10).\nFigura 7.10: Gráfico mostrando relação entre variáveis e modelo linear simples, representado pela linha contínua.\nInterpretação dos resultadosNeste exemplo, rejeitamos hipótese nula de que não existe relação entre o tamanho CRC das populações de D. minutus e temperatura da localidade onde elas ocorrem (F1,107 = 38,92, P < 0,001). Os resultados mostram que o tamanho CRC das populações tem uma relação positiva com temperatura das localidades. Assim, populações de D. minutus em localidades mais quentes apresentam maior CRC que populações em localidades mais frias. Podemos também usar os coeficientes da regressão para entender como mudança na variável preditora (temperatura) afeta o tamanho corporal médio dos anuros. Neste caso, ao usar o comando coef(modelo_regressao), obtemos os valores 16,23 e 0,27, respectivamente os valores intercepto (β0) e da temperatura (β1). O valor de 0,27 indica que mudança de uma unidade na variável preditora (neste caso, graus), aumenta em 0,27 unidades (neste caso, centímetros) da variável dependente. Por exemplo, o modelo indica que o tamanho médio dos indivíduos em locais com temperatura de 16º C é de 20,55 cm, ao passo que em locais com 26º C o tamanho aumenta para 23,25 cm, o que representa um ganho de 13%.","code":"\n## Cabeçalho dos dados\nhead(dados_regressao) \n#>      Municipio      CRC Temperatura Precipitacao\n#> 1     Acorizal 22.98816    24.13000       1228.2\n#> 2  Alpinopolis 22.91788    20.09417       1487.6\n#> 3 Alto_Paraiso 21.97629    21.86167       1812.4\n#> 4    Americana 23.32453    20.28333       1266.2\n#> 5      Apiacas 22.83651    25.47333       2154.0\n#> 6  Arianopolis 20.86989    20.12167       1269.2\n## regressão simples\nmodelo_regressao <- lm(CRC ~ Temperatura, data = dados_regressao)\n## Verificar as premissas do teste\npar(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\nplot(modelo_regressao)\ndev.off() # volta a configuração dos gráficos para o formato padrão \n#> null device \n#>           1\n## Resultados usando a função anova\nanova(modelo_regressao)\n#> Analysis of Variance Table\n#> \n#> Response: CRC\n#>              Df  Sum Sq Mean Sq F value    Pr(>F)    \n#> Temperatura   1  80.931  80.931   38.92 9.011e-09 ***\n#> Residuals   107 222.500   2.079                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Resultados usando a função summary\nsummary(modelo_regressao)\n#> \n#> Call:\n#> lm(formula = CRC ~ Temperatura, data = dados_regressao)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.4535 -0.7784  0.0888  0.9168  3.1868 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 16.23467    0.91368  17.768  < 2e-16 ***\n#> Temperatura  0.26905    0.04313   6.239 9.01e-09 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.442 on 107 degrees of freedom\n#> Multiple R-squared:  0.2667, Adjusted R-squared:  0.2599 \n#> F-statistic: 38.92 on 1 and 107 DF,  p-value: 9.011e-09\n## Gráfico\nggplot(data = dados_regressao, aes(x = Temperatura, y = CRC)) + \n    labs(x = \"Temperatura média anual (°C)\", \n         y = \"Comprimento rostro-cloacal (mm)\") +\n    geom_point(size = 4, shape = 21, fill = \"darkorange\", alpha = 0.7) +\n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    tema_livro() +\n    theme(legend.position = \"none\")"},{"path":"cap7.html","id":"regressão-linear-múltipla","chapter":"Capítulo 7 Modelos lineares","heading":"7.5 Regressão Linear Múltipla","text":"regressão linear múltipla é uma extensão da regressão linear simples. Ela é usada quando queremos determinar o valor da variável resposta (Y) com base nos valores de duas ou mais variáveis preditoras (X1, X2, Xn).\\[Y = \\beta_0 + \\beta_{1}X_1 + \\beta_{n}X_n + \\epsilon_i\\]Onde:\\(\\beta_0\\) = intercepto (intercept) que representa o valor da função quando X = 0\\(\\beta_{n}\\) = inclinação (slope) que mede mudança na variável Y para cada mudança de unidade das variáveis Xn\\(\\epsilon_{1}\\) = erro aleatório referente variável Y que não pode ser explicado pelas variáveis preditorasPremissas da Regressão Linear MúltiplaAs amostras devem ser independentesAs unidades amostrais são selecionadas aleatoriamenteDistribuição normal (gaussiana) dos resíduosHomogeneidade da variância dos resíduosExemplo prático 1 - Regressão Linear MúltiplaExplicação dos dadosUtilizaremos o mesmo exemplo da regressão linear simples. Contudo, além gradiente de temperatura média anual (°C), incluiremos o gradiente de precipitação anual (mm) como outra variável preditora tamanho médio comprimento rostro-cloacal (CRC em mm) de populações de Dendropsophus minutus (Anura:Hylidae) amostradas em 109 localidades Brasil (Boaratti Da Silva 2015).PerguntaO tamanho CRC das populações de D. minutus é influenciado pela temperatura e precipitação das localidades onde os indivíduos ocorrem?PrediçõesOs CRC das populações serão menores em localidades com clima quente e chuvoso que em localidades com clima frio e secoVariáveisdata frame com populações (unidade amostral) nas linhas e CRC (variável resposta) médio (mm) e temperatura e precipitação (variáveis preditoras) como colunasChecklistVerificar se o seu data frame está com unidades amostrais nas linhas e variáveis preditoras e respostas nas colunasAnáliseOlhando os dados usando função head().Códigos para ajustar o modelo de regressão múltipla. 📝 Importante \nMulticolinearidade ocorre quando variáveis preditoras são correlacionadas. Essa correlação é um problema porque variáveis preditoras deveriam ser independentes. Além disso, multicolinearidade aumentam o erro padrão associado aos coeficientes produzindo resultados menos confiáveis. O Fator de Inflação da Variância (VIF) é um teste que quantifica quanto erro padrão dos coeficientes estimados estão inflados devido à multicolinearidade. Na regressão múltipla, cada variável preditora tem um valor de VIF. Alguns autores consideram valores de VIF acima de 10 como fortemente correlacionadas, outros mais conservadores consideram o valor de 5, 3 ou até mesmo 2. Mais detalhes em . F. Zuur, Ieno, Elphick (2010) e Dormann et al. (2013).Vamos analisar se variáveis apresentam multicolinearidade.Os valores são menores que 3, indicando que não há multicolinearidade.Agora vamos verificar premissas de normalidade e homogeneidade das variâncias (Figura 7.11).\nFigura 7.11: Gráficos mostrando premissas da regressão linear múltipla.\nOs resíduos apresentam distribuição normal e variâncias homogêneas.Podemos ver os resultados da análise.Percebam que temperatura tem uma relação significativa e positiva com o tamanho CRC das populações (P < 0.001), enquanto precipitação não apresenta relação com o CRC (P = 0.27). Neste caso, é interessante saber se um modelo mais simples (e.g. contendo apenas temperatura) explicaria distribuição tão bem ou melhor que este modelo mais complexo considerando duas variáveis (temperatura e precipitação).Para isso, podemos utilizar Likelihood-ratio test (LRT) para comparar os modelos. LRT compara dois modelos aninhados, testando se os parâmetros modelo mais complexo diferem significativamente modelo mais simples. Em outras palavras, ele testa se há necessidade de se incluir uma variável extra modelo para explicar os dados. 📝 Importante \nHipótese Nula (H0) teste Likelihood-ratio test (LRT) é de que o modelo mais simples é o melhor.Valor de p < 0.05 rejeita hipótese nula e o modelo mais complexo é o melhorValor de p > 0.05 não rejeita hipótese nula e o modelo mais simples é o melhorInterpretação dos resultadosNeste exemplo, precipitação não está associada com variação tamanho CRC das populações de D. minutus. Por outro lado, temperatura explicou 26% da variação tamanho CRC das populações.","code":"\n## Cabeçalho dos dados\nhead(dados_regressao_mul) \n#>      Municipio      CRC Temperatura Precipitacao\n#> 1     Acorizal 22.98816    24.13000       1228.2\n#> 2  Alpinopolis 22.91788    20.09417       1487.6\n#> 3 Alto_Paraiso 21.97629    21.86167       1812.4\n#> 4    Americana 23.32453    20.28333       1266.2\n#> 5      Apiacas 22.83651    25.47333       2154.0\n#> 6  Arianopolis 20.86989    20.12167       1269.2\n## Regressão múltipla\nmodelo_regressao_mul <- lm(CRC ~ Temperatura + Precipitacao,\n                           data = dados_regressao_mul)\n# Multicolinearidade\nvif(modelo_regressao_mul)\n#>  Temperatura Precipitacao \n#>     1.041265     1.041265\n## Normalidade e homogeneidade das variâncias\nplot_grid(plot_model(modelo_regressao_mul , type = \"diag\"))\n## regressão múltipla\nsummary(modelo_regressao_mul)\n#> \n#> Call:\n#> lm(formula = CRC ~ Temperatura + Precipitacao, data = dados_regressao_mul)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.4351 -0.8026  0.0140  0.9420  3.4300 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  16.7162571  1.0108674  16.537  < 2e-16 ***\n#> Temperatura   0.2787445  0.0439601   6.341 5.71e-09 ***\n#> Precipitacao -0.0004270  0.0003852  -1.108     0.27    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.44 on 106 degrees of freedom\n#> Multiple R-squared:  0.2751, Adjusted R-squared:  0.2614 \n#> F-statistic: 20.12 on 2 and 106 DF,  p-value: 3.927e-08\n## Criando os modelos aninhados\nmodelo_regressao_mul <- lm(CRC ~ Temperatura + Precipitacao, \n                           data = dados_regressao_mul)\nmodelo_regressao <- lm(CRC ~ Temperatura, data = dados_regressao_mul)\n\n## Likelihood-ratio test (LRT)\nlrtest(modelo_regressao_mul, modelo_regressao)\n#> Likelihood ratio test\n#> \n#> Model 1: CRC ~ Temperatura + Precipitacao\n#> Model 2: CRC ~ Temperatura\n#>   #Df  LogLik Df  Chisq Pr(>Chisq)\n#> 1   4 -192.93                     \n#> 2   3 -193.55 -1 1.2558     0.2624\n## Comparando com o modelo somente com o intercepto\n# Criando um modelo sem variáveis, só o intercepto.\nmodelo_intercepto <- lm(CRC ~ 1, data = dados_regressao_mul)\nlrtest(modelo_regressao, modelo_intercepto)\n#> Likelihood ratio test\n#> \n#> Model 1: CRC ~ Temperatura\n#> Model 2: CRC ~ 1\n#>   #Df  LogLik Df  Chisq Pr(>Chisq)    \n#> 1   3 -193.55                         \n#> 2   2 -210.46 -1 33.815  6.061e-09 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"cap7.html","id":"análises-de-variância-anova","chapter":"Capítulo 7 Modelos lineares","heading":"7.6 Análises de Variância (ANOVA)","text":"ANOVA refere-se uma variedade de delineamentos experimentais nos quais variável preditora é categórica e variável resposta é contínua (Gotelli Ellison 2012). Exemplos desses delineamentos experimentais são: ANOVA de um fator, ANOVA de dois fatores, ANOVA em blocos aleatorizados, ANOVA de medidas repetidas e ANOVA split-splot. De forma geral, ANOVA é um teste estatístico usado para comparar média entre grupos amostrados independentemente. Para isso, o teste leva em conta, além das médias dos grupos, variação dos dados dentro e entre os grupos. Neste capítulo, iremos demonstrar os códigos para alguns dos principais delineamentos experimentais.Premissas da ANOVAAs amostras devem ser independentesAs unidades amostrais são selecionadas aleatoriamenteDistribuição normal (gaussiana) dos resíduosHomogeneidade da variância 📝 Importante \nANOVA de medidas repetidas e ANOVA split-plot são análises com desenhos experimentais que apresentam dependência entre amostras, mas controlam essa dependência nas suas formulações matemáticas.","code":""},{"path":"cap7.html","id":"anova-de-um-fator","chapter":"Capítulo 7 Modelos lineares","heading":"7.6.1 ANOVA de um fator","text":"Este teste considera delineamentos experimentais com apenas um fator (ou tratamento) que pode ser composto por três ou mais grupos (ou níveis).Exemplo prático 1 - Anova de um fatorExplicação dos dadosNeste exemplo hipotético, avaliaremos se o adubo X-2020 disponibilizado recentemente mercado melhora o crescimento dos indivíduos de Coffea arabica como divulgado pela empresa responsável pela venda produto. Para isso, foi realizado um experimento com indivíduos de C. arabica cultivados em três grupos: ) grupo controle onde os indivíduos não receberam adubação, ii) grupo onde os indivíduos receberam adição adubo tradicional mais utilizado pelos produtores de C. arabica, e iii) grupo onde os indivíduos receberam adição adubo X-2020.PerguntaO crescimento dos indivíduos de C. arabica é melhorado pela adição adubo X-2020?PrediçõesO crescimento dos indivíduos de C. arabica será maior grupo que recebeu o adubo X-2020VariáveisData frame com plantas (unidade amostral) nas linhas e o crescimento dos indivíduos de C. arabica (variável resposta) e os tratamentos (variável preditora) nas colunasChecklistVerificar se o seu data frame está com unidades amostrais nas linhas e variável preditora e resposta nas colunasAnáliseOlhando os dados e criando o modelo para Anova de um fator.Vamos verificar normalidade dos resíduos e homogeneidade da variância usando os testes de Shapiro-Wilk e Bartlett, respectivamente.Os resíduos apresentam distribuição normal e homogeneidade de variância.Vamos ver os resultados da análise.Percebam que o resultado da ANOVA (Pr(>F) < 0.001) indica que devemos rejeitar hipótese nula que não há diferença entre médias dos grupos. Contudo, os resultados não mostram quais são os grupos que apresentam diferenças. Para isso, temos que realizar testes de comparações múltiplas post-hoc para detectar os grupos que apresentam diferenças significativas entre médias. 📝 Importante \nOs testes post-hoc só devem ser utilizados quando rejeitamos hipótese nula (P < 0.05) teste da ANOVA.Visualizar os resultados em gráfico (Figura 7.12).\nFigura 7.12: Gráfico de caixa mostrando o resultado da ANOVA de um fator.\nInterpretação dos resultadosNeste exemplo, os indivíduos de C. arabica que receberam adubação (tradicional e X-2020) apresentaram maior crescimento que os indivíduos que não receberam adubação. Contudo, diferente que foi divulgado pela empresa, o adubo X-2020 não apresentou melhor desempenho que o adubo tradicional já utilizado pelos produtores.","code":"\n## Cabeçalho dos dados\nhead(dados_anova_simples) \n#>   Crescimento Tratamento\n#> 1       7.190   Controle\n#> 2       6.758   Controle\n#> 3       6.101   Controle\n#> 4       4.758   Controle\n#> 5       6.542   Controle\n#> 6       7.667   Controle\n\n## Análise ANOVA de um fator\nModelo_anova <- aov(Crescimento ~ Tratamento, data = dados_anova_simples) \n## Normalidade \nshapiro.test(residuals(Modelo_anova))\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  residuals(Modelo_anova)\n#> W = 0.94676, p-value = 0.08266\n\n## Homogeneidade da variância\nbartlett.test(Crescimento ~ Tratamento, data = dados_anova_simples)\n#> \n#>  Bartlett test of homogeneity of variances\n#> \n#> data:  Crescimento by Tratamento\n#> Bartlett's K-squared = 0.61835, df = 2, p-value = 0.7341\n## Resultados da anova\nanova(Modelo_anova)\n#> Analysis of Variance Table\n#> \n#> Response: Crescimento\n#>            Df Sum Sq Mean Sq F value    Pr(>F)    \n#> Tratamento  2 340.32 170.160  77.989 3.124e-13 ***\n#> Residuals  33  72.00   2.182                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Diferenças entre os tratamentos\n# Teste de Tuckey's honest significant difference\nTukeyHSD(Modelo_anova)\n#>   Tukey multiple comparisons of means\n#>     95% family-wise confidence level\n#> \n#> Fit: aov(formula = Crescimento ~ Tratamento, data = dados_anova_simples)\n#> \n#> $Tratamento\n#>                                       diff       lwr       upr     p adj\n#> Adubo_X-2020-Adubo_Tradicional  0.04991667 -1.429784  1.529617 0.9962299\n#> Controle-Adubo_Tradicional     -6.49716667 -7.976867 -5.017466 0.0000000\n#> Controle-Adubo_X-2020          -6.54708333 -8.026784 -5.067383 0.0000000\n## Reorganizando a ordem que os grupos irão aparecer no gráfico\ndados_anova_simples$Tratamento <- factor(dados_anova_simples$Tratamento,\n                                         levels = c(\"Controle\", \"Adubo_Tradicional\", \"Adubo_X-2020\"))\n\n## Gráfico\nggplot(data = dados_anova_simples, \n       aes(x = Tratamento, y = Crescimento, color = Tratamento)) + \n    geom_boxplot(fill = c(\"darkorange\", \"darkorchid\", \"cyan4\"), \n                 color = \"black\", show.legend = FALSE, alpha = 0.4) +\n    geom_jitter(shape = 16, position = position_jitter(0.1), \n                cex = 4, alpha = 0.7) +\n    scale_color_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    scale_y_continuous(limits = c(0, 20), breaks = c(0, 5, 10, 15, 20)) +\n    geom_text(x = 1, y = 12, label = \"ab\", color = \"black\", size = 5) +\n    geom_text(x = 2, y = 17, label = \"a\", color = \"black\", size = 5) +\n    geom_text(x = 3, y = 17, label = \"b\", color = \"black\", size = 5) +\n    scale_x_discrete(labels = c(\"Sem adubo\", \"Tradicional\", \"X-2020\")) +\n    labs(x = \"Adubação\", y = \"Crescimento Coffea arabica (cm)\", size = 20) +\n    tema_livro() +\n    theme(legend.position = \"none\") "},{"path":"cap7.html","id":"anova-com-dois-fatores-ou-anova-fatorial","chapter":"Capítulo 7 Modelos lineares","heading":"7.6.2 ANOVA com dois fatores ou ANOVA fatorial","text":"Este teste considera delineamentos amostrais com dois fatores (ou tratamentos) que podem ser compostos por dois ou mais grupos (ou níveis). Esta análise tem uma vantagem, pois permite avaliar o efeito da interação entre os fatores na variável resposta. Quando interação está presente, o impacto de um fator depende nível (ou grupo) outro fator.Exemplo prático 1 - ANOVA com dois fatoresExplicação dos dadosNeste exemplo hipotético, avaliaremos se o tempo que o corpo leva para eliminar uma droga utilizada em exames de ressonância magnética está relacionado com o sistema XY de determinação sexo e/ou com idade dos pacientes. Para isso, foi realizado um experimento com 40 pacientes distribuídos da seguinte maneira: ) 10 indivíduos XX - jovens, ii) 10 indivíduos XX - idosas, iii) 10 indivíduos XY - jovens, e iv) 10 indivíduos XY - idosos.PerguntaO tempo de eliminação da droga é dependente sistema XY de determinação sexo e idade dos pacientes?PrediçõesO tempo de eliminação da droga vai ser mais rápido nos pacientes XX e jovensVariáveisdata frame com os pacientes (unidade amostral) nas linhas e o tempo de eliminação da droga (variável resposta) e os tratamentos sexo e idade dos pacientes (variáveis preditoras) nas colunasChecklistVerificar se o seu data frame está com unidades amostrais nas linhas e variáveis preditoras e respostas nas colunasAnáliseVerificando os dados usando função head().Códigos para realizar ANOVA com dois fatores.Percebam que interação não apresenta um efeito significativo (P > 0.05). Assim, iremos retirar interação e verificar, usando Likelihood-ratio test, se o modelo mais simples é melhor.Analisando o resultado teste (P > 0.05), interação não é importante. Então podemos seguir com o modelo mais simples. Vamos verificar normalidade e homogeneidade da variância (Figura 7.13).\nFigura 7.13: Gráficos mostrando premissas da ANOVA fatorial.\nDois pontos estão fugindo da reta e chamam atenção sobre normalidade da distribuição dos resíduos. homogeneidade da variância está adequada. Por enquanto, vamos seguir análise, mas veja o Capítulo 8 para entender como lidar com modelos que os resíduos não apresentam distribuição normal.Percebam que o resultado da ANOVA (Pr(>F) < 0.001) indica que devemos rejeitar hipótese nula de que não há diferença entre médias sistema XY e idade dos pacientes. Neste caso, não precisamos realizar testes de comparações múltiplas post-hoc porque os fatores apresentam apenas dois níveis. Contudo, se seu delineamento experimental um dos fatores apresentar três ou mais níveis, você deverá utilizar os testes de comparações post-hoc para determinar diferenças entre os grupos.Vamos visualizar os resultados em gráfico (Figura 7.14).\nFigura 7.14: Gráfico de caixa mostrando o resultado da ANOVA fatorial.\nInterpretação dos resultadosNeste exemplo, o sistema XY de determinação sexo e idade dos pacientes têm um efeito tempo de eliminação da droga organismo. Os pacientes XX e jovens apresentaram eliminação mais rápida da droga que pacientes XY e idosos.Exemplo prático 2 - ANOVA com dois fatores com efeito da interaçãoExplicação dos dadosNeste exemplo, novamente hipotético, usaremos os mesmos dados exemplo anterior. Entretanto, alteramos os dados para que agora interação seja significativa.Percebam que interação é significativa (P < 0.05), mas idade não é significativa. Nossa interpretação precisa ser baseada na interação entre os fatores. Vamos visualizar os resultados em gráfico (Figura 7.15).\nFigura 7.15: Gráfico de caixa mostrando o resultado da ANOVA fatorial com interação.\nInterpretação dos resultadosPercebam que linhas se cruzam. Esse é um exemplo clássico de interação. Novamente, para saber resposta fator idade (jovem ou idoso), você precisa saber com qual pessoa (XX ou XY) ele está associado. Jovens são mais rápidos para eliminarem droga em pessoas XX, enquanto os idosos são mais rápidos para eliminarem droga nas pessoas XY.","code":"\n## Cabeçalho dos dados\nhead(dados_dois_fatores) \n#>    Tempo Pessoas Idade\n#> 1 18.952      XX Jovem\n#> 2 16.513      XX Jovem\n#> 3 17.981      XX Jovem\n#> 4 21.371      XX Jovem\n#> 5 14.470      XX Jovem\n#> 6 19.130      XX Jovem\n## Análise Anova de dois fatores \n# A interação entre os fatores é representada por *\nModelo1 <- aov(Tempo ~ Pessoas * Idade, data = dados_dois_fatores) \n\n# Olhando os resultados\nanova(Modelo1)\n#> Analysis of Variance Table\n#> \n#> Response: Tempo\n#>               Df  Sum Sq Mean Sq  F value    Pr(>F)    \n#> Pessoas        1  716.72  716.72 178.8538  1.56e-15 ***\n#> Idade          1 1663.73 1663.73 415.1724 < 2.2e-16 ***\n#> Pessoas:Idade  1    4.77    4.77   1.1903    0.2825    \n#> Residuals     36  144.26    4.01                       \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Criando modelo sem interação.\nModelo2 <- aov(Tempo ~ Pessoas + Idade, data = dados_dois_fatores) \n\n## LRT\nlrtest(Modelo1, Modelo2)\n#> Likelihood ratio test\n#> \n#> Model 1: Tempo ~ Pessoas * Idade\n#> Model 2: Tempo ~ Pessoas + Idade\n#>   #Df  LogLik Df  Chisq Pr(>Chisq)\n#> 1   5 -82.413                     \n#> 2   4 -83.063 -1 1.3012      0.254\n# Verificando as premissas do teste.\nplot_grid(plot_model(Modelo2, type = \"diag\"))\n# Resultados do modelo\nanova(Modelo2)\n#> Analysis of Variance Table\n#> \n#> Response: Tempo\n#>           Df  Sum Sq Mean Sq F value    Pr(>F)    \n#> Pessoas    1  716.72  716.72  177.94 1.041e-15 ***\n#> Idade      1 1663.73 1663.73  413.05 < 2.2e-16 ***\n#> Residuals 37  149.03    4.03                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Gráfico\nggplot(data = dados_dois_fatores_interacao, \n       aes(y = Tempo, x = Pessoas, color = Idade)) + \n    geom_boxplot() +\n    stat_summary(fun = mean, geom =\"point\", aes(group = Idade, x = Pessoas),\n                 color = \"black\",\n                 position = position_dodge(0.7), size  = 4) +\n    geom_link(aes(x = 0.8, y = 31, xend = 1.8, yend = 40), color = \"darkorange\", \n              lwd  = 1.3, linetype = 2) + \n    geom_link(aes(x = 1.2, y = 19, xend = 2.2, yend = 26.5), \n              color = \"cyan4\", lwd  = 1.3, linetype = 2) + \n    labs(x = \"Sistema XY de determinação do sexo\", \n         y = \"Tempo (horas) para eliminar a droga\") +\n    scale_color_manual(values = c(\"darkorange\", \"cyan4\", \"darkorange\", \"cyan4\")) +\n    scale_y_continuous(limits = c(10, 50), breaks = c(10, 20, 30, 40, 50)) +\n    tema_livro()  \n## Olhando os dados\nhead(dados_dois_fatores_interacao2)\n#>    Tempo Pessoas Idade\n#> 1 18.952      XX Jovem\n#> 2 16.513      XX Jovem\n#> 3 17.981      XX Jovem\n#> 4 21.371      XX Jovem\n#> 5 14.470      XX Jovem\n#> 6 19.130      XX Jovem\n\n## Análise anova de dois fatores \nModelo_interacao2 <- aov(Tempo ~ Pessoas * Idade, \n                         data = dados_dois_fatores_interacao2)\n\n## Olhando os resultados\nanova(Modelo_interacao2)\n#> Analysis of Variance Table\n#> \n#> Response: Tempo\n#>               Df  Sum Sq Mean Sq  F value    Pr(>F)    \n#> Pessoas        1  716.72  716.72 178.8538  1.56e-15 ***\n#> Idade          1    4.77    4.77   1.1903    0.2825    \n#> Pessoas:Idade  1 1663.73 1663.73 415.1724 < 2.2e-16 ***\n#> Residuals     36  144.26    4.01                       \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Gráfico\nggplot(data = dados_dois_fatores_interacao2, \n       aes(y = Tempo, x = Pessoas, color = Idade)) + \n    geom_boxplot() +\n    stat_summary(fun = mean, geom =\"point\", aes(group = Idade, x = Pessoas), \n                 color = \"black\", position = position_dodge(0.7), size  = 4) +\n    geom_link(aes(x = 0.8, y = 31, xend = 1.8, yend = 27), color = \"darkorange\", \n              lwd  = 1.3, linetype = 2) + \n    geom_link(aes(x = 1.2, y = 19, xend = 2.2, yend = 41), color = \"cyan4\", \n              lwd  = 1.3, linetype = 2) + \n    labs(x = \"Sistema XY de determinação do sexo\", \n         y = \"Tempo (horas) para eliminar a droga\") +\n    scale_color_manual(values = c(\"darkorange\", \"cyan4\", \"darkorange\", \"cyan4\")) +\n    scale_y_continuous(limits = c(10, 50), breaks = c(10, 20, 30, 40, 50)) +\n    tema_livro() "},{"path":"cap7.html","id":"anova-em-blocos-aleatorizados","chapter":"Capítulo 7 Modelos lineares","heading":"7.6.3 ANOVA em blocos aleatorizados","text":"delineamento experimental com blocos aleatorizados, cada fator é agrupado em blocos, com réplicas de cada nível fator representado em cada bloco (Gotelli Ellison 2012). O bloco é uma área ou período de tempo dentro qual condições ambientais são relativamente homogêneas. O objetivo uso dos blocos é controlar fontes de variações indesejadas na variável dependente que não são de interesse pesquisador. Desta maneira, podemos retirar dos resíduos os efeitos das variações indesejadas que não são nosso interesse, e testar com maior poder estatístico os efeitos dos tratamentos de interesse. 📝 Importante \nOs blocos devem ser arranjados de forma que condições ambientais sejam mais similares dentro dos blocos que entre os blocos.Exemplo prático 1 - ANOVA em blocos aleatorizadosExplicação dos dadosNeste exemplo, avaliaremos riqueza de espécies de anuros amostradas em poças artificiais instaladas diferentes distâncias de seis fragmentos florestais sudeste Brasil (da Silva et al. 2012). Os fragmentos florestais apresentam diferenças entre si que não são interesse pesquisador. Por isso, eles foram incluídos como blocos nas análises. poças artificiais foram instaladas em todos os fragmentos florestais com base seguinte delineamento experimental (da Silva et al. 2012): ) quatro poças interior fragmento 100 m de distância da borda fragmento; ii) quatro poças interior fragmento 50 m de distância da borda fragmento; iii) quatro poças na borda fragmento; iv) quatro poças na matriz de pastagem 50 m de distância da borda fragmento; e v) quatro poças na matriz de pastagem 100 m de distância da borda fragmento. Percebam que todos os tratamentos foram instalados em todos os blocos. 📝 Importante \nOs valores da riqueza de espécies foram alterados em relação publicação original (da Silva et al. 2012) para deixar o exemplo mais didático.PerguntaA distância da poça artificial ao fragmento florestal influencia riqueza de espécies anuros?PrediçõesPoças na borda fragmento florestal apresentarão maior riqueza de espécies que poças distantes da bordaVariáveisData frame com poças (unidade amostral) nas linhas e riqueza de espécies (variável reposta), distância dos fragmentos florestais (variável preditora categórica) e fragmentos florestais (blocos) nas colunasChecklistVerificar se o seu data frame está com unidades amostrais nas linhas e variáveis preditoras e respostas nas colunasAnáliseOlhando os dados usando função head().Análise da ANOVA em blocos.Lembre-se que nos delineamentos experimentais em bloco, o pesquisador não está interessado efeito bloco, mas sim em controlar variação associada ele.O que não pode acontecer é ignorar o efeito bloco que é incorporado pelos resíduos quando não informado modelo. Veja abaixo forma errada de analisar delineamento experimental com blocos.O resultado da ANOVA (Pr(>F) < 0.001) indica que devemos rejeitar hipótese nula que não há diferença entre médias dos grupos. Contudo, os resultados não mostram quais são os grupos que apresentam diferenças. Para isso, temos que realizar testes de comparações múltiplas post-hoc para detectar os grupos que apresentam diferenças significativas entre médias.Visualizar os resultados em gráfico (Figura 7.16).\nFigura 7.16: Gráfico de caixa mostrando o resultado da ANOVA de blocos aleatorizados.\nInterpretação dos resultadosNeste exemplo, rejeitamos hipótese nula de que distância das poças artificiais até bordas dos fragmentos florestais não influência riqueza de espécies de anuros. poças artificiais instaladas nas bordas dos fragmentos florestais apresentaram maior riqueza de espécies que poças distantes.","code":"\n## Cabeçalho dos dados\nhead(dados_bloco) \n#>   Riqueza Blocos    Pocas\n#> 1      90      A  Int-50m\n#> 2      95      A Int-100m\n#> 3     107      A    Borda\n#> 4      92      A  Mat-50m\n#> 5      89      A Mat-100m\n#> 6      92      B  Int-50m\n## Análise Anova em blocos aleatorizados\nmodel_bloco <- aov(Riqueza ~ Pocas + Error(Blocos), data = dados_bloco)\nsummary(model_bloco)\n#> \n#> Error: Blocos\n#>           Df Sum Sq Mean Sq F value Pr(>F)\n#> Residuals  5   1089   217.8               \n#> \n#> Error: Within\n#>           Df Sum Sq Mean Sq F value Pr(>F)  \n#> Pocas      4   1504   376.1   2.907 0.0478 *\n#> Residuals 20   2588   129.4                 \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Forma errada de análisar Anova em blocos\nmodelo_errado <- aov(Riqueza ~ Pocas, data = dados_bloco)\nanova(modelo_errado)\n#> Analysis of Variance Table\n#> \n#> Response: Riqueza\n#>           Df Sum Sq Mean Sq F value  Pr(>F)  \n#> Pocas      4 1504.5  376.12  2.5576 0.06359 .\n#> Residuals 25 3676.5  147.06                  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Teste de Tuckey's honest significant difference\npairs(lsmeans(model_bloco, \"Pocas\"), adjust = \"tukey\")\n#>  contrast                estimate   SE df t.ratio p.value\n#>  Borda - (Int-100m)        16.000 6.57 20   2.436  0.1463\n#>  Borda - (Int-50m)         19.833 6.57 20   3.020  0.0472\n#>  Borda - (Mat-100m)        15.833 6.57 20   2.411  0.1531\n#>  Borda - (Mat-50m)          8.167 6.57 20   1.244  0.7269\n#>  (Int-100m) - (Int-50m)     3.833 6.57 20   0.584  0.9760\n#>  (Int-100m) - (Mat-100m)   -0.167 6.57 20  -0.025  1.0000\n#>  (Int-100m) - (Mat-50m)    -7.833 6.57 20  -1.193  0.7553\n#>  (Int-50m) - (Mat-100m)    -4.000 6.57 20  -0.609  0.9720\n#>  (Int-50m) - (Mat-50m)    -11.667 6.57 20  -1.777  0.4135\n#>  (Mat-100m) - (Mat-50m)    -7.667 6.57 20  -1.167  0.7692\n#> \n#> P value adjustment: tukey method for comparing a family of 5 estimates\n# Reordenando a ordem que os grupos irão aparecer no gráfico.\ndados_bloco$Pocas <- factor(dados_bloco$Pocas, \n                            levels = c(\"Int-100m\", \"Int-50m\", \"Borda\", \"Mat-50m\", \"Mat-100m\"))\n\n## Gráfico\nggplot(data = dados_bloco, aes(x = Pocas, y = Riqueza)) + \n    labs(x = \"Poças artificiais\", y = \"Riqueza de espécies de anuros\") +\n    geom_boxplot(color = \"black\", show.legend = FALSE, alpha = 0.4) +\n    geom_jitter(shape = 16, position = position_jitter(0.1), cex = 4, alpha = 0.7) +\n    scale_x_discrete(labels = c(\"-100m\",\"-50m\",\"Borda\", \"50m\", \"100m\")) +\n    tema_livro() +\n    theme(legend.position = \"none\") "},{"path":"cap7.html","id":"análise-de-covariância-ancova","chapter":"Capítulo 7 Modelos lineares","heading":"7.6.4 Análise de covariância (ANCOVA)","text":"ANCOVA pode ser compreendida como uma extensão da ANOVA com adição de uma variável contínua (covariável) medida em todas unidades amostrais (Gotelli Ellison 2012). ideia é que covariável também afete os valores da variável resposta. Não incluir covariável irá fazer com que variação não explicada pelo modelo se concentre nos resíduos. Incluindo covariável, o tamanho resíduo é menor e o teste para avaliar diferenças nos tratamentos, que é o interesse pesquisador, terá mais poder estatístico.Exemplo prático 1 - ANCOVAExplicação dos dadosNeste exemplo hipotético, avaliaremos o efeito da herbivoria na biomassa dos frutos de uma espécie de árvore na Mata Atlântica. O delineamento experimental permitiu que alguns indivíduos sofressem herbivoria e outros não. Os pesquisadores também mediram o tamanho da raiz dos indivíduos para inseri-la como uma covariável modelo.PerguntaA herbivoria diminui biomassa dos frutos?PrediçõesOs indivíduos que sofreram herbivoria irão produzir frutos com menor biomassa que os indivíduos sem herbivoriaVariáveisdata frame com os indivíduos da espécie de planta (unidade amostral) nas linhas e biomassa dos frutos (variável resposta), herbivoria (variável preditora categórica) e tamanho da raiz (covariável contínua) nas colunasChecklistVerificar se o seu data frame está com unidades amostrais nas linhas e variáveis preditoras e respostas nas colunasAnáliseOlhando os dados usando função head().Códigos para o cálculo da ANCOVA (Figura 7.17).\nFigura 7.17: Gráficos de diagnóstico dos resíduos da ANCOVA.\npremissas da ANCOVA estão adequadas. Vamos olhar os resultados modelo.Percebam que o resultado da ANCOVA (Pr(>F) < 0.001) indica que tanto herbivoria como o tamanho da raiz (covariável) têm efeitos significativos na biomassa dos frutos. Contudo, interação entre variáveis não foi significativa. Vamos usar o Likelihood-ratio test para ver se podemos seguir com um modelo mais simples (sem interação).interação não é importante, pois P > 0.05. Seguiremos com o modelo mais simples.Vamos fazer visualizar os resultados em gráfico (Figura 7.18).\nFigura 7.18: Gráfico de caixa mostrando o resultado da ANCOVA.\nInterpretação dos resultadosNeste exemplo, o tamanho da raiz (covariável) tem uma relação positiva com biomassa dos frutos. Quanto maior o tamanho da raiz, maior biomassa dos frutos. Usando ANCOVA e controlando o efeito da covariável, percebemos que herbivoria também afeta biomassa dos frutos. Os indivíduos com o mesmo tamanho de raiz que não sofreram herbivoria produziram frutos com maior biomassa que os indivíduos com herbivoria.","code":"\n## Cabeçalho dos dados\nhead(dados_ancova) \n#>    Raiz Biomassa Herbivoria\n#> 1 6.225    59.77   Sem_herb\n#> 2 6.487    60.98   Sem_herb\n#> 3 4.919    14.73   Sem_herb\n#> 4 5.130    19.28   Sem_herb\n#> 5 5.417    34.25   Sem_herb\n#> 6 5.359    35.53   Sem_herb\n## Ancova\nmodelo_ancova <- lm(Biomassa ~ Herbivoria * Raiz, data = dados_ancova)\n\n# Verificando as premissas da Ancova\nplot_grid(plot_model(modelo_ancova, type = \"diag\"))\n## Resultados do modelo\nanova(modelo_ancova)\n#> Analysis of Variance Table\n#> \n#> Response: Biomassa\n#>                 Df  Sum Sq Mean Sq F value    Pr(>F)    \n#> Herbivoria       1  1941.9  1941.9  35.101 8.764e-07 ***\n#> Raiz             1 17434.1 17434.1 315.124 < 2.2e-16 ***\n#> Herbivoria:Raiz  1   136.7   136.7   2.471    0.1247    \n#> Residuals       36  1991.7    55.3                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Criando modelo sem interação\nmodelo_ancova2 <- lm(Biomassa ~ Herbivoria + Raiz, data = dados_ancova)\n\n## Likelihood-ratio test\nlrtest(modelo_ancova, modelo_ancova2)\n#> Likelihood ratio test\n#> \n#> Model 1: Biomassa ~ Herbivoria * Raiz\n#> Model 2: Biomassa ~ Herbivoria + Raiz\n#>   #Df  LogLik Df  Chisq Pr(>Chisq)\n#> 1   5 -134.91                     \n#> 2   4 -136.24 -1 2.6554     0.1032\n## Gráfico\nggplot(data = dados_ancova, aes(x = Raiz, y = Biomassa, fill = Herbivoria)) + \n    labs(x = \"Tamanho da raiz (cm)\", y = \"Biomassa dos frutos (g)\") +\n    geom_point(size = 4, shape = 21, alpha = 0.7) +\n    scale_colour_manual(values = c(\"darkorange\", \"cyan4\")) +\n    scale_fill_manual(values = c(\"darkorange\", \"cyan4\"),\n                      labels = c(\"Com herbivoria\", \"Sem herbivoria\")) +\n    geom_smooth(aes(color = Herbivoria), method = \"lm\", show.legend = FALSE) +\n    tema_livro()"},{"path":"cap7.html","id":"generalized-least-squares-gls","chapter":"Capítulo 7 Modelos lineares","heading":"7.7 Generalized Least Squares (GLS)","text":"Em seu artigo clássico publicado em 1993, Pierre Legendre se pergunta se autocorrelação espacial é um problema ou um novo paradigma (Legendre 1993). Segundo o autor, estudar estruturas espaciais é tanto uma necessidade, quanto um desafio para pesquisadores da ecologia e conservação que lidam com dados espacialmente distribuídos. Uma vez que todas variáveis tipicamente utilizadas em estudos de biodiversidade (populações, condições climáticas, diversidade) possuem algum tipo de estrutura espacial, é fundamental compreender os motivos de como incluir esta informação nos modelos analíticos. De fato, Primeira Lei da Geografia postula que “todas coisas estão relacionadas com todas outras, porém coisas próximas estão mais relacionadas que coisas distantes”. Como resultado, os valores observados em uma localidade (e.g., composição de espécies) serão mais afetados pelo conjunto de espécies que ocorre nas localidades vizinhas e, desse modo, alguns pontos de coleta podem não ser estatisticamente independentes. Como vimos anteriormente, um dos pressupostos dos modelos lineares é independência das unidades amostrais. Assim, presença de autocorrelação espacial nos resíduos viola este pressuposto e, consequentemente, aumenta taxa de Erro Tipo (rejeitar hipótese nula quando ela é verdadeira) nos modelos. Uma das soluções para incorporar dependência espacial dos resíduos é usar o método de Mínimos Quadrados Generalizados (Generalized Least Squares - GLS). Diferente dos modelos apresentados anteriormente, este método ajusta explicitamente modelos heteroscedásticos e com resíduos correlacionados (Pinheiro Bates 2000). Para representar estrutura espacial (e, assim, dependência entre observações) é necessário incluir variáveis espaciais (geralmente coordenadas geográficas, .e., vetores bidimensionais: sensu Pinheiro Bates (2000)) argumento corStruct na função gls() pacote nlme. Este modelo basicamente assume que estrutura de covariância é uma função da distância entre localidades (Littell et al. 2006). É importante ressaltar, todavia, que existem diferentes funções de covariância que são discutidas detalhadamente em Littell et al. (2006). Aqui, iremos nos\nconcentrar nas seguintes funções.Esférica: corSpher(form=\\~lat+long)Exponencial: corExp(form=\\~lat+long)Gaussiana: corGaus(form=\\~lat+long)Linear: corLin(form=\\~lat+long)Razão quadrática: corRatio(form=\\~lat+long)Exemplo prático 1 - GLSExplicação dos dadosNeste exemplo, utilizamos os dados de riqueza de espécies de ácaros (Oribatidae) em 70 amostras de musgo (gênero Sphagnum) (Borcard, Legendre, Drapeau 1992). Para cada amostra, além da riqueza de ácaros, os autores registraram quantidade de água substrato e coordenadas geográficas. Os dados completos estão disponíveis pacote vegan.Modelo linear sem incorporar estrutura espacialVamos inicialmente ajustar um modelo sem incorporar estrutura espacial (Figura 7.19).\nFigura 7.19: Gráficos mostrando premissas da regressão linear simples.\nAcessando informação espacial com o GLSComo dito, dependendo da estrutura espacial de suas variáveis (dependentes, independentes, covariáveis), o pressuposto de independência dos resíduos pode ser afetado e, desse modo, o modelo linear convencional terá maior chance de Erro Tipo . Abaixo, iremos comparar um modelo GLS sem incorporar estrutura espacial (o que é exatamente igual ao modelo criado acima Modelo Linear) com diferentes modelos que utilizam explicitamente resíduos correlacionados.Uma maneira de identificar se os resíduos modelo linear apresentam estrutura espacial é fazendo uma figura chamada variograma. O variograma possui três parâmetros: ) nugget, ii) range e iii) sill (Fortin Dale 2005). O nugget é utilizado para quantificar variabilidade observada nos valores menores (ou seja, em pequenas distâncias). O range, por sua vez, é usado para identificar distância máxima em que autocorrelação espacial está presente (Figura 7.20, pontos laranjas). Deste modo, os valores posicionados partir range (Figura 7.20, pontos verdes) representam pontos não correlacionados (Fortin Dale 2005). posição limiar que representa claramente “pausa” crescimento da curva (range) indica os pontos não correlacionados e representa o sill (Figura 7.20). exemplo da Figura 7.20, o sill é constante. Porém, é possível que os valores de sill não sejam constantes (Chiles Delfiner 1999). Um exemplo é o “efeito buraco” que é caracterizado por um ou mais picos (ou vales) variograma que correspondem ao número de valores negativos na covariância. Esses valores sugerem que valores altos podem estar rodeados de valores baixos (Chiles Delfiner 1999). Porém, os detalhamentos desses comportamentos vão além escopo deste livro.\nFigura 7.20: Variograma representando semi-variância y(h) em função intervalor espacial h. Cada ponto representa distância entre localidades e linha variação teórica função de covariância esférica (veja abaixo). Adaptado de: Fortin & Dale (2005).\nAbaixo, podemos analisar o variograma para os resíduos modelo GLS ajustado (Figura 7.21).\nFigura 7.21: Variograma para os resíduos modelo GLS ajustado.\nO primeiro passo é utilizar diferentes variogramas teóricos para identificar o melhor modelo que representa estrutura espacial dos seus dados.Agora, depois de ajustar o modelo GLS com os diferentes variogramas é necessário comparar os modelos para escolher o mais “provável”, utilizando seleção de modelos pelo Critério de Seleção de Akaike (AIC) (Chiles Delfiner 1999; K. . Burnham Anderson 2002; Fortin Dale 2005) (Figura 7.22). AIC é um método estatístico que compara os modelos criados na sua pesquisa e seleciona o melhor entre eles. Diferente LRT que compara apenas modelos aninhados, o AIC compara modelos com diferentes combinações de variáveis preditoras. Uma vez que fórmula AIC penaliza parâmetros extras modelo, se dois modelos explicam igualmente variação dos dados, o modelo com menos parâmetros terá o menor valor de AIC e será selecionado como o melhor modelo. 📝 Importante \nO valor de AIC por si só não tem significado. Ele precisa ser comparado com outro modelo. Contudo, um modelo só será considerado superior outro quando diferença entre os seus valores de AIC (.e, delta) forem maiores que 2. (K. . Burnham Anderson 2002)\nFigura 7.22: Gráfico dos resíduos em relação aos dados ajustados modelo GLS Ratio.\nO variograma ajustado pelo modelo razão quadrática (ratio_model) demonstra que o range não é crescente (indicando correlação espacial entre localidades próximas) e, desse modo, sugere que é mais apropriado usar o modelo GLS que um modelo linear desconsiderando estrutura espacial (Figura 7.23).\nFigura 7.23: Variogramas para os resíduos modelo GLS Normal e Ratio.\nInterpretação dos resultadosDessa forma, o valor mais apropriado da estatística t é de -4.33 (ratio_model) ao invés de -7.78 (no_spat_gls). Neste caso, decisão (quantidade de água afetando riqueza de ácaros) não foi afetada (P < 0.05 nos dois modelos), somente estatística teste.","code":"\n## Calcular a riqueza de espécies em cada comunidade\nriqueza <- specnumber(mite) \n\n## Selecionar a variável ambiental - quantidade de água no substrato\nagua <- mite.env[,2]\n\n## Criar um data.frame com riqueza, quantidade de água no substrato e coordenadas geográficas\nmite_dat <- data.frame(riqueza, agua, coords)\n## Modelo\nlinear_model <- lm(riqueza ~ agua, mite_dat) \n\n## Resíduos\npar(mfrow = c(2, 2)) \nplot(linear_model, which = 1:4)\n\n## Resultados do modelo\nres_lm <- summary(linear_model)\n\n## Coeficiente de determinação e coeficientes\nres_lm$adj.r.squared\n#> [1] 0.4632024\nres_lm$coefficients\n#>               Estimate  Std. Error  t value     Pr(>|t|)\n#> (Intercept) 24.3040255 1.249094710 19.45731 1.687907e-29\n#> agua        -0.0223793 0.002876239 -7.78075 5.481035e-11\n## Modelo gls sem estrutura espacial\nno_spat_gls <- gls(riqueza ~ agua, mite_dat, method = \"REML\")\n## Variograma\nvariog_mod1 <- nlme::Variogram(no_spat_gls, form = ~lat+long, \n                               resType = \"normalized\") \n\n## Gráfico\nplot(variog_mod1)\n\n## Índice I de Moran\n\n## Primeiro precisamos calcular uma matriz de distâncias geográficas entre as comunidades\ndat_dist <- pdist(coords) # matriz de distância\n\nMoran.I(x = mite_dat$riqueza, w = dat_dist)\n#> $observed\n#> [1] -0.2398147\n#> \n#> $expected\n#> [1] -0.01449275\n#> \n#> $sd\n#> [1] 0.009971923\n#> \n#> $p.value\n#> [1] 4.783614e-113\n## Covariância esférica\nespher_model <- gls(riqueza ~ agua, mite_dat, \n                    corSpher(form = ~lat+long, nugget = TRUE))\n\n## Covariância exponencial\nexpon_model <- gls(riqueza ~ agua, mite_dat, \n                   corExp(form = ~lat+long, nugget = TRUE))\n\n## Covariância Gaussiana\ngauss_model <- gls(riqueza ~ agua, mite_dat, \n                   corGaus(form = ~lat+long, nugget = TRUE))\n\n## Covariância linear\ncor_linear_model <- gls(riqueza ~ agua, mite_dat, \n                        corLin(form = ~lat+long, nugget = TRUE))\n\n## Covariância razão quadrática\nratio_model <- gls(riqueza ~ agua, mite_dat, \n                   corRatio(form = ~lat+long, nugget = TRUE))\n## Seleção de modelos\naic_fit <- AIC(no_spat_gls, espher_model, expon_model, gauss_model, cor_linear_model, ratio_model)\naic_fit %>% arrange(AIC)\n#>                  df      AIC\n#> ratio_model       5 373.1186\n#> expon_model       5 373.2439\n#> no_spat_gls       3 383.8616\n#> espher_model      5 387.8616\n#> gauss_model       5 387.8616\n#> cor_linear_model  5 387.8616\n\n## Gráfico\nplot(residuals(ratio_model, type = \"normalized\") ~ fitted(ratio_model))\n## Varigrama\nratio_variog <- Variogram(ratio_model, form = ~lat+long, resType = \"normalized\")\n\n## Resumo dos modelos\nsummary(ratio_model)$tTable \n#>                  Value   Std.Error   t-value      p-value\n#> (Intercept) 22.1612850 2.444329887  9.066405 2.561826e-13\n#> agua        -0.0151894 0.003505891 -4.332536 4.976757e-05\nsummary(no_spat_gls)$tTable\n#>                  Value   Std.Error  t-value      p-value\n#> (Intercept) 24.3040255 1.249094710 19.45731 1.687907e-29\n#> agua        -0.0223793 0.002876239 -7.78075 5.481035e-11\n\n## Gráficos\nplot(ratio_variog, main = \"Variograma como Modelo Ratio\")\nplot(variog_mod1, main = \"Variograma Modelo Normal\")"},{"path":"cap7.html","id":"para-se-aprofundar-3","chapter":"Capítulo 7 Modelos lineares","heading":"7.8 Para se aprofundar","text":"Listamos seguir livros e links que recomendamos para seguir com sua aprendizagem em modelos lineares","code":""},{"path":"cap7.html","id":"livros-3","chapter":"Capítulo 7 Modelos lineares","heading":"7.8.1 Livros","text":"Recomendamos aos interessados() os livros: ) Zar (2010) Biostatiscal analysis, ii) Gotelli & Ellison (2012) primer ecological statistics, iii) Quinn & Keough (2002) Experimental design data analysis biologists, iv) Zuur e colabodores (2007) Analysing Ecological Data e v) Touchon (2021) Applied statistics R: practical guide life sciences.","code":""},{"path":"cap7.html","id":"links-3","chapter":"Capítulo 7 Modelos lineares","heading":"7.8.2 Links","text":"Recomendamos página Curso de Estatística para Pesquisa Científica[http://estat.bio.br/] pesquisador Thiago Rangel, Universidade Federal de Goiás. Nesta página, vocês irão encontrar exemplos e explicações detalhadas sobre análises e filosofia estatística.","code":""},{"path":"cap7.html","id":"exercícios-3","chapter":"Capítulo 7 Modelos lineares","heading":"7.9 Exercícios","text":"7.1\nAvalie se os indivíduos machos de uma espécie de aranha são maiores que fêmeas. Qual sua interpretação sobre o dimorfismo sexual nesta espécie? Faça um gráfico boxplot usando também função geom_jitter(). Use os dados Cap7_exercicio1 disponível pacote ecodados.7.2\nAvalie se o número de polinizadores visitando uma determinada espécie de planta é dependente da presença ou ausência de predadores. mesma planta, em tempos diferentes, foi utilizada como unidade amostral para os tratamentos com e sem predadores. Qual sua interpretação sobre os resultados? Faça um gráfico boxplot ligando os resultados da mesma planta com e sem presença predador. Use os dados Cap7_exercicio2 disponível pacote ecodados.7.3\nAvalie se existe correlação entre o número de filhotes nos ninhos de uma espécie de ave com o tamanho fragmento florestal. Qual sua interpretação dos resultados? Faça um gráfico mostrando relação entre variáveis. Use os dados Cap7_exercicio3 disponível pacote ecodados.7.4\nAvalie se relação entre o tamanho da área de diferentes ilhas e riqueza de espécies de lagartos. Qual sua interpretação dos resultados? Faça um gráfico mostrando relação predita pelo modelo. Use os dados Cap7_exercicio4 disponível pacote ecodados.7.5\nAvalie se existe relação entre abundância de uma espécie de roedor com o tamanho da área dos fragmentos florestais e/ou altitude. Faça uma regressão múltipla. Em seguida, crie diferentes modelos e selecione o mais parcimonioso com base nos valores teste de Likelihood-ratio test (LRT) e Akaike information criterion (AIC). Qual sua interpretação? Use os dados Cap7_exercicio5 disponível pacote ecodados.7.6\nAvalie se o local que machos territoriais ocupam (pasto, cana, floresta) influência peso dos indivíduos. Qual sua interpretação dos resultados? Faça um gráfico com os resultados. Use os dados Cap7_exercicio6 disponível pacote ecodados.7.7\nAvalie se abundância de formigas está relacionada com o fato das domácias estarem abertas ou fechadas e com idade das domácias. Verifique interação entre os fatores. Qual sua interpretação dos resultados? Faça um gráfico com os resultados. Use os dados Cap7_exercicio7 disponível pacote ecodados.7.8\nAvalie se o número de parasitas está relacionado com o tamanho corporal de fêmeas de uma espécie de ave. Além disso, use idade das aves como uma covariável explicando o número de parasitas. Qual sua interpretação dos resultados? Faça um gráfico com os resultados. Use os dados Cap7_exercicio8 disponível pacote ecodados.7.9\nAvalie se presença ou ausência de predadores afeta riqueza de macroinvertebrados em 10 lagos. Os tratamentos dos predadores foram realizados nos mesmos lagos. Qual sua interpretação dos resultados? Faça um gráfico com os resultados. Use os dados Cap7_exercicio9 disponível pacote ecodados.7.10\nAvalie se precipitação anual afeta riqueza de espécies de anuros em 44 localidades na Mata Atlântica. Use coordenadas geográficas para controlar o efeito da autocorrelação espacial. Qual sua interpretação dos resultados das análises com e sem levar em consideração autocorrelação espacial? Use os dados anuros_ambientais disponível pacote ecodados.Soluções dos exercícios.","code":""},{"path":"cap8.html","id":"cap8","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"Capítulo 8 Modelos Lineares Generalizados","text":"","code":""},{"path":"cap8.html","id":"pré-requisitos-do-capítulo-4","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"Pré-requisitos do capítulo","text":"Pacotes e dados que serão utilizados neste capítulo.","code":"\nlibrary(ecodados)\nlibrary(visdat)\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(RVAideMemoire)\nlibrary(DHARMa)\nlibrary(performance)\nlibrary(MuMIn)\nlibrary(piecewiseSEM)\nlibrary(MASS)\nlibrary(ggExtra)\nlibrary(Rmisc)\nlibrary(emmeans) \nlibrary(sjPlot)\nlibrary(bbmle)\nlibrary(glmmTMB)\nlibrary(ordinal)\nlibrary(car)\nlibrary(ecolottery)\nlibrary(naniar)\nlibrary(vcd)\nlibrary(generalhoslem)\n\n## Dados\nlagartos <- ecodados::lagartos\nparasitas <- ecodados::parasitas\nfish <- ecodados::fish\nfragmentos <- ecodados::fragmentos\nuv_cells <- ecodados::uv_cells"},{"path":"cap8.html","id":"introdução-2","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.1 Introdução","text":"Capítulo 7, descrevemos sobre os modelos lineares (também chamados de Modelos Lineares Gerais) que podem ser descritos pelo mesmo modelo matemático de uma equação da reta tipo:\\[Y = \\beta_0 + \\beta_{1}X_i + \\epsilon_i\\]Nesse tipo de estrutura, o que difere uma regressão linear de uma análise de variância é natureza elemento xi, variável contínua para regressão linear e variável categórica caso da ANOVA (que é codificada numa matriz design para desenhos mais complexos). Nesse sentido, o que todos esses métodos têm em comum é variável resposta Y que é um vetor numérico contínuo. Outro elemento em comum desses métodos é distribuição de frequência erro. Se quiser mais detalhes sobre como modelos lineares podem ser escritos na forma de matrizes, consulte introdução de Fox et al. (2015). Todos os modelos lineares assumem que distribuição erro seja Gaussiana (ou Normal). Isso de certa forma limita o tipo de dado que pode ser usado como variável resposta por estas análises. Por exemplo, dados de contagem (e.g., riqueza e/ou abundância de espécies), frequência (e.g., frequência de ocorrência, porcentagem de cobertura vegetal), incidência (e.g., presença ou ausência de uma espécie) ou proporção (e.g., números de animais infectados cada 1000 animais) não são adequados para serem utilizados como variáveis resposta em modelos lineares. Uma prática comum quando nossos dados não são Normais é transformar por logaritmo ou raiz quadrada. entanto, para dados de contagem isso não é recomendado - veja O’Hara & Kotze (2010), Ives (2015) e Warton (2018) para mais detalhes.Nestes casos, devemos recorrer um conjunto de modelos chamados Modelos Lineares Generalizados (GLM). Nestes modelos, o usuário especifica distribuição de frequência que deseja utilizar para modelar variável resposta. Esta distribuição de frequência deve pertencer à família exponencial, que inclui distribuição de Poisson, Gaussiana, binomial, binomial negativa, Gamma, Bernoulli e Beta. Ainda é possível utilizar Cumulative Link Models para modelar dados ordinais (fatores cuja ordem dos elementos importa, tais como muito baixo, baixo, alto e muito alto). Abaixo vamos ver um pouco sobre como um GLM funciona e exemplos com cada uma destas distribuições.","code":""},{"path":"cap8.html","id":"como-um-glm-funciona","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.2 Como um GLM funciona?","text":"Diferentemente modelo linear, um GLM estima os parâmetros por meio de Máxima Verossimilhança (ML) ao invés dos Mínimos Quadrados Comuns, também chamados de Mínimos Quadrados Ordinários (OLS).Portanto, um GLM relaciona distribuição da variável resposta aos preditores lineares por meio de uma função de ligação. Por exemplo, caso da distribuição de Poisson, usa-se uma ligação logarítmica (também chamada de log link) que garante que o valores ajustados são sempre não negativos. Portanto, um GLM é composto por esses três componentes: ) função de distribuição, ii) preditor linear e iii) função de ligação. função de distribuição é uma hipótese sobre distribuição da variável resposta Yi. Isso também define média e variância de Yi. Já função de ligação define relação entre o valor médio de Yi e da parte sistemática. Esta é também chamada de ligação entre média e parte sistemática modelo. Existem três tipos de função de ligação:Identity link, que é definido por g(µ) = μ, e modela média ou valor esperado de Y. Usado em modelos lineares padrãoIdentity link, que é definido por g(µ) = μ, e modela média ou valor esperado de Y. Usado em modelos lineares padrãoLog link, que é g(μ) = log(μ), e modela o log da média. É usado para dados de contagem (que não podem assumir valores negativos) em modelos log-linearLog link, que é g(μ) = log(μ), e modela o log da média. É usado para dados de contagem (que não podem assumir valores negativos) em modelos log-linearLogit link, que é g(μ) = log[μ/(1-μ)], e é usado para dados binários e regressão logísticaLogit link, que é g(μ) = log[μ/(1-μ)], e é usado para dados binários e regressão logísticaLogo, um modelo linear pode ser visto como um caso particular de um GLM em que utiliza distribuição Gaussiana, com identity link.","code":""},{"path":"cap8.html","id":"como-escolher-a-distribuição-correta-para-seus-dados","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.3 Como escolher a distribuição correta para seus dados?","text":"","code":""},{"path":"cap8.html","id":"para-dados-contínuos","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.3.1 Para dados contínuos","text":"Se Y é uma variável contínua, sua distribuição de probabilidade deve ser normal. Nesses casos distribuições recomendadas são Gaussiana (Normal) ou Gamma. Para essas distribuições, o parâmetro de dispersão é estimado separadamente da média e é às vezes, chamado de nuisance parameter. Uma particularidade da distribuição Gamma é que ela só aceita valores contínuos positivos maiores que zero.Se Y é uma variável de proporção que varia continuamente entre 0 e 1 ou 0 e 100, mas não inclui 0 nem 1, distribuição recomendada é beta. Um exemplo é área (Km^2) de floresta numa imagem de satélite, cuja área total é conhecida. Caso variável inclua 0 e 1 (e.g., 100% da imagem é composta por floresta), deve-se realizar uma transformação nos dados.","code":""},{"path":"cap8.html","id":"para-dados-de-contagem","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.3.2 Para dados de contagem","text":"Se Y é binário (e.g., vivo ou morto), distribuição de probabilidade deve ser binomial.Se Y é uma contagem (e.g., abundância ou riqueza de espécies), então distribuição de probabilidade deve ser Poisson ou binomial negativa. Existem também correções dessas distribuições quando apresentam sobredispersão (overdispersion), tais como quasi-Poisson ou quasi-Negative binomial. Falaremos delas momento certo.Para distribuições tais como binomial e Poisson, variância deve ser igual à média e o parâmetro de dispersão é sempre 1. Na maioria dos dados ecológicos esse pressuposto não é cumprido, veremos estratégias para lidar com isso logo à frente.funções Ord_plot() e goodfit() pacote vcd podem auxiliar na escolha da distribuição para dados de contagem.","code":""},{"path":"cap8.html","id":"dados-de-contagem-a-distribuição-de-poisson","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.4 Dados de contagem: a distribuição de Poisson","text":"Para casos em que estamos interessados em quantificar uma variável discreta, ou seja, uma variável positiva, representada sempre por números inteiros, contendo um número finito de possibilidades, devemos utilizar distribuição de Poisson. Esta distribuição é peculiar por ser descrita apenas por um parâmetro livre (\\(\\lambda\\)). Isso quer dizer que tanto média quanto variância dos dados são descritos por um único parâmetro, o que implica em dizer que média e variância têm de ser iguais.Vamos ver um exemplo com dados reais.Exemplo 1Explicação dos dadosNeste exemplo, iremos utilizar dados de riqueza de anfíbios anuros coletados em 40 poças, açudes e brejos ao redor de fragmentos florestais Noroeste Paulista (Prado Rossa-Feres 2014). Os autores mediram seis variáveis em escala local e outras três em escala de paisagem.PerguntaA distância linear para o corpo d’água mais próximo influencia abundância total de espécies de anuros?PrediçõesCorpos d’água mais conectados permitem que indivíduos dispersem entre eles com maior facilidade, suportando melhor dinâmicas de metapopulações. Portanto, espero que poças que estejam mais conectadas entre si tenham maior riqueza total de anuros.VariáveisVariável reposta: riqueza de anuros em 40 poçasVariável preditora: distância da poça focal para mais próxima na escala da paisagemChecklistVerificar se o seu data frame está com unidades amostrais nas linhas (neste caso poças) e variáveis nas colunasAntes de começar com análise, vamos primeiro explorar os dados.Percebam que o data frame contém 40 colunas. Neste conjunto de dados variáveis preditoras já estão padronizadas com média 0 e desvio padrão 1. variáveis com “2” indicam variáveis quadráticas (podem ser usadas para se testar relações não lineares). Também temos riqueza observada e estimada (Riqueza_HB) e coordenadas geográficas (X e Y). Vamos agora explorar os dados e ver como é relação entre riqueza e distância para poça mais próxima. Sempre é recomendado visualizar os dados antes de efetivamente os modelar para se ter uma ideia da relação entre variáveis (Figura 8.1).\nFigura 8.1: Gráfico de dispersão para explorar variáveis.\nAqui vemos que há de fato uma relação linear positiva entre duas variáveis.partir de agora vamos sempre usar uma mesma estrutura para realizar nossos exercícios de modelagem:Primeiro vamos especificar o modeloDepois realizar diagnosePor último realizar inferência partir nosso modeloModelagemO primeiro argumento da função glm() é uma fórmula, em que na parte esquerda temos variável resposta seguida símbolo ~ (lê-se: modelado em função de) seguido pelas variáveis preditoras. Aqui podemos usar uma ou mais variáveis e testar o seu efeito aditivo (usando o sinal de +) ou interação entre elas (usando o sinal de *). Um bom resumo sobre como especificar o seu modelo pode ser encontrada aqui neste blog. Aqui optamos por um modelo bem simples modelando riqueza de anfíbios apenas em função da distância para o fragmento mais próximo.Assim como modelos lineares que vimos Capítulo 7, GLMs com distribuição de Poisson requerem que se realizem testes de pressupostos de normalidade e homogeneidade de variância dos resíduos, assim como sobredispersão e inflação de zeros.Diagnose básica dos resíduos modeloIremos realizar três diagnoses básicas dos GLMs, avaliando diferentes aspectos modelo:Homogeneidade da variância e normalidade dos resíduosSobredispersão (Overdispersion)Inflação de zeros (Zero-inflation)Vamos começar avaliando homogeneidade da variância e normalidade dos resíduos (Figura 8.2).\nFigura 8.2: Diagnose básica dos resíduos modelo GLM Poisson.\nVemos que linhas vermelhas (que indicam tendência dos dados) estão praticamente retas seguindo linha pontilhada, sugerindo que existe homogeneidade de variância dos resíduos. Vemos também que nos quatro plots alguns dados, 1, 7 e 30 (referem-se às linhas data frame) aparecem identificados, pois apresentam ligeiro desvio da normalidade e estão distantes da média. entanto, não é algo para nos preocuparmos, pois não são valores muito extremos. Portanto, diagnose básica indicou que o modelo com Poisson parece ser adequado para modelar estes dados, ao menos em termos de homogeneidade de variância.Diagnose avançada dos resíduos modeloAlguns pacotes permitem calcular outros aspectos modelo que facilitam diagnose, ou seja, se podemos de fato confiar nos parâmetros estimados por eles, incluindo valores de significância. Um pressuposto importante dos modelos de contagem (incluindo Poisson) é sobredispersão (overdispersion).Vejamos como o pacote DHARMa funciona (Figura 8.3).\nFigura 8.3: Diagnose avançada dos resíduos modelo GLM Poisson.\nO plot claramente indica que há problema com overdispersion, mas não em termos de desvios de normalidade (KS test) ou outlier, já que apenas o primeiro foi significativo (aparece em vermelho).Detectando e lidando com overdispersionO que é sobredispersão (overdispersion)? Ela ocorre quando variância observada é muito maior que aquela predita pelo modelo. Para modelos que utilizam distribuição de Poisson, isso ocorre quando variância aumenta com média. Lembre-se de que esta distribuição tem apenas um único parâmetro para descrever tanto média quanto variância (\\(\\lambda\\)). Portanto, variância tem de ser igual à média. entanto, se variância nos dados observados muito maior que média, dizemos que há sobredispersão nos dados.Existem duas formas de diagnosticar overdispersion que estão implementadas na maioria dos pacotes. Aqui vamos demonstrá-las usando funções check_overdispersion() e testDispersion() disponíveis nos pacotes performance e DHARMa, respectivamente.função testDispersion() DHARMa utiliza um método de aleatorização dos resíduos para determinar se há overdispersion nos dados, cuja vantagem é que aborda diretamente variação nos dados, ao invés de medir o ajuste modelo em si, com outros testes (Figura 8.4).\nFigura 8.4: Teste de overdispersion pacote DHARMa.\nAqui temos um gráfico e o resultado novamente teste de overdispersion (que já aparecia gráfico anterior) mostrando que de fato há overdispersion: perceba que o valor de P é significativo. O gráfico nos mostra em cinza distribuição dos resíduos aleatorizados e linha vermelha o valor observado da estatística. Já que linha está bem à direita da distribuição, isso indica overdispersion, se estivesse à esquerda seria o caso de underdispersion.Agora vamos utilizar função check_overdisperion() que utiliza uma distribuição chi-quadrado e o valor de dispersion ratio para testar presença de overdispersion modelo. Esse teste também pode ser feito com função acima ao se especificar o argumento type=\"PearsonChisq\".Quando este resultado é significativo, como vimos na última linha acima, isso indica que há overdispersion.Usando função summary() podemos ter um resumo e descrição dos parâmetros modelo.Na parte de baixo output da função summary() também podemos calcular o dispersion parameter dividindo o residual deviance pelos graus de liberdade dos resíduos. Esta é outra maneira fácil e rápida de detectar overdispersion. Neste exemplo, temos que Dispersion parameter = 1.8119903. Quando esse valor é próximo de 1, isso sugere que não há overdispersion. entanto, se ele maior que 1.5, isso sugere que o modelo sofre de overdispersion e que devemos usar outra distribuição, tal como distribuição binomial negativa, por exemplo.Além disso, uma outra forma de diagnosticar o modelo é calcular os resíduos de Pearson (resíduos normalizados), que é basicamente raiz quadrada da variância da variável resposta.Inflação de zerosQualquer das formas mostradas acima de diagnosticar overdispersion pode ser usada na maioria das vezes, com exceção de dados com muitos zeros (pouca variância). Por isso, devemos também testar se o nosso modelo sofre de inflação de zeros. Vejamos como isso funciona usando funções check_zeroinflation() pacote performanace e testZeroInflation() pacote DHARMa (Figura 8.5).\nFigura 8.5: Teste de inflação de zeros pacote DHARMa.\nTanto função DHARMa quanto performance conseguiram detectar que o modelo tem problemas com overdispersion, mas isso não é causado pelo excesso de zeros. Como já dissemos acima, caso da distribuição Poisson, tanto média quanto variância são modeladas pelo mesmo parâmetro (\\(\\lambda\\)). Isso faz com que esta distribuição não seja muito útil para modelar dados de contagem em que haja muita variância em torno da média. Esse infelizmente é o caso da grande maioria dos dados ecológicos.Por estes motivos, não podemos fazer inferência com este modelo porque os parâmetros estimados não são confiáveis. Mas vejamos como seria feita essa inferência caso este modelo fosse adequado.InferênciaAqui iremos apresentar várias funções para calcular o coeficiente de determinação (R2). caso de GLM(M)s, não há um consenso sobre como se calcula este coeficiente, havendo várias propostas que utilizam maneiras diferentes de estimar homogeneidade de variância e covariância entre observações dos resíduos, veja Nakagawa et al. (2017) e Ives (2015) para maiores detalhes, assim como o help das respectivas funções.Podemos ver que os valores de R2 são bem baixos (em torno de 4 - 5%), independentemente método que usamos pra calculá-lo.Plot modelo preditoVamos realizar visualização ajusto modelo Poisson (Figura 8.6).\nFigura 8.6: Gráfico GLM Poisson.\nInterpretação dos resultadosAqui vemos que há uma leve tendência na relação positiva entre distância para o fragmento mais próximo e riqueza de anfíbios observada. entanto, há uma grande dispersão nos dados ao redor da reta modelo, fazendo com que relação não seja de fato significativa e tenhamos um R2 bem baixo. Caso pudéssemos confiar nos parâmetros deste modelo, poderíamos dizer que existe uma leve tendência um aumento da riqueza observada de anfíbios anuros à medida que aumenta distância da poça para o fragmento mais próximo.","code":"\n## Explorar os dados\nglimpse(fragmentos)\n#> Rows: 40\n#> Columns: 40\n#> $ locality    <chr> \"MAC\", \"MAC\", \"MAC\", \"MAC\", \"MAC\", \"MAC\", \"NOV\", \"NOV\", \"NOV\", \"NOV\", \"PIN\", \"PIN\", \"PIN\", \"PIN\", \"PIN\", \"PLAN\", \"PLAN\", \"PL…\n#> $ site        <chr> \"MacAc1\", \"MacAc2\", \"MacAc3\", \"MacAc4\", \"MacAc5\", \"MacBr1\", \"NovBr1\", \"NovBr2\", \"NovRe2\", \"NovRe3\", \"PinAc1\", \"PinBr1\", \"Pin…\n#> $ Riqueza_obs <int> 3, 11, 10, 10, 3, 9, 2, 8, 9, 8, 6, 4, 8, 8, 6, 17, 15, 13, 8, 10, 12, 14, 14, 12, 8, 4, 10, 7, 13, 19, 16, 13, 13, 14, 11, …\n#> $ Riqueza_HB  <int> 6, 13, 12, 13, 6, 12, 5, 11, 12, 11, 8, 7, 10, 11, 9, 18, 17, 15, 11, 13, 14, 16, 16, 13, 11, 7, 13, 9, 15, 20, 18, 15, 15, …\n#> $ Bsc         <int> 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0\n#> $ Dne         <int> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0\n#> $ Dnm         <int> 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0\n#> $ Dnn         <int> 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n#> $ Dns         <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0\n#> $ Hal         <int> 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1\n#> $ Hra         <int> 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n#> $ Lfu         <int> 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1\n#> $ Lla         <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n#> $ Lpo         <int> 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1\n#> $ Eun         <int> 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0\n#> $ Pce         <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0\n#> $ Pcu         <int> 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1\n#> $ Pfa1        <int> 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1\n#> $ Ppa         <int> 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0\n#> $ Sfm         <int> 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1\n#> $ Sfv         <int> 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0\n#> $ Ebi         <int> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0\n#> $ Esp         <int> 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1\n#> $ hydrop      <dbl> -2.553590, 0.573255, 0.573255, 0.573255, -2.553590, 0.573255, 0.573255, 0.573255, 0.573255, 0.573255, -1.511308, 0.573255, 0…\n#> $ hydrop2     <dbl> -2.235730, 0.606300, 0.606300, 0.606300, -2.235730, 0.606300, 0.606300, 0.606300, 0.606300, 0.606300, -1.667324, 0.606300, 0…\n#> $ vegcov      <dbl> -1.461851, -1.145775, -0.987737, 0.908718, -1.461851, 1.382832, 1.382832, 1.698908, -0.829699, 0.118528, -1.145775, 0.434604…\n#> $ nveg        <dbl> -1.965130, -0.158114, -1.061622, -0.158114, -1.965130, -0.158114, 0.745394, 1.648902, 0.745394, 1.648902, -1.061622, 0.74539…\n#> $ fish        <dbl> -1.508310, 0.646419, -1.508310, 0.646419, -1.508310, -1.508310, 0.646419, 0.646419, 0.646419, 0.646419, -1.508310, 0.646419,…\n#> $ area        <dbl> -2.418270, 0.147353, -0.564022, -0.348279, -2.315159, -0.601947, 1.556190, -0.255774, 0.875432, 1.295467, -1.846249, -0.6635…\n#> $ area2       <dbl> -1.884470, 0.019560, -0.699829, -0.497176, -1.844802, -0.734057, 1.877820, -0.406145, 0.907959, 1.490478, -1.625475, -0.7887…\n#> $ depth       <dbl> -1.232668, 0.821168, -0.704539, 0.821168, -1.306019, -1.306019, -0.645858, -0.205750, 1.554680, 1.554680, -1.232668, -1.1593…\n#> $ forcov      <dbl> -0.604596, -0.020849, -0.013816, -0.171663, 0.203364, 0.203364, 0.562496, -0.157193, -0.793258, -0.397977, 0.022087, -0.0234…\n#> $ forcov2     <dbl> -0.672774, -0.152952, -0.146124, -0.296136, 0.071358, 0.071358, 0.459151, -0.282666, -0.820946, -0.499384, -0.111058, -0.155…\n#> $ forcov10    <dbl> -6.045965, -0.208489, -0.138159, -1.716633, 2.033643, 2.033643, 5.624958, -1.571928, -7.932582, -3.979765, 0.220865, -0.2348…\n#> $ dfrag       <dbl> 0.410084, -0.097045, -1.242271, -1.242271, -0.471888, -1.242271, 1.307931, 1.557264, 0.479895, 0.966774, -0.192341, -1.24227…\n#> $ dfrag2      <dbl> 0.166782, -0.381401, -1.059858, -1.059858, -0.688845, -1.059858, 1.510271, 1.967926, 0.254153, 0.943632, -0.467438, -1.05985…\n#> $ dwater      <dbl> 1.198175, 0.970207, -0.121245, -0.087507, 0.162610, -0.121245, -0.087507, -1.355308, 0.113584, -0.193952, 1.560877, -1.35530…\n#> $ dwater2     <dbl> 1.166645, 0.864035, -0.299232, -0.270350, -0.042156, -0.299232, -0.270350, -1.045428, -0.088840, -0.359939, 1.690588, -1.045…\n#> $ X           <dbl> -49.9376, -49.9353, -49.9348, -49.9334, -49.9270, -49.9271, -49.2742, -49.3303, -49.3084, -49.3179, -48.9214, -48.9206, -48.…\n#> $ Y           <dbl> -20.7408, -20.7410, -20.7419, -20.7462, -20.7453, -20.7451, -21.5187, -21.5282, -21.5024, -21.5148, -21.2273, -21.2227, -21.…\n## Gráfico\nggplot(fragmentos, aes(dfrag, Riqueza_obs)) +\n    geom_point(size = 4, alpha = 0.7) +\n    geom_smooth(method = \"lm\") +\n     labs(x = \"Distância para o fragmento mais próximo\", \n         y = \"Riqueza observada\") +\n    tema_livro()\n## Modelo\nmod_pois <- glm(Riqueza_obs ~ dfrag, family = poisson(link = \"log\"), data = fragmentos)\n## Diagnose básica\npar(mfrow = c(2, 2))\nplot(mod_pois) \n## Diagnose avançada\nsimulationOutput <- simulateResiduals(fittedModel = mod_pois, plot = TRUE)\n## Overdispersion\npar(mfrow = c(1, 1))\ntestDispersion(mod_pois) # modelo tem overdispersion\n#> \n#>  DHARMa nonparametric dispersion test via sd of residuals fitted vs. simulated\n#> \n#> data:  simulationOutput\n#> dispersion = 1.6489, p-value < 2.2e-16\n#> alternative hypothesis: two.sided\n## Testar a presença de overdispersion\ncheck_overdispersion(mod_pois) # modelo tem overdispersion\n#> # Overdispersion test\n#> \n#>        dispersion ratio =  1.657\n#>   Pearson's Chi-Squared = 62.951\n#>                 p-value =  0.007\n## Resumo do modelo\nsummary(mod_pois)\n#> \n#> Call:\n#> glm(formula = Riqueza_obs ~ dfrag, family = poisson(link = \"log\"), \n#>     data = fragmentos)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -3.3467  -0.9110   0.0942   0.8336   2.2773  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   2.3051     0.0500  46.101   <2e-16 ***\n#> dfrag         0.0718     0.0507   1.416    0.157    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 70.868  on 39  degrees of freedom\n#> Residual deviance: 68.856  on 38  degrees of freedom\n#> AIC: 235.29\n#> \n#> Number of Fisher Scoring iterations: 4\n\n## Dispersion parameter\ndeviance(mod_pois) / df.residual(mod_pois)\n#> [1] 1.81199\n## Inflação de zeros - performanace\ncheck_zeroinflation(mod_pois) # para diagnosticar se o modelo sofre de zero inflation\n#> Model has no observed zeros in the response variable.\n#> NULL\n\n## Inflação de zeros - DHARMa\ntestZeroInflation(mod_pois) # para testar se existe zero inflation\n#> \n#>  DHARMa zero-inflation test via comparison to expected zeros with simulation under H0 = fitted model\n#> \n#> data:  simulationOutput\n#> ratioObsSim = NaN, p-value = 1\n#> alternative hypothesis: two.sided\n## Coeficientes estimados pelo modelo\nsummary(mod_pois)\n#> \n#> Call:\n#> glm(formula = Riqueza_obs ~ dfrag, family = poisson(link = \"log\"), \n#>     data = fragmentos)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -3.3467  -0.9110   0.0942   0.8336   2.2773  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   2.3051     0.0500  46.101   <2e-16 ***\n#> dfrag         0.0718     0.0507   1.416    0.157    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 70.868  on 39  degrees of freedom\n#> Residual deviance: 68.856  on 38  degrees of freedom\n#> AIC: 235.29\n#> \n#> Number of Fisher Scoring iterations: 4\n\n## Calculando o R2 do modelo\nr.squaredGLMM(mod_pois)\n#>                  R2m        R2c\n#> delta     0.04925800 0.04925800\n#> lognormal 0.05154558 0.05154558\n#> trigamma  0.04696308 0.04696308\nrsquared(mod_pois)\n#>      Response  family link     method  R.squared\n#> 1 Riqueza_obs poisson  log nagelkerke 0.04919844\nr2(mod_pois)\n#> # R2 for Generalized Linear Regression\n#>   Nagelkerke's R2: 0.059\na1 <- ggplot(fragmentos, aes(dfrag, Riqueza_obs)) +\n    geom_point(cex = 4,alpha = 0.7) +\n    geom_smooth(method = \"glm\", formula = y~x, \n                method.args = list(family = \"poisson\"), se = TRUE) +\n    labs(x = \"Distância para o fragmento mais próximo\", \n         y = \"Riqueza observada\") +\n    tema_livro()\n\nggMarginal(a1, fill = \"red\")"},{"path":"cap8.html","id":"o-que-causa-a-overdispersion","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.4.1 O que causa a overdispersion?","text":"Existem dois conjuntos de causas: aparente ou real.causas aparentes são geradas pela má especificação modelo, tais como:não inclusão de covariáveis ou interações modelopresença de outliers na variável respostaefeitos não lineares da covariável (X2, X3…)escolha errada da função de ligação (link function)causas reais incluem:variância maior que médiamuitos zerosagregação de observaçõescorrelação entre observações (não independência)","code":""},{"path":"cap8.html","id":"o-que-fazer-se-seu-modelo-tiver-overdispersion","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.4.2 O que fazer se seu modelo tiver overdispersion?","text":"Depois de tentar corrigir possíveis más especificações, como listadas acima, existem duas alternativas:usar outra distribuição, tal como binomial negativa, caso o dispersion parameter seja maior que 15 ou 20Usar um modelo com correção de erro da sobredispersão, caso 1.5 < dispersion > 15Geralmente, dados de contagem em estudos ecológicos não seguem uma distribuição Poisson, pois há muita dispersão (variância) nos dados. Logo, o pressuposto da distribuição Poisson, .e., de que média e variância são descritas por um mesmo parâmetro (\\(\\lambda\\)) é quebrado.Como vimos, overdispersion é um problema comum ao analisar dados ecológicos e deve necessariamente ser diagnosticado modelo. Uma maneira de lidar com esse tipo de problema é utilizar uma outra distribuição diferente da Poisson. binomial negativa pode ser entendida como uma mistura das distribuições Poisson e Gamma, ou seja, ela aceita dados de contagem que sejam positivos maiores que zero. grande vantagem desta distribuição é que, diferentemente da Poisson, ela tem um parâmetro para modelar média (\\(\\lambda\\)) e outro para modelar variância (k). Logo, ela permite modelar dados em que média é diferente da variância. Vejamos um exemplo.Aqui vamos continuar com estes dados para ver como o modelo se comporta com essa nova distribuição especificada. Para isso vamos utilizar função glm.nb pacote MASS.ModelagemDiagnose resíduosAssim como fizemos com o modelo com Poisson, vamos agora diagnosticar os resíduos (Figura 8.7).\nFigura 8.7: Diagnose básica dos resíduos modelo GLM binomial Negativo.\nCompare estes gráficos (Figura 8.7) com os modelo anterior com distribuição Poisson (Figura 8.2). Eles são praticamente idênticos, ou seja, o modelo com Poisson já tinha homogeneidade de variância e não tinha problemas com normalidade dos resíduos. Agora vejamos se o problema com overdispersion foi resolvido (Figura 8.8).\nFigura 8.8: Diagnose avançada dos resíduos modelo GLM binomial Negativo.\nNa diagnose modelo pelo DHARMa, vemos que bastou mudar distribuição de probabilidade que o problema de overdispersion foi resolvido (nenhum teste foi significativo quadro da esquerda). Como já sabíamos, não há problemas com homogeneidade de variância (plot da direita mostrando tendência entre o predito e resíduos pra cada quantil), nem de outliers. O dispersion parameter é mais próximo de 1 que modelo com Poisson. Agora sim podemos levar em conta o R2.InferênciaMas esse valor parece ser um pouco menor que anteriormente. Perceba que aqui utilizamos somente uma das funções apresentadas anteriormente, já que se trata de um modelo GLM com binomial negativa, calculamos o R2 pelo método de Nagelkerke.Usando função summary() podemos ter um resumo e descrição dos parâmetros modelo.Interpretação dos resultadosAqui vemos que interpretação resultado em termos de valor de P não mudou, ou seja, distância para o fragmento mais próximo não foi significativa. Mas vejam que o coeficiente (slope) mudou um pouco, antes era 0.0718 (SE=0.0507) e com binomial negativa passa ser 0.07248 (SE=0.06571).Plot modelo preditoVamos realizar visualização ajuste modelo binomial Negativo (Figura 8.9).\nFigura 8.9: Gráfico GLM binomial Negativo.\nAqui vemos que reta predita pelo modelo é muito similar ao que tivemos com o modelo Poisson. entanto, agora que sabemos que este modelo com binomial negativa foi corretamente especificado e podemos confiar nos parâmetros estimados.","code":"\n## Ajuste do modelo\nmod_nb <- glm.nb(Riqueza_obs ~ dfrag, data = fragmentos)\n## Diagnose\npar(mfrow = c(2, 2))\nplot(mod_nb)\npar(mfrow = c(1, 1))\n(chat <- deviance(mod_nb) / df.residual(mod_nb)) # DISPERSION PARAMETER\n#> [1] 1.126184\n## Diagnose avançada\nsimulationOutput <- simulateResiduals(fittedModel = mod_nb, plot = TRUE)\n## Coeficiente de determinação\nrsquared(mod_nb)\n#>      Response                     family link     method  R.squared\n#> 1 Riqueza_obs Negative Binomial(14.7068)  log nagelkerke 0.02935674\n## Coeficientes estimados pelo modelo\nsummary(mod_nb)\n#> \n#> Call:\n#> glm.nb(formula = Riqueza_obs ~ dfrag, data = fragmentos, init.theta = 14.70679964, \n#>     link = log)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.7569  -0.7068   0.0694   0.6194   1.6546  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  2.30504    0.06481  35.567   <2e-16 ***\n#> dfrag        0.07248    0.06571   1.103     0.27    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(14.7068) family taken to be 1)\n#> \n#>     Null deviance: 44.002  on 39  degrees of freedom\n#> Residual deviance: 42.795  on 38  degrees of freedom\n#> AIC: 231.68\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  14.71 \n#>           Std. Err.:  8.62 \n#> \n#>  2 x log-likelihood:  -225.68\n## Gráfico\nggplot(fragmentos, aes(dfrag, Riqueza_obs)) +\n    geom_point(size = 4, alpha = 0.7) +\n    geom_smooth(method = \"glm.nb\", formula = y~x, se = TRUE) +\n    labs(x = \"Distância para o fragmento mais próximo\", \n         y = \"Riqueza observada\") +\n    tema_livro()"},{"path":"cap8.html","id":"dados-de-contagem-modelos-quasi-likelihood","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.5 Dados de contagem: modelos quasi-likelihood","text":"Como dissemos acima, uma outra alternativa para ajustar modelos GLM dados de contagem são os chamados “quasi-likelihood”, tais como quasi-Poisson e quasi-binomial. Dependendo valor dispersion parameter, pode ser útil escolher este tipo de modelo. entanto, eles vêm com uma desvantagem: não é possível calcular o valor de Akaike Information Criterion (AIC) porque estes modelos não retornam um valor de likelihood (verosimilhança). Este parâmetro é comumente utilizado em abordagens estatísticas de teoria da informação para selecionar o melhor modelo que se ajusta aos dados. Neste caso, precisamos utilizar outras funções disponíveis nos pacotes MuMIn, AICcmodavg e bbmle para calcular o QAIC. Para mais detalhes sobre esses modelos, veja o vignette sobre o assunto pacote bbmle.AnáliseAqui vamos apenas exemplificar como um modelo com distribuição quasi-poisson pode ser especificado.Diagnose dos resíduosA função resid não leva em conta sobredispersão e temos de calcular manualmente o parâmetro de dispersão e inclui-lo plot. Portanto, não podemos realizar diagnose de modelos quasi-Poisson apenas com função plot como fazíamos até agora. Então, calculamos primeiramente os resíduos de Pearson e depois dividindo-o pela raiz quadrada parâmetro de dispersão (Figura 8.10).\nFigura 8.10: Diagnose dos resíduos modelo GLM Quasi-Poisson.\nAqui vemos que não existe um padrão claro nos resíduos, muito similar ao que tínhamos anteriormente. Devido às limitações de distribuições “quasi” e dado que já temos um modelo adequado com binomial negativa, sugerimos interpretar apenas o modelo anterior com binomial negativa.","code":"\n## Modelo\nmod_quasipois <- glm(Riqueza_obs ~ dfrag, family = quasipoisson(link = \"log\"), data = fragmentos)\n## Diagnose dos resíduos\nEP <- resid(mod_quasipois, type = \"pearson\")\nED <- resid(mod_quasipois, type = \"deviance\")\nmu <- predict(mod_quasipois, type = \"response\")\nE <- fragmentos$Riqueza_obs - mu\nEP2 <- E / sqrt(1.65662 * mu) # dispersion parameter da quasipoisson\nop <- par(mfrow = c(2, 2))\nplot(x = mu, y = E, main = \"Response residuals\")\nplot(x = mu, y = EP, main = \"Pearson residuals\")\nplot(x = mu, y = EP2, main = \"Pearson residuals scaled\")\nplot(x = mu, y = ED, main = \"Deviance residuals\")\npar(op)\npar(mfrow = c(1, 1))"},{"path":"cap8.html","id":"dados-de-contagem-a-distribuição-binomial","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.6 Dados de contagem: a distribuição binomial","text":"Quando temos dados de proporção (e.g., número de doentes por 1000 habitantes) ou incidência (.e., presença ou ausência), distribuição mais adequada para modelar os dados é distribuição binomial. entanto, temos que especificar o modelo de acordo com o tipo dos dados argumento formula. Vejamos dois exemplos.","code":""},{"path":"cap8.html","id":"análise-com-dados-de-proporção","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.6.1 Análise com dados de proporção","text":"Neste exemplo, vamos ver como podemos modelar proporção de células sanguíneas em função tipo de tratamento.Explicação dos dadosEste conjunto de dados foi coletado por (Franco-Belussi, De Oliveira, Sköld 2018). Os autores utilizaram um desenho experimental típico de uma 2x5 ANOVA fatorial (ou two-way ANOVA) em que temos dois tratamentos (fatores): pigmentação girino com dois níveis (Yes e ) e Tempo de exposição com cinco níveis (controle sem UV, 6 h, 12 h, 18 h e 24 h de exposição à UV).PerguntaA melanina protege girinos contra os efeitos da radiação ultravioleta?PrediçõesComo melanina participa sistema imune inato, ela desempenharia um papel na resposta organismo à radiação UV, auxiliando células imunes combater os seus efeitos deletérios.VariáveisVariável resposta: Contagem diferencial de eosinófilosdata frame com 10 girinos em cada tratamento, totalizando 50 girinosVamos explorar os dados para tentar entender como são relações (Figura 8.11).\nFigura 8.11: Gráfico para explorar relações entre variáveis.\nAqui vemos que quantidade de eosinófilos é muito maior nos girinos sem pigmentação (“albinos”). Já que estes animais não têm pigmentação melânica, células brancas sangue são única ferramenta de combate aos efeitos deletérios da UV.ModelagemAqui vamos usar o cbind argumento formula para dizer que queremos modelar contagem de eosinófilos em relação ao número total de células, ou seja, sua proporção. Aqui temos contagem número de eosinófilos (um tipo de célula da série branca sangue) em lâminas histológicas de girinos da rã-touro (Lithobates catesbeianus) num total de 1000 células.Diagnose básica dos resíduos modeloVamos fazer diagnose básica dos resíduos modelo (Figura 8.12).\nFigura 8.12: Diagnose básica dos resíduos modelo GLM binomial.\nParece que os resíduos não sofrem de heterogeneidade de variância (linha vermelha está reta), mas parece haver um pequeno desvio da normalidade (veja pontos 19, 29 e 32 destacados plot de quantis e de outliers). Vejamos o que o DHARMa nos diz (Figura 8.13).\nFigura 8.13: Diagnose avançada dos resíduos modelo GLM binomial.\nAgora vamos checar qualidade modelo com base na distribuição dos resíduos (Figura ??).Aqui já não resta dúvidas de que os resíduos deste modelo sofrem tanto com heterogeneidade de variância, quanto overdispersion e problemas com outliers. Provavelmente o problema com outliers ocorreu por conta pequeno tamanho amostral.InferênciaSabemos que o modelo não parece ser adequado para os dados, mas vamos interpretá-lo mesmo assim para que possamos entender saída da função summary() e os contrastes entre os níveis dos fatores.Aqui temos tanto tabela com os resultados por níveis dos fatores (summary()), quanto tabela com Deviance que mostra os fatores e suas interações (anova()). Vemos que nenhum fator foi significativo. Caso houvesse algum fator significativo, poderíamos testar significância de cada nível dos fatores usando contrastes, desta forma:Aqui temos o valor de cada combinação de níveis dos fatores, com seu respectivo valor de contraste e o valor de P. Vemos que para girinos sem pigmentação, apenas três contrastes foram significativos.Plot modelo preditoVamos realizar visualização ajuste modelo binomial (Figura 8.14).\nFigura 8.14: Gráfico GLM binomial.\nUsando o geom_violin(), podemos perceber que existe uma dispersão maior nos tratamentos que utilizaram girinos sem pigmentação que nos tratamentos com girinos pigmentados.","code":"\n## Dados\nhead(uv_cells)\n#>     UV Pigmentation Total_Cell Lymphocyte Neutrophil Basophil Monocyte Eosinophil\n#> 1 1.CT          Yes        100         80         18        0        0          2\n#> 2 1.CT          Yes        100         74         17        6        0          3\n#> 3 1.CT          Yes        100         78         22        0        0          0\n#> 4 1.CT          Yes        100         87         13        0        0          0\n#> 5 1.CT          Yes        100         74         21        1        0          4\n#> 6 2.6h          Yes        100         95          4        0        0          1\n\n## Traduzir nomes das colunas e níveis de pigmentação \ncolnames(uv_cells) <- c(\"UV\", \"Pigmentacao\", \"n_celulas\", \"linfocito\", \"neutrofilo\", \"basofilo\", \"monocito\", \"eosinofilo\")\nuv_cells$Pigmentacao[uv_cells$Pigmentacao==\"Yes\"] <- \"sim\"\nuv_cells$Pigmentacao[uv_cells$Pigmentacao==\"No\"] <- \"nao\"\n## Gráfico\n\n# Calcular média e intervalo de confiança\neosinofilo <- summarySE(uv_cells, \n                        measurevar = \"eosinofilo\",\n                        groupvars = c(\"UV\", \"Pigmentacao\"))\n\n# Definir posição de linhas e pontos no gráfico\npd <- position_dodge(0.1)\n\neosinofilo %>% \n  ggplot(aes(x = UV, y = eosinofilo, colour = Pigmentacao,\n             group = Pigmentacao, fill = Pigmentacao)) +\n  geom_errorbar(aes(ymin=eosinofilo-se, ymax=eosinofilo +se), \n                width=.1, size = 1.1, position=pd) +\n  geom_line(position=pd, size = 1.1) +\n  geom_point(pch = 21, colour = \"black\", position=pd, size=3.5) +\n  scale_colour_manual(values = c(\"darkorange\", \"cyan4\")) +\n  scale_fill_manual(values = c(\"darkorange\", \"cyan4\")) +\n  labs(x = \"UV\", y = \"Eosinófilo\", fill=\"Pigmentação\", colour=\"Pigmentação\")+\n  tema_livro()\n## Modelo\nmod1 <- glm(cbind(eosinofilo, n_celulas) ~ UV * Pigmentacao, family = binomial, data = uv_cells)\n## Diagnose dos resíduos\npar(mfrow = c(2, 2))\nplot(mod1)\npar(mfrow = c(1, 1))\n## Diagnose avançada\nsimulationBion <- simulateResiduals(fittedModel = mod1, plot = TRUE)\n## Diagnose avançada\nbinned_residuals(mod1)\n#> Warning: Probably bad model fit. Only about 14% of the residuals are inside the error bounds.\n## Coeficientes estimados pelo modelo\nsummary(mod1)\n#> \n#> Call:\n#> glm(formula = cbind(eosinofilo, n_celulas) ~ UV * Pigmentacao, \n#>     family = binomial, data = uv_cells)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -5.4165  -2.5266  -1.0148   0.8068   8.8233  \n#> \n#> Coefficients:\n#>                        Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)            -1.84516    0.12107 -15.241  < 2e-16 ***\n#> UV2.6h                 -0.17979    0.17835  -1.008   0.3134    \n#> UV3.12h                 0.38414    0.15899   2.416   0.0157 *  \n#> UV4.18h                -0.49825    0.19363  -2.573   0.0101 *  \n#> UV5.24h                -0.39916    0.18848  -2.118   0.0342 *  \n#> Pigmentacaosim         -2.17222    0.35745  -6.077 1.22e-09 ***\n#> UV2.6h:Pigmentacaosim  -0.07152    0.53831  -0.133   0.8943    \n#> UV3.12h:Pigmentacaosim -0.38414    0.50150  -0.766   0.4437    \n#> UV4.18h:Pigmentacaosim  1.13424    0.45981   2.467   0.0136 *  \n#> UV5.24h:Pigmentacaosim  0.68684    0.48370   1.420   0.1556    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 737.36  on 49  degrees of freedom\n#> Residual deviance: 460.85  on 40  degrees of freedom\n#> AIC: 610.35\n#> \n#> Number of Fisher Scoring iterations: 5\nanova(mod1)\n#> Analysis of Deviance Table\n#> \n#> Model: binomial, link: logit\n#> \n#> Response: cbind(eosinofilo, n_celulas)\n#> \n#> Terms added sequentially (first to last)\n#> \n#> \n#>                Df Deviance Resid. Df Resid. Dev\n#> NULL                              49     737.36\n#> UV              4   26.034        45     711.32\n#> Pigmentacao     1  235.682        44     475.64\n#> UV:Pigmentacao  4   14.789        40     460.85\n## Parâmetros\npairs(emmeans(mod1, ~ UV|Pigmentacao))\n#> Pigmentacao = nao:\n#>  contrast      estimate    SE  df z.ratio p.value\n#>  1.CT - 2.6h     0.1798 0.178 Inf   1.008  0.8518\n#>  1.CT - 3.12h   -0.3841 0.159 Inf  -2.416  0.1109\n#>  1.CT - 4.18h    0.4982 0.194 Inf   2.573  0.0753\n#>  1.CT - 5.24h    0.3992 0.188 Inf   2.118  0.2124\n#>  2.6h - 3.12h   -0.5639 0.167 Inf  -3.384  0.0064\n#>  2.6h - 4.18h    0.3185 0.200 Inf   1.593  0.5021\n#>  2.6h - 5.24h    0.2194 0.195 Inf   1.125  0.7933\n#>  3.12h - 4.18h   0.8824 0.183 Inf   4.824  <.0001\n#>  3.12h - 5.24h   0.7833 0.177 Inf   4.414  0.0001\n#>  4.18h - 5.24h  -0.0991 0.209 Inf  -0.474  0.9897\n#> \n#> Pigmentacao = sim:\n#>  contrast      estimate    SE  df z.ratio p.value\n#>  1.CT - 2.6h     0.2513 0.508 Inf   0.495  0.9879\n#>  1.CT - 3.12h    0.0000 0.476 Inf   0.000  1.0000\n#>  1.CT - 4.18h   -0.6360 0.417 Inf  -1.525  0.5461\n#>  1.CT - 5.24h   -0.2877 0.445 Inf  -0.646  0.9675\n#>  2.6h - 3.12h   -0.2513 0.508 Inf  -0.495  0.9879\n#>  2.6h - 4.18h   -0.8873 0.454 Inf  -1.957  0.2876\n#>  2.6h - 5.24h   -0.5390 0.480 Inf  -1.123  0.7942\n#>  3.12h - 4.18h  -0.6360 0.417 Inf  -1.525  0.5461\n#>  3.12h - 5.24h  -0.2877 0.445 Inf  -0.646  0.9675\n#>  4.18h - 5.24h   0.3483 0.382 Inf   0.911  0.8928\n#> \n#> Results are given on the log odds ratio (not the response) scale. \n#> P value adjustment: tukey method for comparing a family of 5 estimates\nggplot(uv_cells, aes(UV, eosinofilo)) +\n    geom_violin(aes(color = Pigmentacao)) +\n    geom_jitter(shape = 16, position = position_jitter(0.1), cex = 4, alpha = 0.7) +\n    scale_colour_manual(values = c(\"darkorange\", \"cyan4\")) +\n    tema_livro()"},{"path":"cap8.html","id":"análise-com-dados-de-incidência","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.7 Análise com dados de incidência","text":"Uma outra aplicação da distribuição binomial é quando temos dados de incidência, ou seja, presença ou ausência, de alguma variável. Por exemplo, presença ou ausência de uma espécie ou indivíduo num local. Neste caso formula é diferente e o modelo é similar uma regressão logística, vejamos.Aqui vamos utilizar os dados sobre autotomia da cauda de lagartos da espécie Coleodactylus meridionalis observados em fragmentos florestais da Mata Atlântica estado de Pernambuco (C. N. Oliveira et al. 2020).PerguntaA probabilidade de lagartos da espécie Coleodactylus meridionalis perderem (autotomizarem) cauda aumenta com o tamanho corpo e de acordo com o sexo dos lagartos?PrediçõesQuanto maior o lagarto, maior probabilidade de autotomia da cauda e que esta resposta poderia também diferir entre sexos devido ao dimorfismo sexual.VariáveisVariável resposta: Presença ou ausência de cauda autotomizada em lagartos encontrados por busca ativaExploração dos dadosEste conjunto de dados possui muitas entradas faltantes (codificadas como NA). Primeiro vamos visualizar o conjunto de dados e depois precisamos remover linhas que contêm dados faltantes. Aqui podemos usar função interna ggplot2::remove_missing() para remover linhas cujas variáveis informadas argumento estejam faltando, vejamos (Figura 8.15).\nFigura 8.15: Gráficos para explorar os dados faltantes.\nReparem que estão faltando 22.9% dos dados. Vamos excluir linhas com os dados faltantes para variável sex (Figura 8.16).\nFigura 8.16: Gráficos para explorar os dados faltantes.\nAgora, seguindo o que já estamos acostumados fazer, vamos visualisar os dados com nossa hipótese (Figura 8.17).\nFigura 8.17: Gráfico para explorar relações entre variáveis.\nModelagemAqui vamos construir dois modelos com mesma distribuição binomial, mas com dois link function: logit e probit. função logit possui caudas um pouco mais achatadas, isto é, curva probit se aproxima dos eixos mais rapidamente que logit. Geralmente não há muita diferença entre elas. Como não temos nenhuma expectativa de qual link function é o melhor, podemos fazer uma seleção de modelos.Existe pouca diferença entre o modelo probit e logit. Como o modelo logit é mais simples vamos interpretá-lo apenas.Diagnose dos resíduos modeloVamos fazer diagnose dos resíduos modelo (Figura 8.18).\nFigura 8.18: Diagnose avançada dos resíduos modelo GLM binomial de incidência.\nVamos realizar visualização ajusto modelo (Figura ??).InferênciaPara modelos com parâmetro de dispersão conhecida (e.g., binomial e Poisson), o chi-quadrado é estatística mais apropriada.Interpretação dos resultadosA interpretação dos resultados é que o tamanho de corpo (SVL) afeta negativamente probabilidade da cauda estar intacta, .e., com o aumento tamanho, probabilidade da cauda permanecer intacta diminui. interação não foi significativa, então o efeito é independente sexo dos lagartos.","code":"\n\n## Traduzir nomes das colunas e níveis de pigmentação \nhead(lagartos)\n#>   Numero    Sex   SVL Intact_tail_length Autotomized_tail_length Tail_state\n#> 1      2   Male 20.70                 NA                   12.88          0\n#> 2      3   Male 21.10                 NA                   13.07          0\n#> 3      6 Female 23.72                 NA                   17.56          0\n#> 4      9   Male 18.84              17.38                      NA          1\n#> 5     21   Male 22.20                 NA                   16.50          0\n#> 6     22   <NA> 20.59                 NA                   12.46          0\ncolnames(lagartos) <- c(\"numero\", \"sexo\", \"SVL\", \"comprimento_cauda\", \"cauda_autotomizada\", \"estado_cauda\")\n\n## Dados faltantes\nvis_miss(lagartos, cluster = TRUE) \n## Removendo dados faltantes\ndados_semNA <- remove_missing(lagartos, vars = \"sexo\") \n\n## Visualizar\nvis_miss(dados_semNA)\n## Gráfico\nggplot(dados_semNA, aes(SVL, estado_cauda)) +\n    geom_point(aes(shape = sexo, colour = sexo), size = 4, alpha = 0.4) +\n    geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n    labs(y = \"Estado da Cauda\", x = \"Comprimento Rostro-Cloacal (mm)\", shape = \"Sexo\", colour = \"Sexo\") +\n    tema_livro()\n## Modelos\nmod_log <- glm(estado_cauda ~ SVL * sexo, data = dados_semNA, family = binomial(link = \"logit\"))\nmod_pro <- glm(estado_cauda ~ SVL * sexo, data = dados_semNA, family = binomial(link = \"probit\"))\n\n# Seleção de modelos\nAICctab(mod_log, mod_pro, nobs = 139)\n#>         dAICc df\n#> mod_pro 0.0   4 \n#> mod_log 0.1   4\n## Diagnóse avançada\nsimulationBion <- simulateResiduals(fittedModel = mod_log, plot = T)\n## Diagnóse avançada\nbinned_residuals(mod_log)\n#> Warning: About 92% of the residuals are inside the error bounds (~95% or higher would be good).\n## Coeficientes estimados pelo modelo\nsummary(mod_log)\n#> \n#> Call:\n#> glm(formula = estado_cauda ~ SVL * sexo, family = binomial(link = \"logit\"), \n#>     data = dados_semNA)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -1.5694  -1.0449  -0.8196   1.2181   1.6310  \n#> \n#> Coefficients:\n#>              Estimate Std. Error z value Pr(>|z|)  \n#> (Intercept)    5.5834     2.5909   2.155   0.0312 *\n#> SVL           -0.2678     0.1178  -2.274   0.0230 *\n#> sexoMale       0.6977     4.4055   0.158   0.8742  \n#> SVL:sexoMale  -0.0442     0.2085  -0.212   0.8321  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 191.07  on 138  degrees of freedom\n#> Residual deviance: 181.38  on 135  degrees of freedom\n#> AIC: 189.38\n#> \n#> Number of Fisher Scoring iterations: 4\nanova(mod_log, test = \"Chisq\" )\n#> Analysis of Deviance Table\n#> \n#> Model: binomial, link: logit\n#> \n#> Response: estado_cauda\n#> \n#> Terms added sequentially (first to last)\n#> \n#> \n#>          Df Deviance Resid. Df Resid. Dev Pr(>Chi)   \n#> NULL                       138     191.07            \n#> SVL       1   9.2563       137     181.82 0.002347 **\n#> sexo      1   0.3920       136     181.43 0.531262   \n#> SVL:sexo  1   0.0454       135     181.38 0.831292   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"cap8.html","id":"dados-de-contagem-com-excesso-de-zeros","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.8 Dados de contagem com excesso de zeros","text":"Quando se analisa abundância ou riqueza de espécies é comum que tenhamos dados com muitos zeros. Esse fenômeno pode ser causado por vários processos ecológicos, tais como locais fora nicho da espécie, falha na detecção, amostras feitas fora habitat ou em locais onde não se espera encontrar espécie (Blasco‐Moreno et al. 2019). Esse tipo de dado é problemático porque rompe com os pressupostos da distribuição Poisson e binomial negativa, podendo inclusive ser uma das causas da overdispersion.Nesses casos, temos de ajustar modelos que levam em conta esse excesso de zeros nos dados. Esses modelos são chamados de zero-inflated e hurdle models (também chamados de zero-altered models), dependendo de como o processo que causou os zeros é modelado.Hurdle models (ou zero-altered models) modelam os dados dividindo-os em dois subconjuntos: um qual reduzimos os dados à presença-ausência, ou seja, todos os dados maiores que 1 são transformados em 1 e usamos, por exemplo, uma distribuição binomial; e uma outra parte que só considera os valores positivos sem zero, utilizando uma Poisson ou binomial negativa truncadas. Ao fazer isso, distribuição truncada assume que os zeros são gerados tanto por processos ecológicos, quanto erros de amostragem (ou seja, é impossível distinguir entre essas duas fontes). Portanto, esses zeros são excluídos da distribuição com dados de contagem. Por exemplo, se uma distribuição binomial negativa usada para modelar parte quantitativa, chamamos o modelo de Zero-altered Negative binomial. interpretação dos modelos deve ser feita de forma conjunta.Modelos com zero inflados funcionam de maneira similar, mas permitem que distribuição Poisson contenha zeros, ou seja, não é utilizada uma distribuição truncada. Ao fazer isso, está distribuição de Poisson pressupõe que os zeros foram gerados por um processo ecológico real, tal como, ausência de habitat adequado.Para ilustrar como podemos lidar com conjuntos de dados complexos, vamos utilizar os dados parasita Raillietiella mottae infectando duas espécies de lagartos que ocorrem Nordeste Brasileiro (Lima et al. 2018). Ao todo, 63 indivíduos de Hemidactylus agrius e 132 de Phyllopezus pollicaris foram amostrados.PerguntaQuais atributos de história de vida dos lagartos são relacionados com o volume (load) de infecção, tais como tamanho e sexo?PrediçõesQuanto maior o lagarto, maior o número de parasitas encontrados, esta resposta poderia também diferir entre sexos devido ao dimorfismo sexual.VariáveisVariável resposta: número parasita Raillietiella mottae, que é um crustáceo parasita, infectando o aparelho respiratório e intestinal de lagartos.Vamos explorar os dados (Figura 8.19).\nFigura 8.19: Gráfico para explorar relações entre variáveis.\nOs gráfico acima mostram contagem parasita Raillietiella mottae nos dois sexos (F e M para fêmea e macho) nas duas espécies de lagartos (Hemidactylus agrius e Phyllopezus pollicaris), tanto na forma de uma distribuição de densidade, quanto de gráfico de dispersão. Aqui podemos ver que de fato existe um excesso de zeros principalmente em P. pollicaris.Quando nos deparamos com dados complexos assim, estratégia é sempre começar com um modelo simples e depois adicionar mais parâmetros. Portanto, vamos iniciar com um modelo Poisson, mesmo sabendo que ele muito provavelmente não será adequado para modelar estes dados.ModelagemAjuste modelo Poisson.DiagnoseAqui vamos utilizar funções pacote performance para o GLM Poisson.diagnose não só nos disse que o modelo possui overdispersion, como também de zero-inflation, como já esperávamos. Vejamos então como melhorar o nosso modelo para lidar com esses dois problemas. Especificamente, vamos utilizar um modelo Hurdle com binomial negativa truncada (ou seja, desconsiderando os zeros), e dois outros modelos com zero-inflated usando distribuição binomial negativa e Poisson. Aqui vamos utilizar o pacote glmmTMB.DiagnoseVamos fazer diagnose desses três modelos ajustados anteriormente.Aqui vemos que o modelo zero-altered (Hurdle Model) conseguiu predizer exatamente quantidade de zeros observada, fazendo com que o modelo seja suficiente para usarmos com esses dados.Mas quando comparamos o AICc entre modelos, os modelos zero-inflated (tanto Poisson, quanto binomial negativa) que tem menos parâmetros, são ranqueados ligeiramente melhor que o modelo binomial negativa zero-altered (ou hurdle). Não podemos distinguir entre os dois modelos com zero-inflated porque o dAICc < 2, ou seja, o ajuste deles aos dados são praticamente iguais. Vejam que diferença de Akaike Weights entre os dois primeiros modelos e o hurdle é bastante substancial (0.34 e 0.52). Além disso, vemos que os modelos que levam em conta o excesso de zeros se ajustam bem melhor aos dados que o modelo simples com distribuição Poisson.\nVamos ver como os modelos se saem em relação aos outros pressupostos.Modelo Hurdle (Figura 8.20).\nFigura 8.20: Diagnose avançada dos resíduos modelo GLM Hurdle.\nModelo zero-inflated Poisson (Figura 8.21).\nFigura 8.21: Diagnose avançada dos resíduos modelo GLM zero-inflated Poisson.\nModelo zero-inflated negative binomial (Figura 8.22).\nFigura 8.22: Diagnose avançada dos resíduos modelo GLM zero-inflated negative binomial.\nOs gráficos de diagnose DHARMa são outra evidência de que tanto o modelo hurdle quanto o zero-inflated Poisson são adequados para os dados, em termos de homogeneidade de variância, outliers e overdispersion.Interpretação dos resultadosApesar de não ter um ajuste tão bom aos dados, o modelo hurdle prediz melhor quantidade de zeros. Portanto, vamos interpretar os coeficientes apenas deste modelo.Para maiores detalhes na interpretação deste tipo de modelo, sugerimos fortemente consultar p. 382-3 de Brooks et al. (2017). Para fatores com mais de um nível, o summary() mostra os resultados usando contraste, para isto toma como referência um dos níveis fator (o primeiro em ordem alfabética) e o compara com os outros. Note que na parte com excesso de zeros, o contraste é positivo para Espécie. Ou seja, o P. pollicaris tem maior chance de ter ausência de parasitas que H. agrius. O contraste para espécie continua sendo positivo na parte condicional modelo, mas o valor parâmetro não é tão alto. Isso quer dizer que P. pollicaris tem abundância de parasitas em média ligeiramente maior que H. agrius. Vemos que interação é significativa entre sexo e espécie na parte modelo com excesso de zeros, mas apenas marginalmente significativa na parte condicional. Portanto, influência sexo na incidência, mas não na abundância parasita depende conjuntamente da espécie. entanto, o CRC só passa ser significativo na parte de excesso de zeros, ou seja, quando modelamos apenas incidência (presença-ausência) parasita. Portanto, o CRC determina se o lagarto vai ou não ser infectado, mas não o quanto vai receber de parasitas. Já tanto o sexo quanto espécie foram significativas em ambas partes modelo, ou seja, esses fatores não influenciam diferentemente infecção e quantidade de parasitas.\nAgora vejamos como podemos plotar predições deste modelo (Figura 8.23).\nFigura 8.23: Gráfico GLM Hurdle.\n","code":"\n## Cabeçalho dos dados\nhead(parasitas)\n#>                     Especie Sexo CRC Raillietiella_mottae\n#> W124 Phyllopezus_pollicaris    F  61                    3\n#> W125 Phyllopezus_pollicaris    F  56                    0\n#> W127 Phyllopezus_pollicaris    M  61                    0\n#> W128 Phyllopezus_pollicaris    M  48                    0\n#> W129 Phyllopezus_pollicaris    F  40                    0\n#> W130 Phyllopezus_pollicaris    M  62                    0\n## Gráficos\nggplot(parasitas, aes(Raillietiella_mottae, fill = Especie)) +\n    geom_density(alpha = 0.4) +\n    facet_grid(Especie ~ Sexo) +\n    scale_fill_manual(values = c(\"darkorange\", \"cyan4\")) +\n    tema_livro() +\n    theme(legend.position = \"none\")\n\nggplot(parasitas, aes(CRC, Raillietiella_mottae, fill = Especie)) +\n    geom_point(size = 4, alpha = 0.4, shape = 21) +\n    facet_grid(Sexo ~ Especie) +\n    scale_fill_manual(values = c(\"darkorange\", \"cyan4\")) +\n    theme(legend.position = \"none\") +\n    labs(x = \"Comprimento Rostro-Cloacal\", y = expression(italic(\"Raillietiella mottae\")))+\n    tema_livro()\n## Modelo\npois_plain <- glm(Raillietiella_mottae ~ CRC + Sexo * Especie, data = parasitas, family = \"poisson\")\n## Diagnose avançada\n# Verificar zero inflation\ncheck_zeroinflation(pois_plain) # para diagnosticar se o modelo sofre de zero inflation\n#> # Check for zero-inflation\n#> \n#>    Observed zeros: 156\n#>   Predicted zeros: 140\n#>             Ratio: 0.90\n\n# Verificar overdispersion\ncheck_overdispersion(pois_plain)\n#> # Overdispersion test\n#> \n#>        dispersion ratio =   1.932\n#>   Pearson's Chi-Squared = 367.133\n#>                 p-value = < 0.001\n## Modelos\n# Hurdle model\nhur_NB <- glmmTMB(Raillietiella_mottae ~ CRC + Sexo * Especie, zi = ~., data = parasitas, family = truncated_nbinom2) \n\n# zero-inflated Poisson\nziNB_mod2 <- glmmTMB(Raillietiella_mottae ~ CRC + Sexo * Especie, zi = ~., data = parasitas, family = nbinom2) \n\n# zero-inflated Negative binomial\nziP_mod2 <- glmmTMB(Raillietiella_mottae ~ CRC + Sexo * Especie, zi = ~., data = parasitas, family = poisson) \n## Diagnose de inflação de zeros\ncheck_zeroinflation(hur_NB) \n#> # Check for zero-inflation\n#> \n#>    Observed zeros: 156\n#>   Predicted zeros: 157\n#>             Ratio: 1.01\ncheck_zeroinflation(ziP_mod2)\n#> # Check for zero-inflation\n#> \n#>    Observed zeros: 156\n#>   Predicted zeros: 140\n#>             Ratio: 0.90\ncheck_zeroinflation(ziNB_mod2)\n#> # Check for zero-inflation\n#> \n#>    Observed zeros: 156\n#>   Predicted zeros: 142\n#>             Ratio: 0.91\n## Seleção de modelos\nICtab(pois_plain, hur_NB, ziP_mod2, ziNB_mod2, type = c(\"AICc\"), weights = TRUE)\n#>            dAICc df weight\n#> ziP_mod2    0.0  10 0.62  \n#> ziNB_mod2   1.6  11 0.28  \n#> hur_NB      3.6  11 0.10  \n#> pois_plain 44.6  5  <0.001\n## Diagnoses\nsimulationOutput <- simulateResiduals(fittedModel = hur_NB, plot = T)\n## Diagnoses\nsimulationOutput <- simulateResiduals(fittedModel = ziP_mod2, plot = T)\n## Diagnoses\nsimulationOutput <- simulateResiduals(fittedModel = ziNB_mod2, plot = T) \n## Coeficientes estimados pelo modelo\nsummary(hur_NB)\n#>  Family: truncated_nbinom2  ( log )\n#> Formula:          Raillietiella_mottae ~ CRC + Sexo * Especie\n#> Zero inflation:                        ~.\n#> Data: parasitas\n#> \n#>      AIC      BIC   logLik deviance df.resid \n#>    277.8    313.8   -127.9    255.8      184 \n#> \n#> \n#> Dispersion parameter for truncated_nbinom2 family (): 4.64 \n#> \n#> Conditional model:\n#>                                     Estimate Std. Error z value Pr(>|z|)  \n#> (Intercept)                          3.03428    2.36511   1.283   0.1995  \n#> CRC                                 -0.05041    0.04861  -1.037   0.2997  \n#> SexoM                               -1.49505    0.71440  -2.093   0.0364 *\n#> EspeciePhyllopezus_pollicaris        0.68945    1.09380   0.630   0.5285  \n#> SexoM:EspeciePhyllopezus_pollicaris  1.75281    0.94217   1.860   0.0628 .\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Zero-inflation model:\n#>                                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)                           7.6283     1.8529   4.117 3.84e-05 ***\n#> CRC                                  -0.1291     0.0369  -3.499 0.000468 ***\n#> SexoM                                -1.0893     0.5867  -1.856 0.063386 .  \n#> EspeciePhyllopezus_pollicaris         2.2701     0.9140   2.484 0.013003 *  \n#> SexoM:EspeciePhyllopezus_pollicaris   2.2002     0.8192   2.686 0.007239 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Gráfico\nparasitas$phat <- predict(hur_NB, type = \"response\")\nparasitas <- parasitas[with(parasitas, order(Sexo, Especie)), ]\n\nggplot(parasitas, aes(x = CRC, y = phat, colour = Especie,\n                      shape = Sexo, linetype = Sexo)) +\n    geom_point(aes(y = Raillietiella_mottae), size = 4, \n               alpha = .7, position = position_jitter(h = .2)) +\n    geom_line(size = 1) +\n    scale_fill_manual(values = c(\"darkorange\", \"cyan4\")) +\n    scale_colour_manual(values = c(\"darkorange\", \"cyan4\")) +\n       labs(x = \"Comprimento Rostro-Cloacal\", \n         y = expression(paste(\"Abundância de \", italic(\"Raillietiella mottae\")))) +\n    tema_livro()"},{"path":"cap8.html","id":"dados-ordinais-os-modelos-cumulative-link","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.9 Dados ordinais: os modelos cumulative link","text":"Uma outra maneira de codificarmos os dados é utilizando categorias ordenadas, tais como ranques. Exemplos incluem escala de Likert, scores e intervalos (e.g., de idade).Para este exemplo, iremos utilizar um outro conjunto de dados artigo de (Franco-Belussi, De Oliveira, Sköld 2018) que manipulou vitro concentração hormônio noradrenalina nos olhos de peixes esgana-gato (Gasterosteus aculeatus) e avaliaram expressão de várias cores conferidas por tipos de células (cromatóforos). Aqui vamos usar os dados efeito da noradrenalina na cor vermelha em machos.\nNeste experimento, os autores realizaram medidas repetidas mesmo animal ao longo tempo (que é um dos fatores deste experimento). Portanto, para não incorrer risco de pseudoréplicas deveríamos incluir esta informação modelo. maneira mais simples de fazê-lo é criar um modelo de efeito misto em que teríamos uma parte fixa (o que de fato estamos interessados em testar) e outra aleatória (variáveis que precisamos controlar). Portanto, Animal será incluído como um fator aleatório, tendo um intercepto estimado separadamente para cada animal modelo. Não iremos entrar em detalhes sobre modelos de efeito misto porque necessitaríamos de um outro capítulo para isso, dada complexidade assunto. entanto, recomendamos aos leitores dois artigos introdutórios muito bons - Harisson (2018) e Bolker et al. (2009).PerguntaA noradrenalina causa uma diminuição da coloração vermelha, via agregação dos pigmentos?PrediçõesA presença de noradrenalina causa agregação dos pigmentos, permitindo que os hormônios reprodutivos atuem.VariáveisVariável resposta: escala de intensidade de cor. Para mais detalhes veja o artigo original (Franco-Belussi, De Oliveira, Sköld 2018).Esses dados, entanto, têm de ser codificados como um fator ordenado antes de entrarmos com eles modelo.Repare que classe objeto muda e temos agora que Red é um Ordered factor.ModelagemVamos ajustar um modelo cumulative link, utilizando função clmm() pacote ordinal.DiagnoseInfelizmente, o pacote ordinal não fornece métodos para lidar com modelos mistos, como o nosso. Então, montamos um modelo fixo apenas para entrar nas duas funções de diagnose. Essas duas funções scale_test() e nominal_test() testam qualidade ajuste (goodness--fit) modelo, similar aos likelihood-ratio test, só que para dados ordinais.Parece que não há problemas com o efeito de escala dado ordinal, mas diagnose sugere que possa haver evidência de rompimento pressuposto de probabilidades proporcionais em relação ao tratamento. Esse é um pressuposto importante de modelos ordinais, os quais assumem que os efeitos de qualquer uma das variáveis explicativas são consistentes (proporcionais) ao longo de diferentes thresholds (que são quebras entre cada par de categorias da variável resposta ordinal).Isto provavelmente se deve ao baixo tamanho amostral. Por questão de brevidade vamos apenas ignorar este aspecto e interpretar o resultado modelo mesmo assim. Mas se o seu modelo apresentar este problema, solução deve ser realizar regressões logísticas separadamente.InferênciaVamos analisar os parâmetros modelo.Aqui vemos que tanto o tratamento, quanto o tempo de exposição foram significativos.Interpretação dos resultadosVamos analisar predição modelo (Figura 8.24).\nFigura 8.24: Gráfico modelo cumulative link.\nNeste gráfico vemos que o grupo tratado com o hormônio começa com um índice de eritróforo ligeiramente maior que o controle, mas logo tem uma brusca redução com uma hora de exposição, passando ter índice 2, demonstrando que houve uma mudança de cor induzida pela dispersão dos cromatóforos. Já controle, o índice não muda em relação ao tempo de exposição. Logo, podemos ver que há uma interação entre os fatores devido esta queda índice grupo tratado e ausência de efeito controle.","code":"\n## Importar os dados\ncores <- read.csv2(\"https://ndownloader.figshare.com/files/10250700\", header = TRUE)\nhead(cores)\n#>   Animal Treatment Time Sex Black Red\n#> 1      1        CT   0h   M     5   5\n#> 2      1        CT   1h   M     5   5\n#> 3      1        CT   2h   M     5   5\n#> 4      1        CT   3h   M     5   5\n#> 5      2        CT   0h   M     5   4\n#> 6      2        CT   1h   M     5   4\n\n## Tradução dos nomes das colunas\ncolnames(cores) <- c(\"animal\", \"tratamento\", \"tempo\", \"sexo\", \"preto\", \"vermelho\")\n\n## Filtrando dados - macho vermelho\nmacho_verm <- filter(cores, sexo == \"M\")\nhead(macho_verm)\n#>   animal tratamento tempo sexo preto vermelho\n#> 1      1         CT    0h    M     5        5\n#> 2      1         CT    1h    M     5        5\n#> 3      1         CT    2h    M     5        5\n#> 4      1         CT    3h    M     5        5\n#> 5      2         CT    0h    M     5        4\n#> 6      2         CT    1h    M     5        4\n## Fator\nmacho_verm$animal <- factor(macho_verm$animal)\nmacho_verm$vermelho_ord <- factor(macho_verm$vermelho, \n                      levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"), \n                      ordered = TRUE)\nstr(macho_verm)\n#> 'data.frame':    40 obs. of  7 variables:\n#>  $ animal      : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 2 2 2 2 3 3 ...\n#>  $ tratamento  : chr  \"CT\" \"CT\" \"CT\" \"CT\" ...\n#>  $ tempo       : chr  \"0h\" \"1h\" \"2h\" \"3h\" ...\n#>  $ sexo        : chr  \"M\" \"M\" \"M\" \"M\" ...\n#>  $ preto       : int  5 5 5 5 5 5 5 5 4 4 ...\n#>  $ vermelho    : int  5 5 5 5 4 4 4 4 4 4 ...\n#>  $ vermelho_ord: Ord.factor w/ 5 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 5 5 5 5 4 4 4 4 4 4 ...\n## Modelo\nmod3 <- clmm(vermelho_ord ~ tratamento + tempo + (1|animal), data = macho_verm, threshold = \"equidistant\")\n## Diagnose\nassumption3 <- clm(vermelho_ord ~ tratamento + tempo, data = macho_verm, threshold = \"equidistant\")\n\nscale_test(assumption3)\n#> Tests of scale effects\n#> \n#> formula: vermelho_ord ~ tratamento + tempo\n#>            Df  logLik    AIC      LRT Pr(>Chi)\n#> <none>        -24.301 60.602                  \n#> tratamento  1 -24.293 62.586 0.015248   0.9017\n#> tempo\nnominal_test(assumption3)\n#> Tests of nominal effects\n#> \n#> formula: vermelho_ord ~ tratamento + tempo\n#>            Df  logLik    AIC    LRT Pr(>Chi)   \n#> <none>        -24.301 60.602                   \n#> tratamento  1 -19.749 53.499 9.1031 0.002552 **\n#> tempo       3 -22.803 63.606 2.9953 0.392356   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Coeficientes estimados pelo modelo\nsummary(mod3)\n#> Cumulative Link Mixed Model fitted with the Laplace approximation\n#> \n#> formula: vermelho_ord ~ tratamento + tempo + (1 | animal)\n#> data:    macho_verm\n#> \n#>  link  threshold   nobs logLik AIC   niter    max.grad cond.H \n#>  logit equidistant 40   -22.89 59.77 226(681) 1.04e-05 4.1e+01\n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  animal (Intercept) 1.438    1.199   \n#> Number of groups:  animal 5 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> tratamentoNA10uM   -4.602      1.228  -3.748 0.000178 ***\n#> tempo1h            -3.602      1.377  -2.616 0.008894 ** \n#> tempo2h            -3.602      1.377  -2.616 0.008894 ** \n#> tempo3h            -3.602      1.377  -2.616 0.008894 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Threshold coefficients:\n#>             Estimate Std. Error z value\n#> threshold.1   -6.198      1.722   -3.60\n#> spacing        4.978      1.254    3.97\nanova(assumption3)\n#> Type I Analysis of Deviance Table with Wald chi-square tests\n#> \n#>            Df   Chisq Pr(>Chisq)    \n#> tratamento  1 15.3616  8.877e-05 ***\n#> tempo       3  9.1992    0.02676 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\npairs(emmeans(mod3, ~ tratamento|tempo, adjust = \"tukey\"))\n#> tempo = 0h:\n#>  contrast    estimate   SE  df z.ratio p.value\n#>  CT - NA10uM      4.6 1.23 Inf   3.748  0.0002\n#> \n#> tempo = 1h:\n#>  contrast    estimate   SE  df z.ratio p.value\n#>  CT - NA10uM      4.6 1.23 Inf   3.748  0.0002\n#> \n#> tempo = 2h:\n#>  contrast    estimate   SE  df z.ratio p.value\n#>  CT - NA10uM      4.6 1.23 Inf   3.748  0.0002\n#> \n#> tempo = 3h:\n#>  contrast    estimate   SE  df z.ratio p.value\n#>  CT - NA10uM      4.6 1.23 Inf   3.748  0.0002\n## Gráfico\n\n# Calcular média e erro padrão\nmacho_verm_res <- summarySE(macho_verm, \n                        measurevar = \"vermelho\",\n                        groupvars = c(\"tempo\", \"tratamento\"))\n\n# Definir posição de linhas e pontos no gráfico\npd <- position_dodge(0.1)\n\nmacho_verm_res %>% \n  ggplot(aes(x = tempo, y = vermelho, colour = tratamento,\n             group = tratamento, fill = tratamento)) +\n  geom_errorbar(aes(ymin=vermelho-se, ymax=vermelho +se), \n                width=.1, size = 1.1, position=pd) +\n  geom_line(position=pd, size = 1.1) +\n  geom_point(pch = 21, colour = \"black\", position=pd, size=3.5) +\n  scale_colour_manual(values = c(\"darkorange\", \"cyan4\")) +\n  scale_fill_manual(values = c(\"darkorange\", \"cyan4\")) +\n  xlab(\"Tempo de exposição (horas)\") +\n  ylab(\"Índice de eritróforos\") +\n  tema_livro()"},{"path":"cap8.html","id":"dados-contínuos-distribuição-beta","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.10 Dados contínuos: distribuição beta","text":"Aqui vamos utilizar como exemplo os dados artigo de Franco-Belussi et al. (2018). Os pesquisadores fizeram um experimento vivo com peixes esgana-gato (Gasterosteus aculeatus) para testar como coloração dos animais respondem ao fármaco ioimbina (YOH), que bloqueia coloração típica que os machos exibem na época de acasalamento, e o tempo de exposição ao mesmo (além de um controle), num desenho de ANOVA fatorial. Como medidas foram feitas repetidamente mesmo animal, iremos incluir o Animal como um fator aleatório modelo.Esses dados contêm variáveis resposta medidas experimento: quantidade de vermelho e preto. Além dos fatores manipulados: Tratamento (controle e presença de YOH) e tempo de exposição.PerguntaA YOH aumenta coloração escura olho e mandíbula dos peixes via dispersão dos pigmentos?PrediçõesA YOH promoverá um escurecimento corpo animal, já que ela inibe ação NorAdrenalia (NA).VariáveisVariável resposta: intensidade de coloração escura em peixes machos. Esses dados são expressos em termos de porcentagem e variam continuamente de 0 100%. Para facilitar modelagem e nos adequarmos à maneira com que função requer os dados, vamos simplesmente dividir por 100 para que os dados variem entre 0 e 1Para modelar os dados vamos utilizar função glmmTMB. Antes disso, vamos analisar graficamente os dados (Figura 8.25). Vamos usar apenas os dados dos machos para este exemplo.\nFigura 8.25: Gráfico para explorar distribuição da variável resposta.\nhistograma podemos ver que os dados de fato variam continuamente intervalo entre 0 e 1, tendo uma distribuição notadamente bimodal.ModelagemVamos ajustar um GLM Beta.DiagnoseAqui utilizaremos o mesmo pacote DHARMa para realizar diagnose modelo\n(Figura 8.26).\nFigura 8.26: Diagnose avançada dos resíduos modelo GLM Beta.\nPodemos ver que o modelo não sofre de heterogeneidade de dispersão, overdispersion, nem problemas com outlier.Interpretação dos resultadosAgora que podemos interpretar saída modelo ajustado com confiança, vamos obter tabela de anova em que teremos os testes de cada fator modelo.Aqui vemos que interação é significativa. Portanto, temos de interpretar os níveis fator da combinação, fazemos isso pacote emmeans colocando barra |.Agora podemos perceber que diferença entre o controle e o tratado só passa ser significativa depois de 1 h de exposição.Isso fica mais evidente quando plotamos os dados (Figura 8.27).\nFigura 8.27: Gráfico modelo GLM beta.\n","code":"\nhead(fish)\n#>   Animal Treatment Time Sex Darkness Redness\n#> 1      1        CT   0h   M    56.42  131.17\n#> 2      1        CT   1h   M    49.53  133.30\n#> 3      1        CT   2h   M    54.94  132.69\n#> 4      1        CT   3h   M    42.19  135.37\n#> 5      2        CT   0h   M    58.93  133.35\n#> 6      2        CT   1h   M    52.45  133.65\n\n## Tradução dos nomes das colunas\ncolnames(fish) <- c(\"animal\", \"tratamento\", \"tempo\", \"sexo\", \"preto\", \"vermelho\")\n## Filtrando os dados\nfish$animal <- factor(fish$animal)\nfish$sexo <- factor(fish$sexo)\nmacho_preto <- dplyr::filter(fish, sexo == \"M\")\n\n## Gráfico\nggplot(macho_preto, aes(preto/100)) +\n    geom_density(colour = \"cyan4\", fill = \"cyan4\", alpha = 0.4) +\n    theme(legend.position = \"none\") +\n    labs(x = \"Índice de escuridão do corpo\")+\n    tema_livro()\n## Modelo\nmod2 <- glmmTMB(preto/100 ~ tratamento * tempo + (1|animal), family = beta_family, data = macho_preto)\n## Diagnóse\nsimulationOutput <- simulateResiduals(fittedModel = mod2, plot = TRUE)\n## Coeficientes estimados pelo modelo\nAnova(mod2)\n#> Analysis of Deviance Table (Type II Wald chisquare tests)\n#> \n#> Response: preto/100\n#>                    Chisq Df Pr(>Chisq)    \n#> tratamento       105.546  1  < 2.2e-16 ***\n#> tempo             40.719  3  7.499e-09 ***\n#> tratamento:tempo  49.262  3  1.147e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## níveis do fator da combinação\npairs(emmeans(mod2, ~ tratamento|tempo))\n#> tempo = 0h:\n#>  contrast estimate    SE df t.ratio p.value\n#>  CT - YOH   0.0283 0.160 30   0.177  0.8609\n#> \n#> tempo = 1h:\n#>  contrast estimate    SE df t.ratio p.value\n#>  CT - YOH  -1.3068 0.181 30  -7.210  <.0001\n#> \n#> tempo = 2h:\n#>  contrast estimate    SE df t.ratio p.value\n#>  CT - YOH  -1.2286 0.182 30  -6.763  <.0001\n#> \n#> tempo = 3h:\n#>  contrast estimate    SE df t.ratio p.value\n#>  CT - YOH  -1.4025 0.185 30  -7.582  <.0001\n#> \n#> Results are given on the log odds ratio (not the response) scale.\n## Gráfico\nescuridao <- summarySE(macho_preto, \n                        measurevar = \"preto\",\n                        groupvars = c(\"tempo\", \"tratamento\"))\n\n# Definir posição de linhas e pontos no gráfico\npd <- position_dodge(0.1)\n\nescuridao %>% \n  ggplot(aes(x = tempo, y = preto, colour = tratamento,\n             group = tratamento, fill = tratamento)) +\n  geom_errorbar(aes(ymin=preto-se, ymax=preto +se), \n                width=.1, size = 1.1, position=pd) +\n  geom_line(position=pd, size = 1.1) +\n  geom_point(pch = 21, colour = \"black\", position=pd, size=3.5) +\n  scale_colour_manual(values = c(\"darkorange\", \"cyan4\")) +\n  scale_fill_manual(values = c(\"darkorange\", \"cyan4\")) +\n  xlab(\"Tempo de experimento (horas)\") +\n  ylab(\"Índice de escuridão do corpo\") +\n  tema_livro()"},{"path":"cap8.html","id":"para-se-aprofundar-4","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.11 Para se aprofundar","text":"Neste capítulo apenas fizemos uma breve introdução aos modelos lineares generalizados. Para conhecer um pouco mais fundo todos os detalhes, recomendamos consulta dos livros - Zurr et al. (2009) Mixed effects models extensions ecology R e Pinheiro & Bates (Pinheiro Bates 2000) Mixed-Effects Models S S-PLUS - que são referências clássicas sobre GLM com aplicações em Ecologia. Para dados ordinais, sugerimos os livros - Agresti (2010) Analysis ordinal categorical data e Agresti (2012) Categorical Data Analysis.","code":""},{"path":"cap8.html","id":"exercícios-4","chapter":"Capítulo 8 Modelos Lineares Generalizados","heading":"8.12 Exercícios","text":"8.1\nBaixe estes dados que foram coletados numa pesquisa de opinião sobre uso de vídeo games por 91 estudantes de graduação outono de 1994. Utilizando estes dados, construa um modelo para: 1) Predizer frequência com que uma pessoa joga vídeo game em função da idade; 2) Predizer nota estudante em função tempo que jogou na semana antes da entrevista. Dica: esses são dados ordinais!8.2\nUma pesquisadora interessada em entender o efeito de diferentes usos de solo sobre abundância de morcegos em paisagens agroecológicas desenvolveu uma simulação para gerar dados parecidos com o que irá coletar em breve campo. Ela utilizou uma abordagem que simula abundância de uma espécie sob deriva ecológica segundo Teoria Neutra da Biodiversidade. simulação gerou os seguintes dados:Além disso, pesquisadora também simulou dados que representam porcentagem solo destinado à agricultura:Agora com esses dados ela pode ter uma idéia que esperar quando pra campo. Ajude pesquisadora construir um modelo linear generalizado que seja adequado para modelar abundância desta espécie de morcego em função da porcentagem de agricultura. Lembre-se de que ela vai precisar diagnosticar o modelo antes de utilizá-lo para fazer uma inferência. Por fim, interprete os resultados e sugira uma possível interpretação para pesquisadora.Soluções dos exercícios.","code":"\nset.seed(42)\nJ <- 5000 # número de indivíduos na comunidade local\ntheta <- 50 #número fundamental da biodiversidade\nm <- 0.05 #taxa de dispersão\ncomm1a <- coalesc(J, m, theta) #simulação\nabund1a <- abund(comm1a) #extração de dados de abundância local\nporc_solo <- rbeta(length(abund1a$com$ab), 10, 90) #simulação uso do solo para agricultura\n\ndados_finais <- data.frame(abund=abund1a$com$ab, solo=porc_solo)\nhead(dados_finais)\n#>   abund       solo\n#> 1   161 0.11063484\n#> 2    93 0.11498853\n#> 3   185 0.05718937\n#> 4    86 0.06134667\n#> 5    35 0.18910965\n#> 6    69 0.12660113"},{"path":"cap9.html","id":"cap9","chapter":"Capítulo 9 Análises Multidimensionais","heading":"Capítulo 9 Análises Multidimensionais","text":"","code":""},{"path":"cap9.html","id":"pré-requisitos-do-capítulo-5","chapter":"Capítulo 9 Análises Multidimensionais","heading":"Pré-requisitos do capítulo","text":"Pacotes e dados que serão utilizados neste capítulo.","code":"\n## Pacotes \nlibrary(ade4)\nlibrary(ecodados)\nlibrary(tidyverse)\nlibrary(vegan) \nlibrary(pvclust)\nlibrary(BiodiversityR)\nlibrary(labdsv)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(ape)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(FD)\nlibrary(palmerpenguins)\nlibrary(GGally)\nlibrary(fields)\nlibrary(ade4)\nlibrary(ggord)\nlibrary(udunits2)\nlibrary(adespatial)\nlibrary(spdep)\nlibrary(mvabund)\nlibrary(reshape)\n\n## Dados\nsp_compos        <- ecodados::bocaina\nspecies          <- ecodados::com_birds\nenv              <- ecodados::env_birds\nxy               <- ecodados::birds.xy\nbocaina.env      <- ecodados::bocaina.env\nbocaina.xy       <- ecodados::bocaina.xy\nanuros_permanova <- ecodados::anuros_permanova\nmacroinv         <- ecodados::macroinv\nfish_comm        <- ecodados::fish_comm\ndata(mite)\ndata(doubs)\ndata(mite.env)\n\n## Traduzir nomes para português \ncolnames(penguins) <- c(\"especies\", \"ilha\", \"comprimento_bico\", \"profundidade_bico\", \"comprimento_nadadeira\", \"massa_corporal\", \"sexo\", \"ano\")"},{"path":"cap9.html","id":"aspectos-teóricos","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.1 Aspectos teóricos","text":"Análises multivariadas avaliam hipóteses cuja variável resposta é definida por múltiplas variáveis ao mesmo tempo, comumente expressa na forma de uma matriz quadrada ou de distância (dissimilaridade).Em geral, análises multivariadas têm três principais utilidades: ) reduzir dimensionalidade dos dados e encontrar principal direção de variação dos mesmos, ii) testar relações entre matrizes, ou ainda, iii) encontrar diferenças entre grupos. Análises multivariadas podem ser utilizadas como análises exploratórias e/ou para descrever padrões em estudos ecológicos. entanto, mesmo quando se deseja apenas explorar o conjunto de dados para encontrar possíveis padrões, necessidade de se ter hipóteses, ou ao menos expectativas priori, não pode ser ignorada. Antes de entrar de cabeça nas análises multivariadas, também sugerimos fortemente o estudo de métodos de amostragem e como fazer boas perguntas (Capítulo 2).Análises multivariadas podem ser divididas, grosseiramente, em dois tipos: agrupamento e ordenação. Análises de agrupamento, em geral, tentam agrupar objetos (observações) ou descritores em grupos de maneira que objetos mesmo grupo sejam mais semelhantes entre si que objetos de outros grupos (Legendre Legendre 2012). Por exemplo, os objetos podem ser localidades como “parcelas”, “riachos” ou “florestas”, enquanto os descritores são diferentes variáveis coletadas para esses objetos (e.g., espécies, variáveis ambientais). análise de ordenação, por sua vez, é uma operação pela qual os objetos (ou descritores) são posicionados num espaço que contém menos dimensões que o conjunto de dados original; posição dos objetos ou descritores em relação aos outros também pode ser usada para agrupá-los.","code":""},{"path":"cap9.html","id":"coeficientes-de-associação","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.1.1 Coeficientes de associação","text":"Assim chamados genericamente, os coeficientes de associação medem o quão parecidos objetos ou descritores são entre si. Objetos estão nas linhas da matriz, enquanto descritores estão nas colunas. Geralmente objetos são nossas unidades amostrais, enquanto os descritores são variáveis. Quando analisamos relação entre objetos fazemos uma análise modo Q, ao passo que o modo R é quando analisamos relação entre descritores. Coeficientes de associação modo Q são medidas de (dis)similaridade ou distância, enquanto para o modo R utilizamos covariância ou correlação. Como já tratamos neste livro sobre covariância e correlação (ver Capítulo 7), neste tópico vamos falar sobre índices de distância e similaridade. Mas qual definição destas duas quantidades?Similaridade são máximas (S=1) quando dois objetos são idênticosDistâncias são o contrário da similaridade (D=1-S) e não têm limites superiores (dependem da unidade de medida)Existem ao menos 26 índices de similaridade que podem ser agrupados de acordo com o tipo de dado (qualitativos ou quantitativos) ou maneira com que lidam com duplos zeros (simétricos ou assimétricos) (Legendre Legendre 2012). seu lado, distâncias só se aplicam dados quantitativos e têm como características serem métricas, semi-métricas ou não-métricas. Vejamos agora os principais índices de similaridade e distância de cada tipo.","code":""},{"path":"cap9.html","id":"métricas-de-distância","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.1.2 Métricas de distância","text":"O principal coeficiente de distância usado em ecologia é distância euclidiana. Além disso, temos ainda Canberra (variação da Distância Euclidiana), Mahalanobis (calcula distância entre dois pontos num espaço não ortogonal, levando em consideração covariância entre descritores), Manhattan (variação da Distância Euclidiana), Chord (elimina diferenças entre abundância total de espécies), 𝜒2 (dá peso maior para espécies raras) e Hellinger (não dá peso para espécies raras). Essas distâncias são recomendadas nos casos em que variáveis de estudo forem contínuas, como por exemplo, variáveis morfométricas ou descritores ambientais.Uma característica comum de conjuntos de dados ecológicos são os vários zeros encontrados em matrizes de composição. Eles surgem porque não encontramos nenhum indivíduo de uma determinada espécie num local, seja porque aquele local não tem condições ambientais adequadas ela, falha na detectabilidade, ou dinâmicas demográficas estocásticas de colonização-extinção (Blasco‐Moreno et al. 2019). Logo, quando dois locais compartilham ausência de espécies, não é possível atribuir uma única razão da dupla ausência. Como essas medidas de distância apresentadas acima assumem que os dados são quantitativos e não de contagem, elas não são adequadas para lidar com dados de abundância ou incidência de espécies, porque atribuem um grau de parecença pares de locais que compartilham zeros (Legendre Legendre 2012). Por esse motivo, precisamos de coeficientes que desconsiderem os duplos zeros. Eles são chamados de assimétricos.Coeficientes assimétricos binários para objetosEsses coeficientes (ou índices) são apropriados para dados de incidência de espécies (presença-ausência) e desconsideram duplas ausências. Os índices deste tipo mais comuns utilizados em ecologia são Jaccard, Sørensen e Ochiai.O coeficiente de Jaccard é dado por:\\[\\beta_j = /(+b+c)\\]\nondea = número de espécies compartilhadasb = número de espécies exclusivas da comunidade 1c = número de espécies exclusivas da comunidade 2A diferença entre os índices de Jaccard e Sørensen é que o índice de Sørensen dá peso dobrado para duplas presenças. Por conta dessas características, estes índices são adequados para quantificar diversidade beta (M. J. Anderson et al. 2011; Legendre De Cáceres 2013). Esses índices variam entre 0 (nenhuma espécie é compartilhada entre o par de locais) 1 (todas espécies são compartilhadas entre o par de locais).O coeficiente de Sørensen é dado por:\\[\\beta_s = 2a/(2a+b+c)\\]\nondea = número de espécies compartilhadasb = número de espécies exclusivas da comunidade 1c = número de espécies exclusivas da comunidade 2Coeficientes binários para descritores (R mode)Se o objetivo calcular similaridade entre descritores binários (e.g., presença ou ausência de características ambientais) de pares de locais, geralmente o coeficiente recomendado é o de Sokal & Michener. Este índice está implementado na função dist.binary() pacote ade4.Coeficientes quantitativos para objetosEstes são os coeficientes utilizados para dados de contagem (e.g., abundância) e quantitativos (e.g., frequência, biomassa, porcentagem de cobertura). Diferentemente das distâncias, estes coeficientes são assimétricos, ou seja, não consideram duplas ausências e, portanto, são adequados para analisar dados de composição de espécies. Além disso, uma outra característica deles é serem semi-métricos. Os índices mais comuns deste tipo são Bray-Curtis (conhecido como percentage difference, em inglês), Chord, log-Chord, Hellinger, chi-quadrado e Morisita-Horn.Todos os índices discutidos até aqui estão implementados nas funções ade4::dist.ktab(), adespatial::dist.ldc() e vegan::vegdist().Coeficientes para descritores (R mode) que incluem mistura de tipos de dadosÉ comum em análises de diversidade funcional que tenhamos um conjunto de atributos (traits) de espécies que são formados por vários tipos de dados: quantitativos (e.g., tamanho de corpo), binários (presença/ausência de uma dada característica), fuzzy (um atributo multiestado codificado em várias colunas), ordinais e circulares (e.g., distribuição de uma fenofase ao longo de um ano). O índice que lida com todos esses dados é o Gower. versão estendida índice de Gower pode ser encontrada na função ade4::dist.ktab().O capítulo 7 de Legendre & Legendre (2012) fornece uma chave dicotômica para escolha índice mais adequado.Padronizações e transformaçõesÉ comum coletarmos múltiplas variáveis ambientais cujas unidades sejam diferentes. Por exemplo, temperatura (ºC), distância da margem (m), área (m2), etc. Para diminuir taxa de Erro Tipo das análises (rejeitar hipótese nula quando ela é verdadeira), é recomendado que padronizemos os dados utilizando distribuição Z, assim todas variáveis passam ter média 0 e desvio padrão 1. Essa operação garante que todas variáveis tenham o mesmo peso nas análises que avaliam o padrão dos objetos considerando os múltiplos descritores. Essa padronização pode ser implementada na função vegan::decostand().Outro problema comum de matrizes de dados de composição de espécies é o alto número de zeros, enquanto outras espécies podem ter altas abundâncias. Isso gera problemas em ordenações. Para diminuir essa discrepância, podemos transformar os dados, por exemplo, utilizando distância de Hellinger ou Chord. Para dados contínuos pode ser que transformação log ou raiz quadrada ajude quando há valores muito discrepantes (leverages) que podem influenciar em demasiado relação entre objetos ou descritores. Isso pode ser feito na função vegan::decostand().","code":""},{"path":"cap9.html","id":"análises-de-agrupamento","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.2 Análises de agrupamento","text":"O objetivo da análise de agrupamento é agrupar objetos admitindo que haja um grau de similaridade entre eles. Esta análise pode ser utilizada ainda para classificar uma população em grupos homogêneos de acordo com uma característica de interesse. grosso modo, uma análise de agrupamento tenta resumir uma grande quantidade de dados e apresentá-la de maneira fácil de visualizar e entender (em geral, na forma de um dendrograma). entanto, os resultados da análise podem não refletir necessariamente toda informação originalmente contida na matriz de dados. Para avaliar o quão bem uma análise de agrupamento representa os dados originais existe uma métrica — o coeficiente de correlação cofenético — o qual discutiremos em detalhes mais adiante.Antes de considerar algum método de agrupamento, pense porque você esperaria que houvesse uma descontinuidade nos dados; ou ainda, considere se existe algum ganho prático em dividir uma nuvem de objetos contínuos em grupos. O padrão apresentado pelo dendrograma depende protocolo utilizado (método de agrupamento e índice de dissimilaridade); os grupos formados dependem nível de corte escolhido.matriz deve conter os objetos serem agrupados (e.g., espécies) nas linhas e variáveis (e.g., locais de coleta ou medidas morfológicas) nas colunas. escolha método de agrupamento é crítica para escolha de um coeficiente de associação. É importante compreender propriedades dos métodos de agrupamento para interpretar corretamente estrutura ecológica que eles evidenciam (Legendre Legendre 2012). De acordo com classificação de Sneath & Sokal (1973), existem cinco tipos de métodos: ) sequenciais ou simultâneos, ii) aglomerativo ou divisivo, iii) monotéticos ou politéticos, iv) hierárquico ou não hierárquicos e v) probabilístico. Sugerimos leitura livro citado anteriormente para aprofundar seus conhecimentos sobre os diferentes métodos.","code":""},{"path":"cap9.html","id":"agrupamento-hierárquico","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.2.1 Agrupamento hierárquico","text":"Métodos hierárquicos podem ser divididos naqueles que consideram o centroide ou média aritmética entre os grupos. O principal método hierárquico que utiliza média aritmética é o UPGMA (Agrupamento pelas médias aritméticas não ponderadas), e o principal método que utiliza centroides é Distância mínima de Ward.O UPGMA funciona da seguinte forma: maior similaridade (ou menor distância) identifica os próximos agrupamentos serem formados. Após esse evento, o método calcula média aritmética das similaridades ou distâncias entre um objeto e cada um dos membros grupo ou, caso de um grupo previamente formado, entre todos os membros dos dois grupos. Todos os objetos recebem pesos iguais cálculo.O método de Ward é baseado critério de quadrados mínimos (OLS), o mesmo utilizado para ajustar um modelo linear (Capítulo 7). O objetivo é definir os grupos de maneira que soma de quadrados (.e. similar ao erro quadrado da ANOVA) dentro dos grupos seja minimizada (Borcard, Gillet, Legendre 2018).entanto, para interpretar os resultados precisamos antes definir um nível de corte, que vai nos dizer quantos grupos existem. Há vários métodos para definir grupos, desde os heurísticos aos que utilizam reamostragem (bootstrap). Se quisermos interpretar este dendrograma, podemos, por exemplo, estabelecer um nível de corte de 50% de distância (ou seja, grupos cujos objetos tenham ao menos 50% de similaridade entre si).ChecklistVerifique se não há espaço nos nomes das colunas e linhasVerifique se não há espaço nos nomes das colunas e linhasSe os dados forem de abundância, recomenda-se realizar transformação de Hellinger (Legendre Gallagher 2001). Esta transformação é necessária porque matriz de comunidades (em especial, com presença de muitas espécies raras) pode causar distorções nos métodos de ordenação baseados em distância Euclidiana (Legendre Gallagher 2001)Se os dados forem de abundância, recomenda-se realizar transformação de Hellinger (Legendre Gallagher 2001). Esta transformação é necessária porque matriz de comunidades (em especial, com presença de muitas espécies raras) pode causar distorções nos métodos de ordenação baseados em distância Euclidiana (Legendre Gallagher 2001)Se matriz original contiver muitos valores discrepantes (e.g., uma espécie muito mais ou muito menos abundante que outras) é necessário transformar os dados usando log1p(). entanto, deve-se fazer ou transformação de Hellinger ou logarítmica e nunca duas ao mesmo tempoSe matriz original contiver muitos valores discrepantes (e.g., uma espécie muito mais ou muito menos abundante que outras) é necessário transformar os dados usando log1p(). entanto, deve-se fazer ou transformação de Hellinger ou logarítmica e nunca duas ao mesmo tempoSe variáveis forem medidas tomadas em diferentes escalas (metros, graus celsius etc.), é necessário padronizar cada variável para ter média 0 e desvio padrão 1. Isso pode ser feito utilizando função decostand() pacote veganSe variáveis forem medidas tomadas em diferentes escalas (metros, graus celsius etc.), é necessário padronizar cada variável para ter média 0 e desvio padrão 1. Isso pode ser feito utilizando função decostand() pacote veganExemplo 1Neste exemplo, vamos utilizar um conjunto de dados que contém girinos de espécies de anuros coletados em 14 poças com diferentes coberturas de dossel (Provete et al. 2014).PerguntaExistem grupos de espécies de anfíbios anuros com padrões de ocorrência similar ao longo das poças?PrediçõesIremos encontrar ao menos dois grupos de espécies: aquelas que ocorrem em poças dentro de floresta (.e., maior cobertura de dossel) versus aquelas que ocorrem em poças de áreas abertas (menor cobertura de dossel)VariáveisVariáveis preditoras: matriz de dados contém abundância das espécies nas linhas e locais (poças) nas colunasAnálisesPara começar, vamos primeiro importar os dados e depois calcular matriz de distância que seja adequada para o tipo de dado que temos (abundância de espécies - dados de contagem) (Figura 9.1).\nFigura 9.1: Dendrograma mostrando uma análise de agrupamento de anuros.\nAvaliando qualidade dendrogramaPrecisamos verificar se o agrupamento reduziu dimensionalidade da matriz de forma eficiente, de maneira não distorcer informação. Fazemos isso calculando o Coeficiente de Correlação Cofenética que é uma medida que nos indica quão bem o resultado agrupamento corresponde às (dis)similaridades originais.Um coeficiente de correlação cofenética > .7 indica uma boa representação. Portanto, o nosso resultado de 0.9455221 é alto, garantindo que o dendrograma é adequado (Figura 9.2). Note que testar “significância” deste coeficiente não é adequado, já que seria algo tautológico (circular). Claro que definir o que seria um valor “alto” ou “baixo” da correlação é um pouco arbitrário, mas pode-se usar como “regra polegar”.\nFigura 9.2: Dendrograma mostrando uma análise de agrupamento de anuros com uma linha de corte formando cinco grupos.\nNesse caso teremos formação de cinco grupos, representados pelos nós que estão abaixo da linha de corte. Portanto, o resultado não suporta nossa hipótese priori que predizia formação de apenas dois grupos de espécies.Exemplo 2No exemplo anterior, vimos que é difícil interpretar os grupos baseado num nível de corte. seguir, vamos utilizar o pacote pvclust que calcula automaticamente o nível de corte de similaridade baseado Bootstrap de cada nó. Uma desvantagem deste método é que ele somente aceita índices de similaridade da função dist(), que possui apenas distância Euclidiana, Manhattan e Canberra. Uma maneira de contornarmos essa limitação é utilizar transformações dos dados disponíveis na função disttransform() pacote BiodiversityR ou função decostand() pacote vegan. Também é possível utilizar transformação de Box-Cox para dados multivariados, disponível material suplementar de Legendre & Borcard (2018). Esta transformação é geralmente utilizada para tornar distribuição dos dados mais simétrica (menos enviesada para valores extremos: reduzir o skewness dos dados).AnálisesVamos utilizar o mesmo conjunto de dados Exemplo 1 para responder à mesma pergunta. Aqui vamos utilizar distância de Chord (que é indicada para dados de composição de espécies) para calcular matriz de distância. Se transformarmos uma matriz usando transformação Chord e depois calcularmos distância Euclidiana, isso equivale à calcular diretamente distância de Chord (Figura 9.3).\nFigura 9.3: Dendrograma mostrando uma análise de agrupamento de anuros com uma linha de corte criada por bootstrap e usando distância de Chord.\nÉ possível notar que existe um único grupo com BS > 95%. Agora vamos tentar usar distância de Hellinger, que é recomendada (junto com distância de Chord) para transformar dados de composição de espécies (Legendre Gallagher 2001) (Figura 9.4).\nFigura 9.4: Dendrograma mostrando uma análise de agrupamento de anuros com uma linha de corte criada por bootstrap e usando distância de Hellinger.\nInterpretação dos resultadosNotem que se mudarmos o coeficiente de associação, o resultado também muda. Agora temos um grupo mais, composto por Dendropsophus minutus e Scinax duartei que não apareciam antes. Isso se deve ao fato de que distância de Hellinger dá menos peso para espécies raras que Chord.Neste sentido, os dados não suportam nossa hipótese inicial da formação de dois grupos, independentemente coeficiente de associação utilizado e cálculo automático nível de corte baseado na reamostragem.","code":"\n## Composição de espécies (seis primeiras localidades)\nhead(sp_compos)\n#>         BP4  PP4 PP3  AP1 AP2 PP1 PP2 BP9  PT1 PT2 PT3 BP2 PT5\n#> Aper      0    3   0    0   2   0   0   0    0   0   0 181   0\n#> Bahe    859   14  14    0  87 312 624 641    0   0   0  14   0\n#> Rict   1772 1517 207  573 796   0   0   0    0   0   0   0   0\n#> Cleuco    0    0   0    0   0   0   0   0    0  29 369   0  84\n#> Dmic      0    0   6   60   4   0   0   0 2758 319  25   0 329\n#> Dmin      0   84 344 1045  90   0   0   0    8   0   0   0   0\n\n## Matriz de similaridade com o coeficiente de Morisita-Horn\ndistBocaina <- vegdist(x = sp_compos, method = \"horn\")\n\n## Agrupamento com a função hclust e o método UPGMA\ndendro <- hclust(d = distBocaina, method = \"average\")\n\n## Visualizar os resultados\nplot(dendro, main = \"Dendrograma\", \n     ylab = \"Similaridade (índice de Horn)\",\n     xlab=\"\", sub=\"\")\n## Coeficiente de correlação cofenética\ncofresult <- cophenetic(dendro)\ncor(cofresult, distBocaina)\n#> [1] 0.9455221\n## Gráfico\nplot(dendro, main = \"Dendrograma\", \n     ylab = \"Similaridade (índice de Horn)\",\n     xlab=\"\", sub=\"\")\nk <- 4\nn <- ncol(sp_compos)\nMidPoint <- (dendro$height[n-k] + dendro$height[n-k+1]) / 2\nabline(h = MidPoint, lty=2)\n## Dados\nhead(t(sp_compos))\n#>     Aper Bahe Rict Cleuco Dmic Dmin Hpoly Lfur Pbar Polf Pmelano Sduar Shay Sobt Ssqual Bokerm\n#> BP4    0  859 1772      0    0    0    61    3  387    0       0     0    0    0      0      0\n#> PP4    3   14 1517      0    0   84   275    0  187    0       0  1150    6    0      0      1\n#> PP3    0   14  207      0    6  344   388    0    0    0       0   428    0    0      0      0\n#> AP1    0    0  573      0   60 1045  1054    0    0    0       0   476   92   13      0      0\n#> AP2    2   87  796      0    4   90  3002    0    0    2       0     7    0    5      0      0\n#> PP1    0  312    0      0    0    0   329    0    0    0       0     0    0    0      0      0\n\n## Passo 1: transformar para distância de Chord\nbocaina_transf <- disttransform(t(sp_compos), \"chord\")\n\n## Passo 2: realizar pvclust com método average e distância euclidiana\nanalise <- pvclust(bocaina_transf, method.hclust = \"average\", method.dist = \"euclidean\", quiet = TRUE) \n\n## Passo 3: dendrograma\nplot(analise, hang=-1, main = \"Dendrograma com valores de P\", \n     ylab = \"Distância Euclideana\",\n     xlab=\"\", sub=\"\")\npvrect(analise)\n## Passo 1: transformar dados com Hellinger\nbocaina_transf2 <- disttransform(t(bocaina), \"hellinger\")\n\n## Passo 2: realizar pvclust com método average e distância euclidiana\nanalise2 <- pvclust(bocaina_transf2, method.hclust=\"average\", method.dist=\"euclidean\", quiet = TRUE) \n\n## Passo 3: dendrograma\nplot(analise2, hang=-1, main = \"Dendrograma com valores de P\", \n     ylab = \"Distância Euclideana\",\n     xlab=\"\", sub=\"\")\nk <- 4\nn <- ncol(sp_compos)\nMidPoint <- (dendro$height[n-k] + dendro$height[n-k+1]) / 2\nabline(h = MidPoint, lty=2)\npvrect(analise2)"},{"path":"cap9.html","id":"agrupamento-não-hierárquico-k-means","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.2.2 Agrupamento não-hierárquico (K-means)","text":"Ao contrário dendrograma, o K-means é um agrupamento não-hierárquico e, desse modo, não é otimizado para buscar grupos menores aninhados em grupos maiores. Resumidamente, podemos calcular o K-means partir de uma matriz quadrada ou de distância. Essa técnica procura particionar os objetos em k grupos de maneira minimizar soma de quadrados entre grupos e maximizá-la dentro dos grupos. Um critério similar ao de uma ANOVA (Capítulo 7). Um diferencial K-means em relação aos agrupamentos hierárquicos é que o usuário pode escolher antecipadamente o número de grupos que deseja formar.Exemplo 1Para este exemplo, iremos utilizar um conjunto de dados disponível pacote ade4 que contém dados de 27 espécies de peixes coletados em 30 pontos ao longo Rio Doubs, na fronteira entre França e Suíça.PerguntaQual é o número de grupos que melhor sumariza o padrão de ocorrência de espécies de peixes ao longo de um riacho? 📝 Importante \nNeste caso, estamos realizando uma análise exploratória e não temos uma predição.VariáveisVariáveis resposta: composição de espécies de peixesChecklistVamos normalizar os dados de abundância antes de entrar na análise propriamente, já que existem muitos zeros na matrizAnálisesVamos iniciar selecionando e padronizando os dados.O argumento centers na função kmeans() indica o número de grupos que se quer formar. Neste exemplo, estamos utilizando centers = 4.O objeto que fornece o resultado contém: ) o tamanho (número de objetos) em cada um dos 4 grupos, ii) o centroide de cada grupo e o pertencimento de cada espécie cada grupo, e iii) o quanto da Soma de Quadrados dos dados é explicada por esta conformação de grupos.entanto, não é possível saber priori qual o número “ideal” de grupos. Para descobrir isso, repetimos o k-means com uma série de valores de K. Isso pode ser feito na função cascadeKM().Tanto calinski quando ssi são bons critérios para encontrar o número ideal de grupos. Quanto maior o valor de ssi, melhor (veja ?cascadeKM() mais detalhes). Os valores de ssi é o critério utilizado pelo algoritimo para achar o agrupamento ótimo dos objetos (Figura 9.5).\nFigura 9.5: Gráficos mostrando os resultados da análise de K-Means.\nInterpretação dos resultadosDiferentemente da nossa predição inicial, o resultado da análise mostra que o número ideal de grupos para explicar variância padrão de ocorrência de espécies é 3. Notem que o SSI máximo é alcançado neste número de grupos 0.1685126 (também indicado pela bola vermelha plot).","code":"\n## Mostrar somente seis primeiras espécies de seis localidades\nhead(doubs$fish)[,1:6]\n#>   Cogo Satr Phph Neba Thth Teso\n#> 1    0    3    0    0    0    0\n#> 2    0    5    4    3    0    0\n#> 3    0    5    5    5    0    0\n#> 4    0    4    5    5    0    0\n#> 5    0    2    3    2    0    0\n#> 6    0    3    4    5    0    0\n\n## Verificar se existem localidades sem nenhuma ocorrência\nrowSums(doubs$fish)\n#>  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \n#>  3 12 16 21 34 21 16  0 14 14 11 18 19 28 33 40 44 42 46 56 62 72  4 15 11 43 63 70 87 89\n\n## Retirar a linha 8 (rio sem nenhuma ocorrência de peixe)\nspe <- doubs$fish[-8,]\n\n## Função do pacote vegan para normalizar os dados\nspe.norm <- decostand(x = spe, method = \"normalize\") \n## K-Means\nspe.kmeans <- kmeans(x = spe.norm, centers = 4, nstart = 100)\nspe.kmeans\n#> K-means clustering with 4 clusters of sizes 8, 12, 6, 3\n#> \n#> Cluster means:\n#>         Cogo        Satr       Phph       Neba        Thth        Teso       Chna       Chto       Lele      Lece       Baba      Spbi       Gogo\n#> 1 0.00000000 0.006691097 0.02506109 0.06987391 0.006691097 0.006691097 0.10687104 0.09377516 0.14194394 0.2011411 0.24327992 0.1326062 0.28386032\n#> 2 0.10380209 0.542300691 0.50086515 0.43325916 0.114024105 0.075651573 0.00000000 0.00000000 0.06983991 0.1237394 0.02385019 0.0000000 0.05670453\n#> 3 0.06167791 0.122088022 0.26993915 0.35942538 0.032664966 0.135403325 0.06212775 0.21568957 0.25887226 0.2722562 0.15647062 0.1574388 0.16822286\n#> 4 0.00000000 0.000000000 0.00000000 0.00000000 0.000000000 0.000000000 0.05205792 0.00000000 0.07647191 0.3166705 0.00000000 0.0000000 0.20500174\n#>         Eslu       Pefl      Rham       Legi       Scer       Cyca       Titi       Abbr      Icme       Acce       Ruru       Blbj      Alal\n#> 1 0.20630360 0.16920496 0.2214275 0.19066542 0.13171275 0.16019126 0.26230024 0.19561641 0.1331835 0.26713081 0.32103755 0.22883055 0.3326939\n#> 2 0.04722294 0.02949244 0.0000000 0.00000000 0.00000000 0.00000000 0.03833408 0.00000000 0.0000000 0.00000000 0.01049901 0.00000000 0.0000000\n#> 3 0.12276089 0.17261621 0.0793181 0.06190283 0.04516042 0.06190283 0.14539027 0.01473139 0.0000000 0.03192175 0.32201597 0.01473139 0.1095241\n#> 4 0.07647191 0.00000000 0.0000000 0.05205792 0.07647191 0.00000000 0.00000000 0.00000000 0.0000000 0.18058775 0.31667052 0.05205792 0.7618709\n#>         Anan\n#> 1 0.18873077\n#> 2 0.00000000\n#> 3 0.04739636\n#> 4 0.00000000\n#> \n#> Clustering vector:\n#>  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \n#>  2  2  2  2  3  2  2  3  2  2  2  2  2  2  3  3  3  3  1  1  1  4  4  4  1  1  1  1  1 \n#> \n#> Within cluster sum of squares by cluster:\n#> [1] 0.4696535 2.5101386 1.7361453 0.3560423\n#>  (between_SS / total_SS =  66.7 %)\n#> \n#> Available components:\n#> \n#> [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\" \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n## Repetindo o K-Means\nspe.KM.cascade <- cascadeKM(spe.norm, inf.gr = 2, sup.gr = 10, iter = 100, criterion = \"ssi\") \n## Resumo dos resultados\nspe.KM.cascade$results\n#>      2 groups  3 groups  4 groups  5 groups   6 groups  7 groups  8 groups  9 groups 10 groups\n#> SSE 8.2149405 6.4768108 5.0719796 4.3015573 3.58561200 2.9523667 2.4840549 2.0521888 1.7599292\n#> ssi 0.1312111 0.1685126 0.1409061 0.1299662 0.08693436 0.1481826 0.1267918 0.1134307 0.1226392\n\n## Gráfico\nplot(spe.KM.cascade, sortg = TRUE)"},{"path":"cap9.html","id":"espécies-indicadoras","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.2.3 Espécies indicadoras","text":"Uma pergunta normalmente feita por ecólogos é: qual espécie pode ser indicadora de uma determinada condição ambiental (e.g., poluição)?O índice IndVal mede dois aspectos das espécies: fidelidade e especificidade. Uma alta fidelidade significa que espécies ocorrem em todos os locais grupo, e uma alta especificidade significa que espécies ocorrem somente naquele grupo. Uma boa espécie indicadora é aquela na qual todos os indivíduos ocorrem em todas amostras referentes um grupo específico. especificidade é dada pela divisão da abundância média da espécie grupo pela somatória das abundâncias médias dos grupos. Fidelidade é igual ao número de lugares grupo onde espécie está presente dividido pelo número total de lugares grupo (Dufrêne Legendre 1997).Espécies raras podem receber o mesmo valor de IndVal das espécies indicadoras, porém são chamadas de indicadoras assimétricas, uma vez que contribuem com especificidade habitat, mas não servem para predizer grupos. Ao contrário, espécies indicadoras são verdadeiros indicadores simétricos e podem ser usadas para predizer grupos.análise procede da seguinte forma:Uma matriz de distância é construída e unidades amostrais são classificadas com alguma análise de agrupamento, hierárquico ou nãoUma matriz de distância é construída e unidades amostrais são classificadas com alguma análise de agrupamento, hierárquico ou nãoA variável ambiental para qual se deseja classificar os grupos é inseridaA variável ambiental para qual se deseja classificar os grupos é inseridaAs espécies indicadoras de cada grupo são formadas através cálculo da especificidade e fidelidade, obtendo-se o valor de IndVal para cada espécieAs espécies indicadoras de cada grupo são formadas através cálculo da especificidade e fidelidade, obtendo-se o valor de IndVal para cada espéciePor fim, o conjunto de dados originais é comparado para ver se análise faz sentidoPor fim, o conjunto de dados originais é comparado para ver se análise faz sentidoO cálculo da significância índice de IndVal é feito por aleatorização de Monte Carlo. Os métodos de Monte Carlo utilizam números aleatórios de dados reais para simular certos padrões esperados na ausência de um processo ecológico específico (Legendre Legendre 2012). Assim, o valor índice é aleatorizado 999 vezes (ou o número de vezes que você optar) dentro dos tratamentos e o valor de P é dado pelo número de vezes em que o índice observado foi igual ou maior que os valores aleatorizados. Portanto, o IndVal fornece um conjunto de espécies que são indicadoras de um grupo de locais (e.g., muito poluídos), que por sua vez precisam ser definidos utilizando alguma técnica de agrupamento, como que vimos anteriormente.Exemplo 1Para este exemplo, vamos usar o mesmo conjunto de dados utilizado acima com abundância de 16 espécies de girinos coletados em 14 poças com diferentes graus de cobertura de dossel na Serra da Bocaina (Provete et al. 2014).PerguntaPodemos utilizar espécies de girinos como indicadoras da fitofisionomia?PrediçõesEspécies terrestres serão indicadoras de área aberta, enquanto espécies arborícolas serão indicadoras de áreas florestaisVariáveisVariáveis resposta: mesma matriz já utilizada contendo abundância de girinos ao longo de poças na Serra da BocainaAnálisesO IndVal está disponível tanto pacote indicspecies, quando labdsv. Para este exemplo, iremos usar o labdsv. Primeiro, vamos agrupar unidades amostrais (poças) que informa os grupos de fitofisionomias onde poças se localizam e para os quais deseja-se encontrar espécies indicadoras.Para apresentar uma tabela dos resultados para todas espécies temos de processar os dados.Interpretação dos resultadosNo resultado apresentado, podemos ver que temos duas espécies indicadoras da fitofisionimia 1: Rhinella icterica (Rict) e Scinax duartei (Sduar). Nenhuma espécie foi indicadora dos outros grupos neste exemplo.","code":"\n## Dados\nhead(bocaina)\n#>         BP4  PP4 PP3  AP1 AP2 PP1 PP2 BP9  PT1 PT2 PT3 BP2 PT5\n#> Aper      0    3   0    0   2   0   0   0    0   0   0 181   0\n#> Bahe    859   14  14    0  87 312 624 641    0   0   0  14   0\n#> Rict   1772 1517 207  573 796   0   0   0    0   0   0   0   0\n#> Cleuco    0    0   0    0   0   0   0   0    0  29 369   0  84\n#> Dmic      0    0   6   60   4   0   0   0 2758 319  25   0 329\n#> Dmin      0   84 344 1045  90   0   0   0    8   0   0   0   0\nfitofis <- c(rep(1, 4), rep(2, 4), rep(3, 4), rep(4, 4), rep(5, 4))\n\n## Análise de espécies indicadoras\nres_indval <- indval(t(sp_compos), fitofis)\n\n# A função summary só exibe o resultado para as espécies indicadoras\nsummary(res_indval)\n#>       cluster indicator_value probability\n#> Rict        1          0.8364       0.011\n#> Sduar       1          0.7475       0.034\n#> Bahe        2          0.6487       0.048\n#> \n#> Sum of probabilities                 =  7.984 \n#> \n#> Sum of Indicator Values              =  7.3 \n#> \n#> Sum of Significant Indicator Values  =  2.23 \n#> \n#> Number of Significant Indicators     =  3 \n#> \n#> Significant Indicator Distribution\n#> \n#> 1 2 \n#> 2 1\n## Resultados\ntab_indval <- cbind.data.frame(maxcls = res_indval$maxcls,\n                               ind.value = res_indval$indcls,\n                               P = res_indval$pval)\ntab_indval\n#>         maxcls ind.value     P\n#> Aper         3 0.2432796 1.000\n#> Bahe         2 0.6487329 0.048\n#> Rict         1 0.8363823 0.011\n#> Cleuco       3 0.4128631 0.385\n#> Dmic         3 0.6645244 0.195\n#> Dmin         1 0.7032145 0.101\n#> Hpoly        2 0.6208711 0.259\n#> Lfur         3 0.2279412 1.000\n#> Pbar         1 0.2813725 0.624\n#> Polf         3 0.2437500 1.000\n#> Pmelano      2 0.2500000 1.000\n#> Sduar        1 0.7474527 0.034\n#> Shay         3 0.4930269 0.416\n#> Sobt         2 0.2222222 0.683\n#> Ssqual       3 0.2500000 1.000\n#> Bokerm       2 0.4583333 0.228\n\n## Espécies\ntab_indval[tab_indval$P < 0.05, ]\n#>       maxcls ind.value     P\n#> Bahe       2 0.6487329 0.048\n#> Rict       1 0.8363823 0.011\n#> Sduar      1 0.7474527 0.034"},{"path":"cap9.html","id":"análises-de-ordenação","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.3 Análises de Ordenação","text":"análises de ordenação representam um conjunto de métodos e técnicas multivariadas que organizam objetos (e.g., localidades, indivíduos) em alguma ordem considerando o conjunto de descritores que podem estar mais ou menos relacionados entre si. Se os descritores estiverem bem relacionados, eles são redundantes e organizam os objetos de forma similar. Esse fenômeno é observado, por exemplo, quando queremos organizar um conjunto de unidades amostrais baseado em variáveis que indicam um mesmo processo: ver o padrão de similaridade geral de lagos usando várias variáveis relacionadas com produtividade sistema: clorofila-, concentração de fósforo, nitrogênio, entre outros. Por exemplo, tais métodos permitem identificar se existem grupo de espécies que ocorrem exclusivamente em um determinado hábitat. Ao buscar esta ordem, técnicas de ordenação possuem três principais utilidades: ) reduzir dimensionalidade e revelar padrões, ii) separar variáveis mais e menos importantes em combinações complexas e iii) separar relações mais e menos fortes ao comparar variáveis preditoras e dependentes.Em geral, os métodos são divididos em ordenações irrestritas (ou análise de gradiente indireto) e restritas (ou análise de gradiente direto). ordenações irrestritas organizam os objetos (e.g., espécies) de acordo com sua estrutura de covariância (ou correlação), o que demonstra que proximidade (ou distância) dentro espaço multidimensional representa semelhança (ou diferença) dos objetos. Por outro lado, ordenações restritras posicionam os objetos (e.g., espécies) de acordo com sua relação linear com outras variáveis coletadas nas mesmas unidades amostrais (e.g., temperatura e precipitação). Ao passo que ordenações irrestritas dependem somente de uma matriz (e.g., espécies por localidades), ordenações restritas utilizam mínimo duas matrizes (e.g., espécies por localidades e variáveis climáticas por localidade). Desse modo, fica claro esta diferença entre os dados utilizados que análises irrestritas são mais exploratórias, enquanto análises restritas são ideais para testar hipóteses com dados multidimensionais. tabela seguir apresenta principais análises utilizadas em ecologia.","code":""},{"path":"cap9.html","id":"ordenação-irrestrita","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.4 Ordenação irrestrita","text":"Ordenações irrestritas, ou análise de gradiente indireto ou ainda análises de fator, são um conjunto de métodos multivariados que lidam com uma única matriz quadrada. Esta matriz pode ou não ter pesos nas linhas ou colunas. Geralmente, o objetivo deste tipo de análise é resumir informação contida na matriz de maneira gráfica, por meio de um diagrama de ordenação. Quanto maior e mais complexa matriz, mais eficiente é este tipo de análise. Os tipos de análise irão diferir de acordo com o tipo de dado contido nesta matriz, se contínuo ou contagem, etc. De maneira geral, essas ordenações irrestritas calculam combinações lineares, cuja formulação irá diferir ligeiramente entre os métodos. Da mesma forma, essas combinações lineares irão preservar um tipo de distância. Por exemplo, Análise de Componentes Principais preserva distância Euclidiana, enquanto Análise de Correspondência preserva distância de chi-quadrado.","code":""},{"path":"cap9.html","id":"análise-de-componentes-principais-pca","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.4.1 Análise de Componentes Principais (PCA)","text":"Análise de Componentes Principais (Principal Component Analysis - PCA) é uma das ordenações mais utilizadas em diversas áreas conhecimento. Em Ecologia, ela se popularizou por facilitar visualização de dados complexos como de distribuição de espécies em diferentes localidades e de potenciais variáveis explicativas. Ao mesmo tempo que ganhou tamanha popularidade, PCA tem sido empregada de maneira incorreta, uma vez que muitos estudos utilizam visualização gráfica da ordenação (o biplot) para interpretar “relações” entre variáveis preditoras (ambientais) e dependentes (espécies). Porém, como informado anteriormente, ordenações irrestritas utilizam estrutura de covariância dos objetos para organizar suas relações de similaridade.Antes de explicar análise, imagine que vamos usar uma matriz com cinco espécies de aranhas que foram encontradas em oito cidades diferentes. quantidade de indivíduos de cada espécie coletada em cada cidade será o valor de preenchimento desta matriz. Sendo assim, matriz possui oito objetos (cidades, representando unidades amostrais) e cinco descritores (espécies).Tabela 9.1: Matriz com cinco espécies de aranhas que foram encontradas em oito cidades diferentes.O primeiro passo da PCA é obter uma matriz centralizada, onde cada valor é subtraído da média da coluna que aquele valor pertence. Esta centralização pode ser calculada com função scale().O segundo passo é calcular uma matriz de covariância (ou matriz de dispersão) e, partir desta matriz, obter os autovalores e autovetores. Os autovalores representam porcentagem de explicação de cada eixo e podem ser calculados dividindo soma autovalor de cada eixo pela soma de todos os autovalores. exemplo que apresentamos, os dois primeiros eixos representam 47,20% e 35,01% de toda variação, respectivamente. Os autovetores, por sua vez, representam os valores que multiplicam variáveis originais e, desse modo, indicam direção desses valores. Por fim, os componentes principais (Matriz F) são obtidos multiplicando os autovetores com os valores da matriz centralizada.Agora, é possível visualizar relação entre cidades e similaridade nas espécies de aranhas que vivem em cada uma delas (Figura 9.6).\nFigura 9.6: Biplot da PCA ordenando cidades pela composição das espécies de aranhas.\nChecklistVerifique se todas variáveis utilizadas são contínuas. Caso contrário, considere utilizar PCoA (veja mais próximo tópico)Verifique se todas variáveis utilizadas são contínuas. Caso contrário, considere utilizar PCoA (veja mais próximo tópico)Apesar exemplo acima ter apresentado ocorrência de espécies de aranhas em diferentes cidades, é fundamental saber que utilizar PCA com esses dados pode ser problemático. Assim, tenha cuidado em usar dados de composição de espécies, especialmente abundância, com PCA, uma vez que ‘duplos zeros’ podem gerar distorções na ordenação (Legendre Legendre 2012). Como alternativa, é possível utilizar PCA com dados padronizados com o método de Hellinger (Legendre Gallagher 2001).Apesar exemplo acima ter apresentado ocorrência de espécies de aranhas em diferentes cidades, é fundamental saber que utilizar PCA com esses dados pode ser problemático. Assim, tenha cuidado em usar dados de composição de espécies, especialmente abundância, com PCA, uma vez que ‘duplos zeros’ podem gerar distorções na ordenação (Legendre Legendre 2012). Como alternativa, é possível utilizar PCA com dados padronizados com o método de Hellinger (Legendre Gallagher 2001).Exemplo 1Neste exemplo vamos utilizar um conjunto de dados morfológicos de pinguins arquipélago Palmer (Península Antártica) disponíveis pacote palmerpenguins. Os dados representam medidas comprimento e largura bico (mm), comprimento da nadadeira (mm) e massa corporal (gramas) de três espécies: Adélie, Chinstrap e Gentoo. Como descrito acima, PCA deve ser utilizada para exploração de dados ou para testes posteriori (e.g., Regressão de Componentes Principais - PCR, tópico explorado mais frente nesse capítulo). Neste exemplo, iremos usar estrutura de perguntas e predições para manter proposta livro.PerguntaExiste diferenças nas características morfológicas das espécies de pinguins arquipélago Palmer?PrediçõesPinguins com dieta diferente possuem diferentes características morfológicasVariáveisPreditora: espécie (categórica com três níveis)Dependentes: variáveis morfológicas (contínua)AnálisesAntes de começar, é necessário remover dados ausentes (se houver) e editar nomes das variáveis (ponto importante para determinar como devem aparecer gráfico).Agora sim, os dados estão prontos para fazer PCA. Um argumento é essencial na análise, o scale.unit. Se você utilizar dentro deste argumento seleção TRUE, função padroniza automaticamente variáveis para terem média 0 e variância 1. Esta padronização é essencial quando variáveis estão em escalas muito diferentes. exemplo selecionado, temos variáveis como comprimento bico (em milímetros) e massa corporal (em gramas).Apesar da simplicidade código para executar PCA, o objeto resultante da análise possui diversas informações que são essenciais para sua plena interpretação. Dentre elas, se destacam os autovalores, escores e cargas (loadings). Os autovalores representam porcentagem de explicação de cada eixo. Os escores representam coordenadas (posições espaço multidimensional) representando os objetos (geralmente localidades ou indivíduos) e descritores (geralmente espécies ou variáveis ambientais e espaciais). Os loadings, por sua vez, representam combinação linear entre os escores (nova posição valor descritor espaço ordenado) e os valores originais dos descritores (Figura 9.7).\nFigura 9.7: Scree plot mostrando porcentagem de contribuição de cada eixo para ordenação dos dados de penguins.\nO pacote FactoMineR criou uma função (dimdesc()) que seleciona melhores variáveis (aquelas mais explicativas) para cada eixo através de uma análise fatorial. exemplo com pinguins, o primeiro eixo (objeto pca.p$eig) explica ~69% da variação morfológica. função dimdesc() mostra que quatro variáveis morfológicas estão fortemente associadas com o eixo 1. Porém, enquanto comprimento da nadadeira, massa corporal e comprimento bico estão positivamente associados com o eixo 1 (correlação positiva), largura bico tem relação negativa. O eixo 2, por sua vez, explica ~20% da variação, sendo relacionado somente com largura e comprimento bico.Agora podemos utilizar o famoso biplot para representar comparação morfológica dos pinguins dentro e entre espécies (Figura 9.8).\nFigura 9.8: Biplot da PCA ordenando os dados morfológicos de penguins.\n","code":"\n## Dados\naranhas <- data.frame(\n    sp1 = c(5, 7, 2, 0, 0, 0, 0, 0),\n    sp2 = c(0, 6, 3, 4, 0, 0, 0, 0),\n    sp3 = c(0, 0, 0, 9, 12, 3, 0, 0),\n    sp4 = c(0, 0, 0, 0, 4, 10, 8, 0),\n    sp5 = c(0, 0, 0, 0, 0, 6, 9, 12),\n    row.names = paste0(\"cidade\", 1:8))\n\n## Centralização\naranha.cent <- as.data.frame(base::scale(aranhas, center = TRUE, scale=FALSE))\n## Matriz de covaiância\nmatriz_cov <- cov(aranha.cent)\n\n## Autovalores e autovetores\neigen_aranhas <- eigen(matriz_cov)\nautovalores <- eigen_aranhas$values\nautovetores <- as.data.frame(eigen_aranhas$vectors)\nautovalores # eigenvalue\n#> [1] 36.733031 27.243824  9.443805  2.962749  1.438020\n\ncolnames(autovetores) <- paste(\"PC\", 1:5, sep=\"\")\nrownames(autovetores) <- colnames(aranhas)\nautovetores\n#>            PC1         PC2         PC3         PC4        PC5\n#> sp1 -0.2144766 -0.38855265 -0.29239380 -0.02330706  0.8467522\n#> sp2 -0.2442026 -0.17463316 -0.01756743  0.94587037 -0.1220204\n#> sp3 -0.3558368  0.80222917  0.27591770  0.10991178  0.3762942\n#> sp4  0.4159852  0.41786654 -0.78820962  0.17374202  0.0297183\n#> sp5  0.7711688 -0.01860152  0.46560957  0.25003826  0.3544591\n\n## Componentes principais\nmatriz_F <- as.data.frame(as.matrix(aranha.cent) %*% as.matrix(autovetores))\nmatriz_F\n#>               PC1        PC2        PC3        PC4        PC5\n#> cidade1 -2.979363 -4.4720575 -1.1533417 -3.2641923  0.5433206\n#> cidade2 -4.873532 -6.2969618 -1.8435339  2.3644158  1.5047024\n#> cidade3 -3.068541 -3.8302991 -0.3288626 -0.3566600 -2.3629973\n#> cidade4 -6.086322  3.9922356  2.7216169  1.6250305 -0.7918743\n#> cidade5 -4.513082  8.7689219  0.4668012 -1.1337476  0.9439633\n#> cidade6  5.812374  3.9444494 -3.9520584  0.4197281 -0.1376205\n#> cidade7  8.361421  0.6462243 -1.8065636  0.4926235 -0.2625625\n#> cidade8  7.347046 -2.7525126  5.8959421 -0.1471979  0.5630683\n\n## Porcentagem de explicação de cada eixo\n100 * (autovalores/sum(autovalores))\n#> [1] 47.201691 35.008126 12.135225  3.807112  1.847846\n## Gráfico\nggplot(matriz_F, aes(x = PC1, y = PC2, label = rownames(matriz_F))) +\n    geom_label() + \n    geom_hline(yintercept = 0, linetype=2) +\n    geom_vline(xintercept = 0, linetype=2) +\n    xlim(-10, 10) +\n    tema_livro()\n## Verificar se existem NAs nos dados\nsum(is.na(penguins))\n#> [1] 19\n\n## Remover dados ausentes (NA), quando houver\npenguins <- na.omit(penguins)\n\n## Manter somentes dados contínuos que pretende aplicar a PCA\npenguins_trait <- penguins[, 3:6]\n## Compare com este código a variância das variáveis\npenguins_trait %>% \n    dplyr::summarise(across(where(is.numeric), \n                            ~var(.x, na.rm = TRUE)))\n#> # A tibble: 1 × 4\n#>   comprimento_bico profundidade_bico comprimento_nadadeira massa_corporal\n#>              <dbl>             <dbl>                 <dbl>          <dbl>\n#> 1             29.9              3.88                  196.        648372.\n\n## Agora, veja o mesmo cálculo se fizer a padronização (scale.unit da função PCA)\npenguins_pad <- decostand(x = penguins_trait, method = \"standardize\")\npenguins_pad %>% \n    dplyr::summarise(across(where(is.numeric), \n                            ~var(.x, na.rm = TRUE)))\n#>   comprimento_bico profundidade_bico comprimento_nadadeira massa_corporal\n#> 1                1                 1                     1              1\n\n## PCA\npca.p <- PCA(X = penguins_trait, scale.unit = TRUE, graph = FALSE)\n## Autovalores: porcentagem de explicação para usar no gráfico\npca.p$eig \n#>        eigenvalue percentage of variance cumulative percentage of variance\n#> comp 1  2.7453557              68.633893                          68.63389\n#> comp 2  0.7781172              19.452929                          88.08682\n#> comp 3  0.3686425               9.216063                          97.30289\n#> comp 4  0.1078846               2.697115                         100.00000\n\n## Visualização da porcentagem de explicação de cada eixo\n# nota: é necessário ficar atento ao valor máximo do eixo 1 da análise para determinar o valor do ylim (neste caso, colocamos que o eixo varia de 0 a 70).\nfviz_screeplot(pca.p, addlabels = TRUE, ylim = c(0, 70), main = \"\", \n               xlab = \"Dimensões\",\n               ylab = \"Porcentagem de variância explicada\") \n## Outros valores importantes\nvar_env <- get_pca_var(pca.p)\n\n## Escores (posição) das variáveis em cada eixo\nvar_env$coord \n#>                            Dim.1      Dim.2      Dim.3       Dim.4\n#> comprimento_bico       0.7518288 0.52943763 -0.3900969 -0.04768208\n#> profundidade_bico     -0.6611860 0.70230869  0.2585287  0.05252186\n#> comprimento_nadadeira  0.9557480 0.00510580  0.1433474  0.25684871\n#> massa_corporal         0.9107624 0.06744932  0.3592789 -0.19204478\n\n## Contribuição (%) das variáveis para cada eixo\nvar_env$contrib \n#>                          Dim.1        Dim.2     Dim.3     Dim.4\n#> comprimento_bico      20.58919 36.023392267 41.279994  2.107420\n#> profundidade_bico     15.92387 63.388588337 18.130600  2.556942\n#> comprimento_nadadeira 33.27271  0.003350291  5.574092 61.149849\n#> massa_corporal        30.21423  0.584669105 35.015313 34.185789\n\n## Loadings - correlação das variáveis com os eixos\nvar_env$cor \n#>                            Dim.1      Dim.2      Dim.3       Dim.4\n#> comprimento_bico       0.7518288 0.52943763 -0.3900969 -0.04768208\n#> profundidade_bico     -0.6611860 0.70230869  0.2585287  0.05252186\n#> comprimento_nadadeira  0.9557480 0.00510580  0.1433474  0.25684871\n#> massa_corporal         0.9107624 0.06744932  0.3592789 -0.19204478\n\n## Qualidade da representação da variável. Esse valor é obtido multiplicado var_env$coord por var_env$coord\nvar_env$cos2\n#>                           Dim.1        Dim.2      Dim.3       Dim.4\n#> comprimento_bico      0.5652466 2.803042e-01 0.15217561 0.002273581\n#> profundidade_bico     0.4371669 4.932375e-01 0.06683710 0.002758546\n#> comprimento_nadadeira 0.9134542 2.606919e-05 0.02054847 0.065971260\n#> massa_corporal        0.8294881 4.549411e-03 0.12908133 0.036881196\n\n## Escores (posição) das localidades (\"site scores\") em cada eixo \nind_env <- get_pca_ind(pca.p)\n## Variáveis mais importantes para o Eixo 1\ndimdesc(pca.p)$Dim.1 \n#> $quanti\n#>                       correlation       p.value\n#> comprimento_nadadeira   0.9557480 5.962756e-178\n#> massa_corporal          0.9107624 3.447018e-129\n#> comprimento_bico        0.7518288  7.830597e-62\n#> profundidade_bico      -0.6611860  3.217695e-43\n#> \n#> attr(,\"class\")\n#> [1] \"condes\" \"list\"\n\n## Variáveis mais importantes para o Eixo 2\ndimdesc(pca.p)$Dim.2 \n#> $quanti\n#>                   correlation      p.value\n#> profundidade_bico   0.7023087 8.689230e-51\n#> comprimento_bico    0.5294376 1.873918e-25\n#> \n#> attr(,\"class\")\n#> [1] \"condes\" \"list\"\nfviz_pca_biplot(X = pca.p, \n                geom.ind = \"point\", \n                fill.ind = penguins$especies, \n                col.ind = \"black\",\n                alpha.ind = 0.7,\n                pointshape = 21, \n                pointsize = 4,\n                palette = c(\"darkorange\", \"darkorchid\", \"cyan4\"),\n                col.var = \"black\",\n                invisible = \"quali\",\n                title = NULL) +\n    labs(x = \"PC1 (68.63%)\", y = \"PC2 (19.45%)\") + \n    xlim(c(-4, 5)) +\n    ylim(c(-3, 3)) +\n    tema_livro()"},{"path":"cap9.html","id":"análises-de-coordenadas-principais-pcoa","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.4.2 Análises de Coordenadas Principais (PCoA)","text":"Diferentemente da PCA, Análises de Coordenadas Principais (Principal Coordinate Analysis - PCoA) é uma análise de ordenação irrestrita que aceita dados de diferentes tipos, como contínuos, categóricos, ordinais, binários, entre outros. Assim, PCoA é aplicada para casos em que distância euclidiana não é aplicada (como na PCA). Desse modo, o primeiro passo da análise é calcular uma matriz de similaridade ou de distância (discutido acima). Depois, os passos para obter autovalores e autovetores são bastante parecidos com PCA. Da mesma forma, os eixos da PCoA e os valores ou posições dos objetos nesses eixos representam relação de semelhança (ou diferença) baseada nos descritores desses objetos. diferença, neste caso, é que PCoA representa um espaço não-euclidiano, que irá ser afetado pela escolha método de similaridade.utilizações mais comuns da PCoA são ordenação: ) da matriz de composição de espécies usando distância apropriada (Jaccard, Sorensen, Bray-Curtis), ii) da matriz de variáveis ambientais com mistos (contínuos, categóricos, circulares, etc.), e iii) da matriz filogenética, método PVR (Diniz-Filho, de Sant’Ana, Bini 1998). Abaixo, exemplificamos ordenação da matriz de composição de espécies.ChecklistCompare dimensões das matrizes utilizadas para PCoA. Com bastante frequência, tentativa de combinar dados categóricos (algum descritor dos objetos) com os valores obtidos com PCoA gera erros para plotar figura ou para executar análise. Verifique, então, se linhas são mesmas (nome das localidades ou indivíduos e quantidade)Compare dimensões das matrizes utilizadas para PCoA. Com bastante frequência, tentativa de combinar dados categóricos (algum descritor dos objetos) com os valores obtidos com PCoA gera erros para plotar figura ou para executar análise. Verifique, então, se linhas são mesmas (nome das localidades ou indivíduos e quantidade)É fundamental conhecer o tipo de dados que está usando para selecionar medida de distância apropriada. Essa escolha vai afetar qualidade da ordenação e sua habilidade para interpretar relação de semelhança entre os objetos comparadosÉ fundamental conhecer o tipo de dados que está usando para selecionar medida de distância apropriada. Essa escolha vai afetar qualidade da ordenação e sua habilidade para interpretar relação de semelhança entre os objetos comparadosDiferente da PCA, PCoA aceita dados ausentes se medida de distância escolhida também não tiver esta limitação. Por exemplo, distância de Gower produz matrizes de similaridade mesmo com dados ausentes em determinados objetosDiferente da PCA, PCoA aceita dados ausentes se medida de distância escolhida também não tiver esta limitação. Por exemplo, distância de Gower produz matrizes de similaridade mesmo com dados ausentes em determinados objetosEm alguns casos, autovalores negativos são produzidos na ordenação com PCoA. Veja principais causas desses valores em Legendre & Legendre (2012). Apesar deste problema, os autovalores mais importantes (eixos iniciais) não são afetados e, deste modo, qualidade da representação dos objetos espaço multidimensional não é afetada. Alguns autores sugerem utilizar correções métodos de correção, como Lingoes ou Cailliez (Legendre Legendre 2012).Em alguns casos, autovalores negativos são produzidos na ordenação com PCoA. Veja principais causas desses valores em Legendre & Legendre (2012). Apesar deste problema, os autovalores mais importantes (eixos iniciais) não são afetados e, deste modo, qualidade da representação dos objetos espaço multidimensional não é afetada. Alguns autores sugerem utilizar correções métodos de correção, como Lingoes ou Cailliez (Legendre Legendre 2012).Exemplo 1Neste exemplo, vamos utilizar composição de ácaros Oribatidae em 70 manchas de musgo coletados por Borcard et al. (1992).PerguntaA composição de espécies de ácaros muda entre diferentes topografias?PrediçõesIremos encontrar ao menos dois grupos de espécies: aquelas que ocorrem em poças dentro de floresta versus aquelas que ocorrem em poças de áreas abertasVariáveisPreditora: topografia (categórica com dois níveis)Dependentes: composição de espécies de ácaroAnálisesVamos primeiramente padronizar dos dados, calcular uma matriz de distância com método Bray-Curtis e depois calcular PCoA.Assim como na PCA, porcentagem de explicação dos eixos é uma das informações mais importantes pois descrevem efetividade da redução da dimensionalidade dos dados.Para visualizar os resultados da PCoA, vamos exportar os escores dos eixos para usar pacote ggplot2 (Figura 9.9).\nFigura 9.9: Biplot da PCoA ordenando espécies de ácaros entre diferentes topografias.\nLimitações importantes das ordenações irrestritasCom frequência, pesquisadores utilizam análises como PCA e PCoA para “testar” diferenças na composição de espécies entre determinados fatores relevantes (altitude, clima, etc.). Porém, como falado acima, análises de ordenação irrestritas não são utilizadas para testar qualquer hipótese. Ao invés disso, essas análises representam uma poderosa ferramente para explorar padrões em variáveis dependentes ou independentes para ajudar na interpretação ou mesmo para testar hipóteses em análises combinadas com ordenações restritas.","code":"\n## Padronização dos dados com Hellinger\nmite.hel <- decostand(x = mite, method = \"hellinger\") \n\n## Cálculo da matriz de distância com método Bray-Curtis\nsps.dis <- vegdist(x = mite.hel, method = \"bray\") \n\n## PCoA\npcoa.sps <- pcoa(D = sps.dis, correction = \"cailliez\")\n## Porcentagem de explicação do Eixo 1\n100 * (pcoa.sps$values[, 1]/pcoa.sps$trace)[1]\n#> [1] 49.10564\n\n## Porcentagem de explicação dos Eixo 2\n100 * (pcoa.sps$values[, 1]/pcoa.sps$trace)[2]\n#> [1] 14.30308\n\n## Porcentagem de explicação acumulada dos dois primeiros eixos \nsum(100 * (pcoa.sps$values[, 1]/pcoa.sps$trace)[1:2])\n#> [1] 63.40872\n\n## Selecionar os dois primeiros eixos\neixos <- pcoa.sps$vectors[, 1:2]\n\n## Juntar com algum dado categórico de interesse para fazer a figura\npcoa.dat <- data.frame(topografia = mite.env$Topo, eixos)\n## Escores dos dois primeiros eixos\neixos <- pcoa.sps$vectors[, 1:2] \n\n## Combinar dados dos escores com um dado categórico de interesse para nossa pergunta\npcoa.dat <- data.frame(topografia = mite.env$Topo, eixos)\n\n### Gráfico biplot da PCoA\nggplot(pcoa.dat, aes(x = Axis.1, y = Axis.2, fill = topografia, \n                     color = topografia, shape = topografia)) +\n    geom_point(size = 4, alpha = 0.7) + \n    scale_shape_manual(values = c(21, 22)) + \n    scale_color_manual(values = c(\"black\", \"black\")) + \n    scale_fill_manual(values = c(\"darkorange\", \"cyan4\")) + \n    labs(x = \"PCO 1 (49.11%)\", y = \"PCO 2 (14.30%)\") + \n    geom_hline(yintercept = 0, linetype = 2) + \n    geom_vline(xintercept = 0, linetype = 2) +\n    tema_livro()"},{"path":"cap9.html","id":"regressão-de-componentes-principais-pcr","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.4.3 Regressão de Componentes Principais (PCR)","text":"Uma maneira de testar hipóteses utilizando ordenações irrestritas é utilizando os resultados da ordenação (escores) como variáveis preditoras ou dependentes como, por exemplo, em modelos lineares (e.g., regressão múltipla Capítulo 7). O primeiro passo é utilizar uma ordenação, como PCA, para gerar os “novos” dados que serão usados na análise. utilização desses novos dados (que representam coordenadas principais ou escores da PCA) vai depender da pergunta em questão. Por exemplo, pode ser que esses valores representem gradientes climáticos e, por este motivo, serão utilizados como variáveis preditoras em um modelo linear (e.g., regressão múltipla). Por outro lado, esses valores podem representar o espaço morfológicos de espécies de peixes e, como consequência, serão utilizados como variáveis dependentes para entender o efeito da presença de predador sobre morfologia. É importante ressaltar que existem algumas limitações importantes na PCR como, por exemplo, coordenadas principais (escores da PCA) utilizadas como variáveis preditoras podem não representar, biologicamente, mais importantes para explicar variação na variável resposta (Hadi Ling 1998).ChecklistCompare dimensões das matrizes utilizadas para PCR. Com bastante frequência, tentativa de combinar dados categóricos (algum descritor dos objetos) com os valores obtidos com PCoA gera erros para plotar figura ou para executar análise. Verifique, então, se linhas são mesmas (nome das localidades ou indivíduos e quantidade)Compare dimensões das matrizes utilizadas para PCR. Com bastante frequência, tentativa de combinar dados categóricos (algum descritor dos objetos) com os valores obtidos com PCoA gera erros para plotar figura ou para executar análise. Verifique, então, se linhas são mesmas (nome das localidades ou indivíduos e quantidade)Estudos recentes têm criticado utilização de PCR para testar hipóteses ecológicas pelo fato dos escores não representarem, necessariamente, variação total das variáveis originais, bem como relação entre variável preditora e dependenteEstudos recentes têm criticado utilização de PCR para testar hipóteses ecológicas pelo fato dos escores não representarem, necessariamente, variação total das variáveis originais, bem como relação entre variável preditora e dependenteExemplo 1Neste exemplo, vamos utilizar composição de espécies de aves em 23 regiões dos alpes franceses. Os dados ambientais (env) representam variáveis climáticas (temperatura e chuva) e altitude.PerguntaGradientes climáticos afetam riqueza de aves?PrediçõesO aumento da umidade e redução da temperatura aumentam o número de espécies de avesVariáveisPreditora: temperatura e chuva (contínuas) e altitude (categórica com três níveis)Dependentes: riqueza de espécies de avesO objeto env.pca$eig demonstra que os três primeiros eixos explicam 94.54% da variação total dos dados climáticos. O intuito da PCR é reduzir dimensionalidade, ou seja, o número de variáveis preditoras ou dependentes para facilitar interpretação e garantir que variáveis não sejam correlacionadas. O próximo passo então é obter os valores dos escores que representam os valores convertidos para serem usados em uma determinada análise, como regressão múltipla.Agora que os dados foram combinados em uma única data frame, podemos utilizar os códigos apresentados Capítulo 7 para testar nossa hipótese (Figura 9.10).\nFigura 9.10: Diagnósticos dos modelo PCR para aves.\nComo percebemos, Dim.1 foi o único gradiente ambiental que afetou riqueza de espécies. Para interpretar esta dimensão (e outras importantes), podemos usar função dimdesc() para verificar variáveis mais importantes. Neste caso, os valores mais extremos de correlação (maior que 0.8) indicam que temperatura mês de janeiro e julho bem como chuva mês de julho foram variáveis mais importantes para determinar o gradiente ambiental expresso na dimensão 1. Assim, podemos fazer um gráfico para representar relação entre Eixo 1 (gradiente chuva-temperatura) e riqueza de espécies de aves. Valores negativos eixo 1 (Gradiente ambiental - PC1) representam localidades com mais chuva, ao passo que valores positivos indicam localidades com temperaturas maiores (Figura 9.11).\nFigura 9.11: Modelo linear representando relação entre Eixo 1 (gradiente chuva-temperatura) e riqueza de espécies de aves.\nExemplo 2É possível que os dados utilizados em seu estudo sejam mistos, ou seja, incluem tanto variáveis categóricas quanto contínuas. Como falado acima, nesses casos análise indicada é PCoA. Assim como na PCA, podemos extrair os escores da PCoA para utilizar em análises univariadas e multivariadas.PerguntaVariáveis climáticas, vegetacionais e topográficas afetam riqueza de ácaros?PrediçõesA densidade da vegetação e disponibilidade de água aumentam riqueza de espécies de ácarosVariáveisPreditoras: densidade de substrato e disponibilidade de água (contínuas), tipo de substrato (categórica com 7 níveis), densidade arbusto (ordinal com 3 níveis), e topografia (categórica com 2 níveis)Dependentes: riqueza de espécies de ácarosO primeiro passo então é utilizar um método de distância apropriado para o seu conjunto de dados. Em nosso exemplo, utilizaremos distância de Gower, que é usada para dados mistos (veja Capítulo 14).O próximo passo é exportar os escores para análises posteriori (Figura 9.12).\nFigura 9.12: Diagnósticos dos modelo PCR para ácaros.\nFinalmente, após interpretar os resultados modelo, podemos fazer figura com variáveis (eixos) importantes (Figura 9.13).\nFigura 9.13: Modelo linear representando relação entre Eixo 1 e Eixo 2 e riqueza de espécies de ácaros.\n","code":"\n## Dados\nenv_cont <- env[,-8]\nenv.pca <- PCA(env_cont, scale.unit = TRUE, graph = FALSE)\nvar_env <- get_pca_var(env.pca) \n\n## Contribuição (%) das variáveis para cada eixo\nvar_env$contrib \n#>             Dim.1      Dim.2      Dim.3      Dim.4       Dim.5\n#> mini.jan 10.93489 22.2975487 16.1607726  7.6025527  0.01782438\n#> maxi.jan 20.18065  3.2890767  2.1814486  4.2756350 41.05646526\n#> mini.jul 11.87396 21.1379132  0.3428843  0.7750666 44.70209396\n#> maxi.jul 18.47244  0.9159957 56.5369988  9.4368661  2.59283074\n#> rain.jan  9.95206 21.5387403  6.5737927 53.7375738  4.44283706\n#> rain.jul 16.14997 11.2368132  7.2608047 19.6972097  0.71454880\n#> rain.tot 12.43603 19.5839121 10.9432983  4.4750959  6.47339980\n\n## Loadings - correlação das variáveis com os eixos\nvar_env$cor \n#>               Dim.1     Dim.2       Dim.3       Dim.4        Dim.5\n#> mini.jan  0.6830371 0.6766524 -0.21924927  0.12298817 -0.004517369\n#> maxi.jan  0.9279073 0.2598807 -0.08055260  0.09223249  0.216804944\n#> mini.jul  0.7117620 0.6588220  0.03193603 -0.03926930 -0.226225907\n#> maxi.jul  0.8877675 0.1371462  0.41008461 -0.13702428  0.054483561\n#> rain.jan -0.6516187 0.6650391 -0.13983474 -0.32698110  0.071319550\n#> rain.jul -0.8300858 0.4803509  0.14696011  0.19796389 -0.028601865\n#> rain.tot -0.7284135 0.6341424  0.18041856  0.09435932  0.086088397\nind_env <- get_pca_ind(env.pca)\nenv.pca$eig \n#>        eigenvalue percentage of variance cumulative percentage of variance\n#> comp 1 4.26652359             60.9503370                          60.95034\n#> comp 2 2.05340251             29.3343216                          90.28466\n#> comp 3 0.29745014              4.2492878                          94.53395\n#> comp 4 0.19896067              2.8422953                          97.37624\n#> comp 5 0.11448717              1.6355310                          99.01177\n#> comp 6 0.04312874              0.6161248                          99.62790\n#> comp 7 0.02604718              0.3721025                         100.00000\n## Passo 1: obter os primeiros eixos \npred.env <- ind_env$coord[, 1:3] \n\n## Passo 2: calcular a riqueza de espécies\nriqueza <- specnumber(species)\n\n## Passo 3: combinar os dois valores em um único data.frame\ndat <- data.frame(pred.env, riqueza) \n## Regressão múltipla\nmod1 <- lm(riqueza ~ Dim.1 + Dim.2 + Dim.3, data = dat)\npar(mfrow = c(2, 2))\nplot(mod1) # verificar pressupostos dos modelos lineares\nsummary(mod1) # resultados do  teste\n#> \n#> Call:\n#> lm(formula = riqueza ~ Dim.1 + Dim.2 + Dim.3, data = dat)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.4008 -1.1729  0.4356  1.2072  2.4571 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 13.30435    0.37639  35.347  < 2e-16 ***\n#> Dim.1        0.68591    0.18222   3.764  0.00131 ** \n#> Dim.2       -0.09961    0.26267  -0.379  0.70874    \n#> Dim.3       -0.21708    0.69014  -0.315  0.75654    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.805 on 19 degrees of freedom\n#> Multiple R-squared:  0.4313, Adjusted R-squared:  0.3415 \n#> F-statistic: 4.804 on 3 and 19 DF,  p-value: 0.01179\n\ndimdesc(env.pca)$Dim.1 \n#> $quanti\n#>          correlation      p.value\n#> maxi.jan   0.9279073 1.846790e-10\n#> maxi.jul   0.8877675 1.607390e-08\n#> mini.jul   0.7117620 1.396338e-04\n#> mini.jan   0.6830371 3.282701e-04\n#> rain.jan  -0.6516187 7.559358e-04\n#> rain.tot  -0.7284135 8.112903e-05\n#> rain.jul  -0.8300858 9.588034e-07\n#> \n#> attr(,\"class\")\n#> [1] \"condes\" \"list\"\nggplot(dat, aes(x = Dim.1, y = riqueza)) + \n    geom_smooth(method = lm, fill = \"#525252\", color = \"black\") + \n    geom_point(size = 4, shape = 21, alpha = 0.7, color = \"#1a1a1a\", fill = \"cyan4\") +\n    labs(x = \"Gradiente ambiental (PC1)\", y = \"Riqueza de aves\") + \n    tema_livro()\n## Matriz de distância\nenv.dist <- gowdis(mite.env)\n\n## PCoA\nenv.mite.pco <- pcoa(env.dist, correction = \"cailliez\")\n\n## Porcentagem de explicação do Eixo 1\n100 * (env.mite.pco$values[, 1]/env.mite.pco$trace)[1]\n#> [1] 61.49635\n\n## Porcentagem de explicação dos Eixo 2\n100 * (env.mite.pco$values[, 1]/env.mite.pco$trace)[2]\n#> [1] 32.15486\n## Selecionar os dois primeiros eixos\npred.scores.mite <- env.mite.pco$vectors[, 1:2] \n\n## Juntar com os dados da área para fazer a figura \nmite.riqueza <- specnumber(mite)\npred.vars <- data.frame(riqueza = mite.riqueza, pred.scores.mite)\n\n### Regressão múltipla \nmod.mite <- lm(riqueza ~ Axis.1 + Axis.2, data = pred.vars)\npar(mfrow = c(2, 2))\nplot(mod.mite)\nsummary(mod.mite)\n#> \n#> Call:\n#> lm(formula = riqueza ~ Axis.1 + Axis.2, data = pred.vars)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -10.6874  -2.3960  -0.1378   2.5032   8.6873 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  15.1143     0.4523  33.415  < 2e-16 ***\n#> Axis.1      -11.4303     2.0013  -5.711  2.8e-07 ***\n#> Axis.2        5.6832     2.7677   2.053   0.0439 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.784 on 67 degrees of freedom\n#> Multiple R-squared:  0.3548, Adjusted R-squared:  0.3355 \n#> F-statistic: 18.42 on 2 and 67 DF,  p-value: 4.225e-07\ng_acari_axi1 <- ggplot(pred.vars, aes(x = Axis.1, y = riqueza)) + \n    geom_smooth(method = lm, fill = \"#525252\", color = \"black\") + \n    geom_point(size = 4, shape = 21, alpha = 0.7, color = \"#1a1a1a\", fill=\"cyan4\") + \n    labs(x = \"Gradiente ambiental (PC1)\", y = \"Riqueza de ácaros\") + \n    tema_livro()\n\ng_acari_axi2 <- ggplot(pred.vars, aes(x = Axis.2, y = riqueza)) + \n    geom_smooth(method = lm, fill = \"#525252\", color = \"black\") + \n    geom_point(size = 4, shape = 21, alpha = 0.7, color = \"#1a1a1a\", fill = \"darkorange\") + \n    labs(x = \"Gradiente ambiental (PC2)\", y = \"Riqueza de ácaros\") + \n    tema_livro()\n\n## Função para combinar os dois plots em uma única janela\ngrid.arrange(g_acari_axi1, g_acari_axi2, nrow = 1)"},{"path":"cap9.html","id":"ordenação-restrita","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.5 Ordenação restrita","text":"ordenação restrita ou análise de gradiente direto, organiza os objetos de acordo com suas relações com outras variáveis (preditoras) coletadas nas mesmas unidades amostrais. O exemplo mais comum na ecologia é de investigar relação entre diversas variáveis ambientais (matriz X) coletadas em n localidades e abundância (ou presença ausência) de y espécies coletadas nas mesmas localidades (matrix Y). Com frequência, outros dados são utilizados como coordenadas geográficas das unidades amostrais (matriz W), os atributos funcionais das espécies coletadas (matriz T) e relação filogenética dessas espécies (matriz P). Diversos métodos são utilizados para combinar duas ou mais matrizes, mas neste capítulo iremos apresentar Análise de Redundância (RDA), Análise de Redundância parcial (RDAp) e métodos espaciais para incluir matriz W nas análises de gradiente direto.","code":""},{"path":"cap9.html","id":"análise-de-redundância-rda","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.5.1 Análise de Redundância (RDA)","text":"RDA é uma análise semelhante à regressão múltipla (Capítulo 7), mas que usa dados multivariados como variável dependente. duas matrizes comuns, matriz X (n unidades amostrais e m variáveis) e matriz Y (n unidades amostrais e p descritores - geralmente, espécies). O primeiro passo da RDA é centralizar (assim como na PCA, exemplo acima) matrizes X e Y. Após centralização, realiza-se regressões lineares entre X e Y para obter os valores preditos de Y (ou seja, os valores de Y que representação uma combinação linear com X). O passo seguinte é realizar uma PCA dos valores preditos de Y. Este último procedimento gera os autovalores, autovetores e os eixos canônicos que correspondem às coordenadas dos objetos (unidades amostrais), variáveis preditoras e das variáveis resposta. diferença da ordenação valor de Y predito e da ordenação somente de Y (como na PCA implementada acima) é que segunda mostra posição prevista pela relação linear entre X e Y. Logo, essa é exatamente o motivo da ordenação ser conhecida como restrita, pois variação em Y é restrita (linearmente) pela variação de X. Assim como na regressão múltipla, estatística da RDA é representada pelo valor de R2 e F. O valor de R2 indica força da relação linear entre X e Y e o valor F representa o teste global de significância. Além disso, é possível testar significância de cada um dos eixos da ordenação (e presença de pelo menos um eixo significativo é pré-requisito para que exista relação linear entre X e Y) e de cada uma das variáveis preditoras da matriz X.ChecklistVariáveis preditoras: importante verificar se: ) estrutura de correlação das variáveis ambientais, e ii) presença de autocorrelação espacialVariáveis preditoras: importante verificar se: ) estrutura de correlação das variáveis ambientais, e ii) presença de autocorrelação espacialComposição de espécies como matriz Y: fundamental observar se os valores utilizados representam abundância ou presença-ausência e qual necessidade de padronização (e.g., Hellinger)Composição de espécies como matriz Y: fundamental observar se os valores utilizados representam abundância ou presença-ausência e qual necessidade de padronização (e.g., Hellinger)Assim como em modelos de regressão linear simples e múltipla, os valores de R2 ajustado devem ser selecionados ao invés valor de R2Assim como em modelos de regressão linear simples e múltipla, os valores de R2 ajustado devem ser selecionados ao invés valor de R2Exemplo 1Espécies de aves que ocorrem em localidades com diferentes altitudes.PerguntaO clima e altitude modificam composição de espécies de aves?PrediçõesDiferenças climáticas (temperatura e chuva) e altitudinais alteram composição de espécies de avesVariáveisPreditoras: Temperatura e precipitação (contínuas) e altitude (categórica com três níveis)Dependente: composição de espécies de avesAnálisesDepois de selecionar um subconjunto dos dados com o método Forward Selection e padronizá-los (média 0 e desvio padrão 1), o modelo da RDA é construído como modelos lineares (Figura 9.14). Ordenação Multi-Escala (MSO) também é ajustada para analisar autocorrelação espacial.\nFigura 9.14: Ordenação multi-escala (MSO) para entender os resultados da ordenação em relação à distância geográfica.\nDa mesma forma que nas ordenações irrestritas, podemos fazer um gráfico para visualizar ordenações, mas agora esse gráfico é denominado “triplot”, pois possui os dados das duas ordenações (Figura 9.15).\nFigura 9.15: Triplot da RDA.\nInterpretação dos resultadosOs objetos res.axis, res.var e r_quadr mostram, respectivamente, ) dimensões (RDA1, RDA2, etc.) que possuem variação na composição de espécies, ii) variáveis preditoras que explicam esta variação, e iii) o valor R2 ajustado. Neste exemplo, podemos observar que somente dimensão 1 (RDA1) representa uma variação significativa da composição de espécies (P = 0,001). variáveis rain.jul, maxi.jul e altitude foram todas preditoras importantes da composição de espécies, mas rain.jul se destacada com maior valor de F. Além disso, o valor R2 ajustado de 0.381 indica forte contribuição dessas variáveis preditoras. Porém, uma das limitações desta análise é não considerar que tanto espécies quanto variáveis preditoras podem estar estruturadas espacialmente. Como resultado, os resíduos das análises podem apresentar autocorrelação espacial que, por sua vez, aumenta o Erro Tipo (Legendre Legendre 2012). figura obtida com o código msoplot(bird.rda) demonstra que existe autocorrelação espacial em algumas distâncias da análise. Veja abaixo algumas alternativas para resíduos com autocorrelação espacial.","code":"\n## Passo 1: transformação de hellinger da matriz de espécies\n# caso tenha dados de abundância.\nspecies.hel <- decostand(x = species, method = \"hellinger\")\n\n## Passo 2: selecionar variáveis importantes\n# Para isso, é necessário remover a variável categórica.\nenv.contin <- env[, -8]\n\n## Evite usar variáveis muito correlacionadas\nsel.vars <- forward.sel(species.hel, env.contin)\n#> Testing variable 1\n#> Testing variable 2\n#> Testing variable 3\n#> Procedure stopped (alpha criteria): pvalue for variable 3 is 0.212000 (> 0.050000)\nsel.vars$variables\n#> [1] \"rain.jul\" \"maxi.jul\"\nenv.sel <- env[,sel.vars$variables]\n\n## Passo 3: padronizar matriz ambiental (somente variáveis contínuas)\nenv.pad <- decostand(x = env.sel, method = \"standardize\")\n\n## Matriz final com variáveis preditoras\nenv.pad.cat <- data.frame(env.pad, altitude = env$altitude)\n## RDA com dados selecionados e padronizados\nrda.bird <- rda(species.hel ~ rain.jul + maxi.jul + altitude, data = env.pad.cat)\n\n## Para interpretar, é necessário saber a significância dos eixos para representar a relação entre as variáveis preditoras e a composição de espécies\nres.axis <- anova.cca(rda.bird, by = \"axis\") \nres.axis\n#> Permutation test for rda under reduced model\n#> Forward tests for axes\n#> Permutation: free\n#> Number of permutations: 999\n#> \n#> Model: rda(formula = species.hel ~ rain.jul + maxi.jul + altitude, data = env.pad.cat)\n#>          Df Variance       F Pr(>F)    \n#> RDA1      1 0.045759 12.0225  0.001 ***\n#> RDA2      1 0.009992  2.6252  0.061 .  \n#> RDA3      1 0.007518  1.9752  0.136    \n#> RDA4      1 0.003582  0.9410  0.470    \n#> Residual 18 0.068510                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Em seguida, é possível identificar quais são as variáveis que contribuem ou que mais contribuem para a variação na composição de espécies \nres.var <- anova.cca(rda.bird, by = \"term\") ## Qual variável?\nres.var\n#> Permutation test for rda under reduced model\n#> Terms added sequentially (first to last)\n#> Permutation: free\n#> Number of permutations: 999\n#> \n#> Model: rda(formula = species.hel ~ rain.jul + maxi.jul + altitude, data = env.pad.cat)\n#>          Df Variance      F Pr(>F)    \n#> rain.jul  1 0.036514 9.5936  0.001 ***\n#> maxi.jul  1 0.011264 2.9596  0.016 *  \n#> altitude  2 0.019071 2.5053  0.011 *  \n#> Residual 18 0.068510                  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Além disso, é possível obter o valor do R2 do modelo\nr_quadr <- RsquareAdj(rda.bird)\nr_quadr\n#> $r.squared\n#> [1] 0.4938685\n#> \n#> $adj.r.squared\n#> [1] 0.3813949\n\n## Ordenação multi-escala (MSO) para entender os resultados da ordenação em relação à distância geográfica\nbird.rda <- mso(rda.bird, xy, grain = 1, permutations = 99)\nmsoplot(bird.rda)\n#> Error variance of regression model underestimated by -2.7 percent\n## Triplot da RDA\nggord(rda.bird, ptslab = TRUE, size = 1, addsize = 3, repel = TRUE) +\n    geom_hline(yintercept = 0, linetype = 2) +\n    geom_vline(xintercept = 0, linetype = 2) + \n    tema_livro()"},{"path":"cap9.html","id":"análise-de-redundância-parcial-rdap","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.5.2 Análise de Redundância parcial (RDAp)","text":"Um dos problemas da abordagem anterior é que tanto composição de espécies como variáveis ambientais estão estruturadas espacialmente. Talvez mais importantes, para que os valores de probabilidade da RDA sejam interpretados corretamente (e para evitar Erro Tipo ), os resíduos modelo não devem estar correlacionados espacialmente, como demonstrado com análise MSO. Uma alternativa é incluir matriz de dados espaciais (matrix W) como valor condicional dentro da RDA. Esta análise é conhecida como RDA parcial.Porém, obtenção dos dados espaciais da matriz W é mais complexo que simplesmente incluir dados de localização geográfica (latitude e longitude), como feito em alguns modelos lineares (e.g., Generalized Least Squares - GLS Capítulos 7 e 10). Existem diversas ferramentas que descrevem e incorporam o componente espacial em métodos multidimensionais, mas os Mapas de Autovetores de Moran (MEM) são certamente os mais utilizados (Dray et al. 2012). análise MEM consiste na ordenação (PCoA) de uma matriz truncada obtida através da localização geográfica das localidades utilizando distância euclidiana, matriz de conectividade e matriz espacial ponderada. Os autovalores obtidos MEM são idênticos aos coeficientes de correlação espacial de Moran . Um procedimento chave desta análise é definição de um limiar de trucamento (inglês truncate threshold). Este limiar é calculado partir de uma “árvore de espaço mínimo” (Minimum Spanning Tree - MST) que conecta todos os pontos de coleta. Na prática, pontos com valores menores que o limiar definido pela MST indicam que eles estão conectados e, assim possuem correlação positiva. Outro ponto importante desta análise é obtenção da matriz espacial ponderada (Spatial Weighting Matrix - SWM). seleção da matriz SWM é parte essencial cálculo dos MEM e não deve ser feita arbitrariamente (Bauman, Drouet, Fortin, et al. 2018). Por este motivo, análise recebe este nome (Legendre Legendre 2012). Finalmente, o método produz autovetores que representam preditores espaciais que podem ser utilizados na RDA parcial (e em outras análises). É importante ressaltar que o critério de seleção número de autovetores é bastante debatido na literatura e, para isso, sugerimos leitura dos seguintes artigos: (Diniz-Filho et al. 2012; Bauman, Drouet, Fortin, et al. 2018; Bauman, Drouet, Dray, et al. 2018).Então, o primeiro passo para realizar uma RDA parcial é de gerar os autovetores espaciais (MEMs).Depois de gerar os valores dos autovetores espaciais (MEM), é possível executar RDA parcial utilizando esses valores argumento Conditional.Se você comparar os resultados objeto res.p.var (RDA parcial) com res.var (RDA simples) é possível perceber como estrutura espacial nos resíduos aumenta probabilidade de cometer Erro Tipo . O modelo da RDA parcial mostra que não existem qualquer efeito direto das variáveis ambientais sobre composição de espécies (conclusão com RDA simples). Na verdade, tanto composição de espécies quanto variáveis climáticas estão fortemente estruturadas espaço, como demonstramos seguir.Como resultado, é possível que variação ambiental espacialmente estruturada seja o principal efeito sobre composição de espécies. Uma maneira de visualizar contribuição relativa de diferentes matrizes (ambiental e espacial, por exemplo) é utilizar o método de partição de variância. O resultado deste modelo indica que, de fato, não existe efeito direto das variáveis ambientais e sim componente representado pela autocorrelação espacial dessas variáveis (Figura 9.16).\nFigura 9.16: Diagrama de Venn mostrando partição de variância da RDA.\n","code":"\n## Dados\n# Matriz padronizada de composição de espécies. \nhead(species.hel)[, 1:6] \n#>     Fauvette_orphee Fauvette_des_jardins Fauvette_a_tete_noire Fauvette_babillarde Fauvette_grisette Fauvette_pitchou\n#> S01               0            0.3651484             0.3651484           0.2581989         0.2581989                0\n#> S02               0            0.3333333             0.3333333           0.2357023         0.2357023                0\n#> S03               0            0.3162278             0.3162278           0.3162278         0.2236068                0\n#> S04               0            0.4200840             0.3429972           0.2425356         0.0000000                0\n#> S05               0            0.3872983             0.3162278           0.2236068         0.3162278                0\n#> S06               0            0.3779645             0.3779645           0.2672612         0.0000000                0\n\n## Latitude e longitude\nhead(xy)\n#>       x   y\n#> S01 156 252\n#> S02 141 217\n#> S03 171 233\n#> S04 178 215\n#> S05 123 189\n#> S06 154 195\n\n## dados ambientais padronizados e altitude\nhead(env.pad.cat)\n#>     rain.jul   maxi.jul      altitude\n#> S01 1.333646  0.1462557    Montanhoso\n#> S02 1.468827 -0.6848206 Intermediário\n#> S03 1.505694 -0.2099199    Montanhoso\n#> S04 1.296778 -2.0699476    Montanhoso\n#> S05 1.075572 -0.3682201         Plano\n#> S06 1.100151 -0.6056705 Intermediário\n\n## Passo 1: Gerar um arquivo LIST W: list binária de vizinhança\nmat_knn <- knearneigh(as.matrix(xy), k = 2, longlat = FALSE)\nmat_nb <- knn2nb(mat_knn, sym = TRUE)\nmat_listw <- nb2listw(mat_nb, style = \"W\")\nmat_listw\n#> Characteristics of weights list object:\n#> Neighbour list object:\n#> Number of regions: 23 \n#> Number of nonzero links: 58 \n#> Percentage nonzero weights: 10.96408 \n#> Average number of links: 2.521739 \n#> \n#> Weights style: W \n#> Weights constants summary:\n#>    n  nn S0       S1       S2\n#> W 23 529 23 18.84444 96.01111\n\n## Passo 2: Listar os métodos \"candidatos\" para obter a matriz SWM\nMEM_mat <- scores.listw(mat_listw, MEM.autocor = \"positive\")\ncandidates <- listw.candidates(xy, nb = c(\"gab\", \"mst\", \"dnear\"), \n                               weights = c(\"binary\", \"flin\"))\n\n## Passo 3: Selecionar a melhor matriz SWM e executar o MEM\nW_sel_mat <- listw.select(species.hel, candidates, MEM.autocor = \"positive\",\n                          p.adjust = TRUE, method = \"FWD\")\n#> Procedure stopped (alpha criteria): pvalue for variable 5 is 0.112000 (> 0.050000)\n#> Procedure stopped (alpha criteria): pvalue for variable 3 is 0.059000 (> 0.050000)\n#> Procedure stopped (alpha criteria): pvalue for variable 3 is 0.060000 (> 0.050000)\n#> Procedure stopped (alpha criteria): pvalue for variable 4 is 0.157000 (> 0.050000)\n\n## Passo 4: Matriz dos preditores espaciais escolhidos (MEMs)\nspatial.pred <- as.data.frame(W_sel_mat$best$MEM.select)\n\n## É necessário atribuir os nomes das linhas\nrownames(spatial.pred) <- rownames(xy) \n## Combinar variáveis ambientais e espaciais em um único data.frame\npred.vars <- data.frame(env.pad.cat, spatial.pred)\n\n## RDA parcial\nrda.p <- rda(species.hel ~\n                 rain.jul + maxi.jul + altitude + # Preditores ambientais\n                 Condition(MEM1 + MEM2 + MEM4 + MEM5), # Preditores espaciais\n             data = pred.vars)\n\n## Interpretação\n# Para interpretar, é necessário saber a significância dos eixos para representar a relação entre as variáveis preditoras e a composição de espécies.\nres.p.axis <- anova.cca(rda.p, by = \"axis\") \nres.p.axis\n#> Permutation test for rda under reduced model\n#> Forward tests for axes\n#> Permutation: free\n#> Number of permutations: 999\n#> \n#> Model: rda(formula = species.hel ~ rain.jul + maxi.jul + altitude + Condition(MEM1 + MEM2 + MEM4 + MEM5), data = pred.vars)\n#>          Df Variance      F Pr(>F)\n#> RDA1      1 0.008471 2.1376  0.316\n#> RDA2      1 0.004830 1.2189  0.779\n#> RDA3      1 0.003240 0.8176  0.890\n#> RDA4      1 0.001891 0.4773  0.895\n#> Residual 14 0.055477\n\n## Contribuição\n# Em seguida, é possível identificar quais são as variáveis que contribuem ou que mais contribuem para a variação na composição de espécies.\nres.p.var <- anova.cca(rda.p, by = \"term\") ## Qual variável?\nres.p.var\n#> Permutation test for rda under reduced model\n#> Terms added sequentially (first to last)\n#> Permutation: free\n#> Number of permutations: 999\n#> \n#> Model: rda(formula = species.hel ~ rain.jul + maxi.jul + altitude + Condition(MEM1 + MEM2 + MEM4 + MEM5), data = pred.vars)\n#>          Df Variance      F Pr(>F)\n#> rain.jul  1 0.004406 1.1119  0.349\n#> maxi.jul  1 0.004446 1.1220  0.338\n#> altitude  2 0.009579 1.2087  0.241\n#> Residual 14 0.055477\nRsquareAdj(rda.p)\n#> $r.squared\n#> [1] 0.1361661\n#> \n#> $adj.r.squared\n#> [1] 0.02330319\n## Padrão espacial na composição de espécies\npca.comp <- dudi.pca(species.hel, scale = FALSE, scannf = FALSE)\nmoran.comp <- moran.mc(pca.comp$li[, 1], mat_listw, 999)\n\n## Padrão espacial das variáveis ambientais\nenv$altitude <- as.factor(env$altitude)\nca.env <- dudi.hillsmith(env, scannf = FALSE)\nmoran.env <- moran.mc(ca.env$li[, 1], mat_listw, 999)\n\n## Estrutura espacial na composição de espécies?\nmoran.comp\n#> \n#>  Monte-Carlo simulation of Moran I\n#> \n#> data:  pca.comp$li[, 1] \n#> weights: mat_listw  \n#> number of simulations + 1: 1000 \n#> \n#> statistic = 0.62815, observed rank = 1000, p-value = 0.001\n#> alternative hypothesis: greater\n\n## Estrutura espacial na variação ambiental?\nmoran.env\n#> \n#>  Monte-Carlo simulation of Moran I\n#> \n#> data:  ca.env$li[, 1] \n#> weights: mat_listw  \n#> number of simulations + 1: 1000 \n#> \n#> statistic = 0.72714, observed rank = 1000, p-value = 0.001\n#> alternative hypothesis: greater\n## Triplot da RDA\n# Partição de variância.\npv.birds <- varpart(species.hel, env.pad.cat, spatial.pred)\nplot(pv.birds, cutoff = -Inf)\nmtext(\"Diagrama de Venn: partição de variância\")"},{"path":"cap9.html","id":"análise-de-redundância-baseada-em-distância-db-rda","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.5.3 Análise de Redundância baseada em distância (db-RDA)","text":"Acima, apresentamos análise RDA que permite comparar, por exemplo, se variáveis ambientais determinam variação na composição de espécies. RDA utiliza os dados brutos de composição de espécies e é baseada em uma PCA (.e., requer distância euclidiana). Porém, em algumas situações é necessário utilizar outras medidas de distância tais como Bray-Curtis ou Gower. Esta análise é bastante útil também quando matriz resposta já é uma matriz de distância, tais como que são geradas por decomposição da diversidade beta. Neste caso, Legendre & Anderson (1999) propuseram análise db-RDA (RDA baseada em distância). O primeiro passo da análise é gerar uma matriz de distância partir da matriz bruta (e.g., composição de espécies). Depois, técnica executa uma PCoA (corrigindo potenciais autovetores com autovalores negativos; ver discussão acima) para comparar com os termos modelo (matriz X, como na RDA).\nPara exemplificar como esta análise pode ser implementada R, vamos usar o mesmo exemplo demonstrado na análise RDA (ver acima). Porém, vamos fazer um ajuste sutil na pergunta “o clima e altitude modificam dissimilaridade (diversidade beta) de espécies de aves?”. Veja que neste caso, estamos perguntando diretamente sobre uma medida de diversidade beta, que será calculada com o método Bray-Curtis. variáveis preditoras serão mesmas obtidas exemplo da RDA: env.pad.cat.AnáliseInterpretação dos resultadosAssim como apresentado na RDA, os resultados obtidos em db.res.var sugerem que variáveis rain.jul, maxi.jul e altitude afetam diversidade beta de aves. O valor R2 ajustado (0.495) sugere forte relação entre o gradiente ambiental e diversidade beta. comparação entre o valor db_r_quadr e r_quadr indica que db-RDA teve melhor ajuste para comparar o mesmo conjunto de dados.","code":"\n## Passo 1: transformação de hellinger da matriz de espécies\n# caso tenha dados de abundância.\nspecies.hel <- decostand(species, \"hellinger\")\n\n## Passo 2: gerar matriz de distância (Diversidade beta: Bray-Curtis)\ndbeta <- vegdist(species.hel, \"bray\")\n\n## Passo 2: dbRDA \ndbrda.bird <- capscale (dbeta~rain.jul+maxi.jul+altitude, data=env.pad.cat)\n\n# Para interpretar, é necessário saber a significância dos eixos para representar a relação entre as variáveis preditoras e a composição de espécies\ndb.res.axis <- anova.cca(dbrda.bird, by = \"axis\") \ndb.res.axis\n#> Permutation test for capscale under reduced model\n#> Forward tests for axes\n#> Permutation: free\n#> Number of permutations: 999\n#> \n#> Model: capscale(formula = dbeta ~ rain.jul + maxi.jul + altitude, data = env.pad.cat)\n#>          Df SumOfSqs       F Pr(>F)    \n#> CAP1      1 0.274186 16.9355  0.001 ***\n#> CAP2      1 0.044965  2.7773  0.110    \n#> CAP3      1 0.018141  1.1205  0.656    \n#> CAP4      1 0.009500  0.5868  0.788    \n#> Residual 18 0.291420                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Em seguida, é possível identificar quais são as variáveis que contribuem ou que mais contribuem para a variação na composição de espécies \ndb.res.var <- anova.cca(dbrda.bird, by = \"term\") ## Qual variável?\ndb.res.var\n#> Permutation test for capscale under reduced model\n#> Terms added sequentially (first to last)\n#> Permutation: free\n#> Number of permutations: 999\n#> \n#> Model: capscale(formula = dbeta ~ rain.jul + maxi.jul + altitude, data = env.pad.cat)\n#>          Df SumOfSqs       F Pr(>F)    \n#> rain.jul  1 0.209981 12.9697  0.001 ***\n#> maxi.jul  1 0.041846  2.5847  0.070 .  \n#> altitude  2 0.094966  2.9328  0.012 *  \n#> Residual 18 0.291420                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Além disso, é possível obter o valor do R2 do modelo\ndb_r_quadr <- RsquareAdj(dbrda.bird)\ndb_r_quadr\n#> $r.squared\n#> [1] 0.6116758\n#> \n#> $adj.r.squared\n#> [1] 0.5253816"},{"path":"cap9.html","id":"permanova","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.6 PERMANOVA","text":"PERMANOVA é uma sigla, em inglês, de Permutational Multivariate Analysis Variance, análise proposta por Marti Anderson (M. J. Anderson 2001). PERMANOVA é usada para testar hipóteses multivariadas que comparam abundância de diferentes espécies em resposta diferentes tratamentos ou gradientes ambientais. Essa análise foi desenvolvida como forma de solucionar algumas limitações da tradicional ANOVA multivariada (MANOVA). Em especial, o pressuposto da MANOVA de distribuição normal multivariada é raramente encontrado em dados ecológicos.O primeiro passo da PERMANOVA é selecionar uma medida de distância apropriada aos dados e, além disso, verificar necessidade de padronização ou transformação dos dados. Em seguida, distâncias são comparadas entre os grupos de interesse (por exemplo, tratamento versus controle) usando estatística F de maneira muito parecida com uma ANOVA (Capítulo 7), chamada de pseudo-F:\\[F_pseudo = (SSa / SSr)*[(N-g) / (g-1)]\\]Nessa equaçãoSSa representa soma dos quadrados entre gruposSSr soma de quadrados dentro grupo (residual)N o número de unidades amostraisg os grupos (ou níveis da variável categórica)Esta fórmula pseudo-F é específica para desenho experimental com um fator. Outros desenhos mais complexos são apresentados em Anderson (2001, 2017). O cálculo valor de probabilidade é realizado por métodos de permutação que são discutidos em Anderson & Ter Braak (2003).Exemplo 1Espécies de aves que ocorrem em localidades com diferentes altitudes.PerguntaO clima e altitude modificam composição de espécies de aves?PrediçõesDiferenças climáticas (temperatura e chuva) e altitudinais alteram composição de espécies de avesVariáveisPreditoras: Temperatura e chuva (contínuas) e altitude (categórica com três níveis)Dependente: composição de espécies de avesPara reduzir o número de variáveis modelo, você pode considerar duas abordagens. primeira, e mais importante delas, é manter somente variáveis preditoras que você tenha razão biológica para mantê-la e, além disso, que esteja relacionada com suas hipóteses. Assim, uma vez que você já removeu variáveis que não possuem relevância biológica, você deve usar diferentes métodos para remover variáveis muito correlacionadas (forward selection, Variance Inflation Factor (VIF), entre outros). Neste exemplo, vamos simplesmente fazer uma correlação múltipla e remover variáveis com correlação maior que 0.9 ou -0.9. função ggpairs() mostra um gráfico bem didático para representar relação entre todas variáveis e o valor (r) desta correlação (mais em Capítulo 7) (Figura 9.17).\nFigura 9.17: Correlograma mostrando correlação entre variáveis.\nApós selecionar variáveis modelo, vamos executar PERMANOVA e entender principais etapas para interpretar corretamente o teste. função adonis() pacote vegan é uma boa opção. Porém, é importante referir o programa PRIMER e PERMANOVA+ como ótima opção para implementar PERMANOVA e ter maior controle em desenhos complexos (M. J. Anderson, Gorley, Clarke 2008). Assim como nos modelos lineares apresentados Capítulo 7, os argumentos seguem o mesmo formato, com variável dependente separada por um “~” das variáveis preditoras. Porém, alguns autores demonstraram que PERMANOVA (assim como Mantel e ANOSIM) não pode identificar se diferenças significativas teste (usando estatística pseudo-F) devem-se à diferenças posição, na dispersão ou ambos. Ou seja, ao comparar grupos não é possível identificar se existe mudanças de composição (posição) ou se variação da composição de espécies dentro de um grupo (dispersão) é maior que variação dentro outro grupo (M. J. Anderson Walsh 2013). Para solucionar este problema, é possível combinar PERMANOVA com análise PERMDISP (ou BETADISPER, como chamado pacote vegan implementado através da função betadisper()). Esta análise permite comparar se existe heterogeneidade nas variâncias entre grupos. Deste modo, com presença de heterogeneidade de variâncias (valor BETADISPER significativo), é possível saber que diferenças entre os grupos ocorrem principalmente por diferenças na dispersão e não, necessariamente, de posição. Mais detalhes sobre relevância de combinar essas duas análises estão disponíveis em Anderson & Walsh (2013).Em nosso exemplo, temperatura, precipitação e altitude afetaram variação na composição de espécies. Porém, para identificar se diferenças de composição entre os níveis da variável altitude, é necessário interpretar os resultados da análise BETADISPER. O código permutest(betad.aves) mostra que o valor de probabilidade da análise foi de 0.253, ou seja, hipótese nula de que variância entre grupos é homogênea é aceita. Assim, não existe diferenças na dispersão entre grupos, sugerindo que diferença encontrada na PERMANOVA (objeto perm.aves) se deve, em parte, à mudança na composição de espécies de aves entre diferentes altitudes (R2 = 0.135). Além disso, precipitação (R2 = 0.183) e temperatura (R2 = 0.127) foram fatores importantes na variação da composição de espécies.Como falado anteriormente, análises de ordenação irrestritas (PCA, PCoA, nMDS) são utilizadas para explorar dados. Uma maneira poderosa de usá-las é combinando com análises que testam hipóteses, como PERMANOVA e RDA. literatura ecológica tem usado Análise de Escalonamento não-Métrico (non-Metric Multidimensional Scaling - nMDS) combinado com análises multidimensionais de variância (como PERMANOVA) para visualização da similaridade na composição de espécies dentro e entre grupos. seguir, implementamos o nMDS na matriz de composição de espécies de ácaros (Figura 9.18).\nFigura 9.18: Biplot mostrando o resultado nMDS.\n","code":"\n## Composição de espécies padronizar com método de Hellinger\nspecies.hel <- decostand(x = species, method = \"hellinger\")\n\n## Matriz de distância com método Bray-Curtis\nsps.dis <- vegdist(x = species.hel, method = \"bray\")\n## Verifica correlação entre as variáveis\nggpairs(env) +\n    tema_livro()\n\n## Após verificar a estrutura de correlação, vamos manter somente três variáveis\nenv2 <- env[, c(\"mini.jan\", \"rain.tot\", \"altitude\")]\n## PERMANOVA\nperm.aves <- adonis2(sps.dis ~ mini.jan + rain.tot + altitude, data = env2)\nperm.aves ### Diferenças entre os tratamentos?\n#> Permutation test for adonis under reduced model\n#> Terms added sequentially (first to last)\n#> Permutation: free\n#> Number of permutations: 999\n#> \n#> adonis2(formula = sps.dis ~ mini.jan + rain.tot + altitude, data = env2)\n#>          Df SumOfSqs      R2      F Pr(>F)    \n#> mini.jan  1  0.09069 0.15997 6.3307  0.001 ***\n#> rain.tot  1  0.12910 0.22771 9.0118  0.001 ***\n#> altitude  2  0.08929 0.15749 3.1163  0.011 *  \n#> Residual 18  0.25787 0.45483                  \n#> Total    22  0.56695 1.00000                  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## BETADISPER\nbetad.aves <- betadisper(sps.dis, env2$altitude)\npermutest(betad.aves)\n#> \n#> Permutation test for homogeneity of multivariate dispersions\n#> Permutation: free\n#> Number of permutations: 999\n#> \n#> Response: Distances\n#>           Df    Sum Sq   Mean Sq      F N.Perm Pr(>F)\n#> Groups     2 0.0042643 0.0021322 1.4672    999  0.265\n#> Residuals 20 0.0290636 0.0014532\n## Matriz de distância representando a variação na composição de espécies (método Bray-Curtis)\nas.matrix(sps.dis)[1:6, 1:6]\n#>           S01        S02        S03       S04       S05       S06\n#> S01 0.0000000 0.15133734 0.16720405 0.2559122 0.2559882 0.2588892\n#> S02 0.1513373 0.00000000 0.04114702 0.1190172 0.1289682 0.1391056\n#> S03 0.1672040 0.04114702 0.00000000 0.1420233 0.1358127 0.1410867\n#> S04 0.2559122 0.11901720 0.14202325 0.0000000 0.1140283 0.1199803\n#> S05 0.2559882 0.12896823 0.13581271 0.1140283 0.0000000 0.2129054\n#> S06 0.2588892 0.13910558 0.14108668 0.1199803 0.2129054 0.0000000\n\n## É preciso calcular uma primeira \"melhor\" solução do nMDS\nsol1 <- metaMDS(sps.dis)\n#> Run 0 stress 0.1344042 \n#> Run 1 stress 0.3825954 \n#> Run 2 stress 0.1272417 \n#> ... New best solution\n#> ... Procrustes: rmse 0.07890815  max resid 0.3526073 \n#> Run 3 stress 0.1344042 \n#> Run 4 stress 0.1338432 \n#> Run 5 stress 0.1626783 \n#> Run 6 stress 0.1336481 \n#> Run 7 stress 0.1272417 \n#> ... Procrustes: rmse 6.723274e-06  max resid 1.889203e-05 \n#> ... Similar to previous best\n#> Run 8 stress 0.1338432 \n#> Run 9 stress 0.1378506 \n#> Run 10 stress 0.1272418 \n#> ... Procrustes: rmse 0.0001085155  max resid 0.0004162969 \n#> ... Similar to previous best\n#> Run 11 stress 0.1272418 \n#> ... Procrustes: rmse 4.197901e-05  max resid 0.000160606 \n#> ... Similar to previous best\n#> Run 12 stress 0.1378506 \n#> Run 13 stress 0.1272418 \n#> ... Procrustes: rmse 5.310796e-05  max resid 0.0001988503 \n#> ... Similar to previous best\n#> Run 14 stress 0.1580523 \n#> Run 15 stress 0.1344042 \n#> Run 16 stress 0.1272418 \n#> ... Procrustes: rmse 2.586759e-05  max resid 9.827497e-05 \n#> ... Similar to previous best\n#> Run 17 stress 0.1584787 \n#> Run 18 stress 0.1344042 \n#> Run 19 stress 0.1344042 \n#> Run 20 stress 0.1344042 \n#> *** Solution reached\n\n## Melhor solução\n# Depois, executar a mesma função, mas utilizando uma \"melhor solução inicial\" para evitar resultdos subótimos no nMDS .\nnmds.beta <- metaMDS(sps.dis, previous.best = sol1)\n#> Starting from 2-dimensional configuration\n#> Run 0 stress 0.1272417 \n#> Run 1 stress 0.1272418 \n#> ... Procrustes: rmse 2.188819e-05  max resid 7.340362e-05 \n#> ... Similar to previous best\n#> Run 2 stress 0.1338432 \n#> Run 3 stress 0.1584788 \n#> Run 4 stress 0.1344043 \n#> Run 5 stress 0.1338433 \n#> Run 6 stress 0.1336481 \n#> Run 7 stress 0.1344042 \n#> Run 8 stress 0.1338432 \n#> Run 9 stress 0.1272418 \n#> ... Procrustes: rmse 2.708248e-05  max resid 0.0001008894 \n#> ... Similar to previous best\n#> Run 10 stress 0.1336481 \n#> Run 11 stress 0.1344042 \n#> Run 12 stress 0.3798204 \n#> Run 13 stress 0.1336481 \n#> Run 14 stress 0.1336481 \n#> Run 15 stress 0.1344042 \n#> Run 16 stress 0.1336481 \n#> Run 17 stress 0.1338432 \n#> Run 18 stress 0.1272418 \n#> ... Procrustes: rmse 2.303898e-05  max resid 8.758403e-05 \n#> ... Similar to previous best\n#> Run 19 stress 0.1344042 \n#> Run 20 stress 0.1378506 \n#> *** Solution reached\n\n## Stress\n# O stress é o valor mais importante para interpretar a qualidade da ordenação.\nnmds.beta$stress # valor ideal entre 0 e 0.2\n#> [1] 0.1272417\n\n## Exportar os valores para fazer gráfico \ndat.graf <- data.frame(nmds.beta$points, altitude = env2$altitude)\n\n## Definir os grupos (\"HULL\") para serem categorizados no gráfico \ngrp.mon <- dat.graf[dat.graf$altitude == \"Montanhoso\", ][chull(dat.graf[dat.graf$altitude == \"Montanhoso\", c(\"MDS1\", \"MDS2\")]), ]\n\ngrp.int <- dat.graf[dat.graf$altitude == \"Intermediário\", ][chull(dat.graf[dat.graf$altitude == \"Intermediário\", c(\"MDS1\", \"MDS2\")]), ]\n\ngrp.pla <- dat.graf[dat.graf$altitude == \"Plano\", ][chull(dat.graf[dat.graf$altitude == \"Plano\", c(\"MDS1\", \"MDS2\")]), ]\n\n## Combinar dados dos grupos para cada Convex Hull\nhull.data <- rbind(grp.mon, grp.int, grp.pla) \nhead(hull.data)\n#>            MDS1        MDS2      altitude\n#> S04 -0.10578586 -0.10682766    Montanhoso\n#> S01 -0.25332406  0.04199004    Montanhoso\n#> S11 -0.12504415  0.14477005    Montanhoso\n#> S15  0.09166059  0.09857114    Montanhoso\n#> S18  0.01968127 -0.12417579 Intermediário\n#> S06 -0.16053825 -0.08924030 Intermediário\n\n## Gráfico\nggplot(dat.graf, aes(x = MDS1, y = MDS2, color = altitude, shape = altitude)) + \n    geom_point(size = 4, alpha = 0.7) + \n    geom_polygon(data = hull.data, aes(fill = altitude, group = altitude), alpha = 0.3) + \n    scale_color_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    scale_fill_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    labs(x = \"NMDS1\", y = \"NMDS2\") + \n    tema_livro()"},{"path":"cap9.html","id":"mantel-e-mantel-parcial","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.7 Mantel e Mantel parcial","text":"O teste de Mantel se aplica quando temos duas ou mais matrizes de distância (triangulares). Em estudos ecológicos, essas matrizes normalmente descrevem dados de distância ou (dis)similaridade na composição de espécies, distância geográfica, e similaridade ambiental. Mas este teste também é amplamente aplicado na área de genética de paisagem para testar hipóteses sobre como composição genética de uma população varia em função ambiente e/ou espaço geográfico (Legendre Fortin 2010). O teste nada mais é que uma generalização coeficiente de correlação de Pearson (Capítulo 7) para lidar com matrizes de distância. Como essas matrizes descrevem relação entre vários pares de locais, temos repetição mesmo local em outros pares. Neste caso é preciso um procedimento semelhante ao de correção para múltiplos testes.Já o Mantel parcial pretende testar correlação entre duas matrizes controlando para o efeito de uma terceira. Por exemplo, testar associação entre dissimilaridade na composição de espécies e dissimilaridade ambiental, controlando para distância geográfica entre os locais. Isso nos permite levar em conta possível autocorrelação espacial nas variáveis ambientais, ou seja, estaríamos testando de fato o efeito da dissimilaridade ambiental “pura” na dissimilaridade na composição de espécies, evitando o efeito espaço em si. Portanto, note que sempre estamos raciocinando em termos de (dis)similaridade e não dos dados brutos em si, seja composição de espécies ou variáveis ambientais. É importante ter isto em mente na hora de interpretar os resultados.Exemplo 1Neste exemplo, vamos utilizar um conjunto de dados que contém girinos de espécies de anfíbios anuros coletados em 14 poças d’água e os respectivos dados de variáveis ambientais destas mesmas poças (Provete et al. 2014).PerguntaExiste correlação entre dissimilaridade na composição de espécies de girinos e dissimilaridade ambiental?PrediçõesAssumindo que processos baseados nicho sejam predominantes, quanto mais diferentes poças forem maior será diferença na composição de espéciesVariáveisVariáveis: matriz de distância (Bray-Curtis) da composição das espécies e outra matriz de distância (Euclideana) das variáveis ambientaisAnálisesAqui vamos utilizar além conjunto de dados bocaina já conhecido, outras duas matrizes disponíveis pacote ecodados: bocaina.xy e bocaina.env, que contêm coordenadas geográficas e variáveis ambientais das poças, respectivamente.Vamos verificar correlação entre distância geográfica em linha reta e dissimilaridade ambiental (Figura 9.19)\nFigura 9.19: Relação das matrizes de (dis)similaridade dos dados.\nVamos verificar correlação entre distância geográfica em linha reta e dissimilaridade na composição de espécies (Figura 9.20)\nFigura 9.20: Relação das matrizes de (dis)similaridade dos dados.\nVamos verificar correlação entre dissimilaridade ambiental e dissimilaridade na composição de espécies (Figura 9.21)\nFigura 9.21: Relação das matrizes de (dis)similaridade dos dados.\nInterpretação dos resultadosOs resultados mostraram que existe uma relação positiva, embora fraca, entre dissimilaridade ambiental e dissimilaridade na composição de espécies. Este resultado está de acordo com nossa hipótese inicial. entanto, vimos também que existe uma relação positiva e significativa entre distância em linha reta entre poças e dissimilaridade ambiental. Logo, para de fato testar relação entre o ambiente e composição de espécies temos de levar em conta o espaço.Para isto temos de realizar um teste de Mantel Parcial.Agora sim, vimos que relação entre ambiente e espécies se mantém, mas que força dessa relação diminuiu bastante (veja o valor de r) quando controlamos o espaço.","code":"\n## Dados\nhead(bocaina.env)\n#>       pH  tur   DO  temp veg  dep canopy conduc hydro area\n#> PP3 7.07 0.66 2.26 18.93  30 0.35   30.3 0.0100   per  139\n#> PP4 6.85 0.33 2.78 20.53   0 0.33   32.1 0.0100   per  223\n#> AP1 6.64 0.40 4.09 15.26   5 3.50   42.0 0.0105   per  681\n#> AP2 6.13 0.80 3.47 15.40  70 2.50   82.0 0.0300   per  187\n#> PP1 5.80 0.75 3.22 16.75  70 0.70   91.4 0.0100   per   83\n#> PP2 5.32 4.00 2.10 15.90  95 0.27   75.3 0.0100   per  108\nhead(bocaina.xy)\n#>         Long      Lat\n#> PT5 -44.6239 -22.7243\n#> PP3 -44.6340 -22.7228\n#> PP4 -44.6334 -22.7228\n#> BP4 -44.6212 -22.7262\n#> BP2 -44.6155 -22.7289\n#> PT1 -44.6170 -22.7350\n\n## Matriz de distância geográfica\nDist.km <- as.dist(rdist.earth(bocaina.xy, miles=F)) # matriz de distância geográfica (geodésica) considerando a curvatura da Terra\n\n## Padronizações\ncomp.bo.pad <- decostand(t(bocaina), \"hellinger\")\nenv.bocaina <- decostand(bocaina.env[,-9], \"standardize\")\n\n## Dissimilaridades\ndissimil.bocaina <- vegdist(comp.bo.pad, \"bray\")\ndissimil.env <- vegdist(env.bocaina, \"euclidian\")\n## Mantel\n# Espaço vs. ambiente\nmantel(Dist.km, dissimil.env) \n#> \n#> Mantel statistic based on Pearson's product-moment correlation \n#> \n#> Call:\n#> mantel(xdis = Dist.km, ydis = dissimil.env) \n#> \n#> Mantel statistic r: 0.4848 \n#>       Significance: 0.005 \n#> \n#> Upper quantiles of permutations (null model):\n#>   90%   95% 97.5%   99% \n#> 0.250 0.330 0.393 0.447 \n#> Permutation: free\n#> Number of permutations: 999\n\n## Preparar os dados para o gráfico\nmatrix.dist.env <- data.frame(x = melt(as.matrix(Dist.km))$value, \n                              y = melt(as.matrix(dissimil.env))$value)\n\n## Gráfico\nggplot(matrix.dist.env , aes(x, y)) +\n    geom_point(size = 4, shape = 21, fill = \"darkorange\", alpha = 0.7) +\n    labs(x = \"Distância geográfica (km)\", \n         y = \"Dissimilaridade Ambiental\") +\n    tema_livro()                              \n## Mantel\n# Espaço vs. composição\nmantel(Dist.km, dissimil.bocaina) \n#> \n#> Mantel statistic based on Pearson's product-moment correlation \n#> \n#> Call:\n#> mantel(xdis = Dist.km, ydis = dissimil.bocaina) \n#> \n#> Mantel statistic r: 0.3745 \n#>       Significance: 0.007 \n#> \n#> Upper quantiles of permutations (null model):\n#>   90%   95% 97.5%   99% \n#> 0.149 0.215 0.279 0.335 \n#> Permutation: free\n#> Number of permutations: 999\n\n## Preparar os dados para o gráfico\nmatrix.dist.bocaina <- data.frame(x = melt(as.matrix(Dist.km))$value, \n                              y = melt(as.matrix(dissimil.bocaina))$value)\n\n## Gráfico\nggplot(matrix.dist.bocaina , aes(x, y)) +\n    geom_point(size = 4, shape = 21, fill = \"darkorange\", alpha = 0.7) +\n    labs(x = \"Distância geográfica (km)\", \n         y = \"Dissimilaridade (Bray-Curtis)\") +\n    tema_livro()\n## Mantel\n# Ambiente vs. composição\nmantel(dissimil.env, dissimil.bocaina) \n#> \n#> Mantel statistic based on Pearson's product-moment correlation \n#> \n#> Call:\n#> mantel(xdis = dissimil.env, ydis = dissimil.bocaina) \n#> \n#> Mantel statistic r: 0.2898 \n#>       Significance: 0.02 \n#> \n#> Upper quantiles of permutations (null model):\n#>   90%   95% 97.5%   99% \n#> 0.171 0.227 0.266 0.322 \n#> Permutation: free\n#> Number of permutations: 999\n\n## Preparar os dados para o gráfico\nmatrix.bocaina.env <- data.frame(x = melt(as.matrix(dissimil.bocaina))$value, \n                                 y = melt(as.matrix(dissimil.env))$value)\n\n## Gráfico\nggplot(matrix.bocaina.env, aes(x, y)) +\n    geom_point(size = 4, shape = 21, fill = \"darkorange\", alpha = 0.7) +\n    labs(x = \"Similaridade Bocaina\", \n         y = \"Similaridade Ambiental\") +\n    tema_livro()\n## Mantel Parcial\nmantel.partial(dissimil.bocaina, dissimil.env, Dist.km) #comp. vs. ambiente controlando o espaço\n#> \n#> Partial Mantel statistic based on Pearson's product-moment correlation \n#> \n#> Call:\n#> mantel.partial(xdis = dissimil.bocaina, ydis = dissimil.env,      zdis = Dist.km) \n#> \n#> Mantel statistic r: 0.1334 \n#>       Significance: 0.123 \n#> \n#> Upper quantiles of permutations (null model):\n#>   90%   95% 97.5%   99% \n#> 0.148 0.200 0.229 0.278 \n#> Permutation: free\n#> Number of permutations: 999"},{"path":"cap9.html","id":"mantel-espacial-com-modelo-nulo-restrito-considerando-autocorrelação-espacial","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.8 Mantel espacial com modelo nulo restrito considerando autocorrelação espacial","text":"Embora seja um tipo de análise bastante popular, o Mantel e sua versão parcial parecem não ser muito adequadas para lidar com autocorrelação espacial. Os autores dos artigos abaixo sugerem inclusive que esse tipo de análise não é adequado para testar maioria das hipóteses em Ecologia, incluindo diversidade beta. Esse é um tema bastante discutido na literatura e não vamos nos alongar muito sobre isso, mas referimos o() leitor() para os seguintes artigos chave: Legendre et al. (2015), Legendre & Fortin (2010), e Legendre (2000).entanto, Crabot et al. (2019) propuseram uma modificação teste que leva em conta autocorrelação espacial ao gerar o modelo nulo usado para o teste de hipótese. É utilizado um procedimento chamado Moran Spectral Randomization porque preserva autocorrelação global medida por meio de Moran global. grande vantagem é que este teste utiliza toda flexibilidade dos Moran Eigenvector Maps (MEMs), que é apresentado na parte de RDA, permitindo analisar relações entre matrizes em múltiplas escalas espaciais. Vejamos como o teste funciona.Primeiro precisamos construir um objeto com duas matrizes que desejamos testar, nosso caso composição de espécies e espaço. Depois precisamos construir explicitamente nossa hipótese de relação espacial entre os locais por meio de uma rede de vizinhança. Esta rede é uma estrutura que nos dirá como (e eventualmente com qual intensidade) se há relação entre os locais. Aqui, entende-se que nos referimos à probabilidade de fluxo de indivíduos entre os locais.Note que até aqui não há nada de novo, apenas utilizamos uma função diferente para realizar o teste de Mantel, agora pacote ade4 ao invés vegan. Agora temos de construir nossa rede de vizinhança. Optamos neste exemplo por uma rede bastante simples, que é Minimum Spanning Tree. Essa é rede espacial mínima que mantém todos os pontos ligados entre si (Figura 9.22).\nFigura 9.22: Rede de vizinhança com Minimum Spanning Tree\ngráfico, podemos ver “cara” da rede. Cada ponto representa um local (poça) e linhas representam ligação hipotética entre eles, plotamos espaço geográfico, ou seja, o Norte aponta pra cima. Agora temos apenas que converter o formato nb para listw e finalmente entrar com esses objetos na função que realizará análise.Interpretação dos resultadosVamos comparar com o Mantel comum.Note que o valor da estatística teste, ou seja, o r de Mantel é exatamente o mesmo. entanto, o valor de P que indicava que o teste havia sido significativo anteriormente mudou e passou ser não significativo. Isso ocorreu porque, embora maneira de calcular estatística teste seja mesma, forma de construir distribuição nula, partir da qual será calculado o valor de P mudou drasticamente, resultando num teste não significativo.","code":"\n## Mantel\ncompos_espac <- mantel.randtest(sqrt(dissimil.bocaina), Dist.km)\ncompos_espac\n#> Monte-Carlo test\n#> Call: mantel.randtest(m1 = sqrt(dissimil.bocaina), m2 = Dist.km)\n#> \n#> Observation: 0.3738076 \n#> \n#> Based on 999 replicates\n#> Simulated p-value: 0.005 \n#> Alternative hypothesis: greater \n#> \n#>     Std.Obs Expectation    Variance \n#> 3.265589171 0.002421976 0.012933827\n## Minimum Spanning Tree\nnb.boc <- mst.nb(Dist.km) # calcula uma Minimum Spanning Tree\nplot(nb.boc, bocaina.xy)\n## Conversão\nlw <- nb2listw(nb.boc)\n\n## Mantel espacial\nmsr(compos_espac, lw, method = \"pair\")\n#> Monte-Carlo test\n#> Call: msr.mantelrtest(x = compos_espac, listwORorthobasis = lw, method = \"pair\")\n#> \n#> Observation: 0.3738076 \n#> \n#> Based on 999 replicates\n#> Simulated p-value: 0.127 \n#> Alternative hypothesis: greater \n#> \n#>     Std.Obs Expectation    Variance \n#>  1.21335033  0.17433665  0.02702631\n## Mantel comum\ncompos_espac\n#> Monte-Carlo test\n#> Call: mantel.randtest(m1 = sqrt(dissimil.bocaina), m2 = Dist.km)\n#> \n#> Observation: 0.3738076 \n#> \n#> Based on 999 replicates\n#> Simulated p-value: 0.005 \n#> Alternative hypothesis: greater \n#> \n#>     Std.Obs Expectation    Variance \n#> 3.265589171 0.002421976 0.012933827"},{"path":"cap9.html","id":"procrustes-e-protest","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.9 PROCRUSTES e PROTEST","text":"Uma alternativa ao Mantel para comparar o grau de associação entre conjuntos multidimensionais de dados (e.g., matriz de espécies e matriz ambiental) é análise conhecida como Procrustes (Gower 1971; Jackson 1995). Esta análise compara duas matrizes (objetos nas linhas e variáveis nas colunas) minimizando soma dos desvios quadrados entre os valores através das seguintes técnicas: translação, escalonamento e rotação. É importante notar que os objetos devem ser os mesmos e, desse modo, o número de linhas é obrigatoriamente igual, ao passo que o número de colunas pode variar. posição objeto Xi (= 1, …n) é comparada com o objeto Yi da matriz correspondente, movimentando os pontos via translação, reflexão, rotação e dilatação. Assim, os objetos da matriz X resultam na matriz X’ que é representada pelo melhor ajuste (menor valor residual da soma dos quadrados: estatística m12) entre matriz X e Y. estatística m12 mede, então, o grau de concordância entre duas matrizes. Os valores de m12 variam de 1 (sem concordância) 0 (máxima concordância) (Peres-Neto Jackson 2001; Lisboa et al. 2014). Para testar significância valor observado de m12, Jackson (1995) sugeriu um teste de aleatorização chamado PROTEST.possibilidades de aplicação Procrustes para ecologia são diversas. Dentre elas, se destacam : ) morfometria geométrica e ii) ecologia de comunidades (concordância de comunidades). Uma característica interessante Procrustes é possibilidade de usar matriz bruta (e.g., variáveis ambientais) ou matriz de distância, o que amplia possibilidades analíticas. Para comparações com matrizes brutas, análises de ordenação irrestrita (PCA, CA) devem ser realizadas antes Procrustes. Por outro lado, para comparações com matrizes de distância (e.g., Euclidiana, Jaccard, Bray-Curtis), análises como PCoA e nMDS devem preceder à análise de Procrustes (veja Figura 4 em Peres-Neto Jackson 2001). Nos dois casos, ordenação é importante para gerar matrizes com mesma dimensionalidade o que, por sua vez, permite comparar matrizes com número de colunas diferentes.Exemplo 1Neste exemplo, vamos utilizar dois conjuntos de dados simulados de peixes e macroinvertebrados aquáticos coletados em riachos. Neste exemplo hipotético, existe uma rede de 12 riachos com diferentes graus de poluição. Em cada riacho, foi produzida uma lista de espécies de peixes e de macroinvertebrados (com dados de abundância).PerguntaExiste concordância na distribuição na composição de espécies de peixes e macroinvertebrados?PrediçõesAssumindo que alguns poluentes, tais como metais pesados, podem atuar como filtro ambiental e limitar ocorrência de determinados organismos aquáticos, espera-se encontrar alta concordância entre duas matrizes (peixes e macroinvertebrados). Ou seja, comunidade tanto de peixes quanto de macroinvertebrados responderão de forma similar ao gradiente ambientalVariáveisVariáveis: composição das espécies de peixes e macroinverebrados aquáticosAnálisesNo exemplo escolhido, duas matrizes representam composição de espécies em diferentes riachos. Desse modo, o ideal é transformar esta matriz bruta em matriz de distância com o método Bray-Curtis (vegdist()). O próximo passo, então, é realizar uma Análise de Coordenadas Principais (PCoA) (veja abaixo) para gerar duas matrizes de ordenação que serão comparadas com função procrustes(). Por fim, utilizamos função protest() para testar significância da concordância entre matrizes (Figura 9.23).\nFigura 9.23: Biplot mostrando o resultado da análise de PROCRUSTES.\nInterpretação dos resultadosA função protest() apresenta dois resultados importantes: ) estatística m12 e ii) o valor de significância (p). exemplo, o valor de m12 é igual 0.6185 e o valor de p é 0,019. Desse modo, podemos concluir que existe concordância entre composição de espécies de peixes e macroinvertebrados aquáticos, sugerindo que o nível de poluição pode gerar padrões previsíveis de distribuição de diferentes organismos aquáticos. figura produzida pelo código plot(concord) mostra um gráfico típico Procrustes, onde base da seta representa matriz X e ponta da seta matriz alvo (Y) após rotação para comparar o grau de concordância entre X e Y. Cada ponto representa um objeto (linha) das matrizes X e Y. Quanto menor o tamanho das setas, mais concordantes são observações em cada objeto.","code":"\n## Dados\nhead(fish_comm)\n#>       sp1 sp2 sp3 sp4 sp5 sp6 sp7 sp8 sp9 sp10 sp11 sp12 sp13 sp14 sp15 sp16 sp17 sp18 sp19 sp20 sp21 sp22 sp23 sp24\n#> site1   0   0   0   1   1   0   4   0   0    0    1    0    1    1    0    5    0    0    0    0    0    1    0    3\n#> site2   0   0   0   1   0   1   5   0   2    1    1    2    1    0    0    1    0    0    0    0    0    0    1    0\n#> site3   0   1   0   4   0   0   0   1   5    0    5    0    0    5    0    1    0    1    0    0    2    0    0    0\n#> site4   0   5   0   1   0   1   0   0   2    0    0    0    0    2    1    0    0    0    0    1    1    0    0    2\n#> site5   0   2   0   0   0   0   0   0   0    0    4    0    0    1    0    4    0    0    0    1    5    0    5    0\n#> site6   0   2   1   0   0   0   0   1   1    0    0    2    0    2    0    0    0    0    0    0    0    0    4    0\nhead(macroinv)\n#>       sp1 sp2 sp3 sp4 sp5 sp6 sp7 sp8 sp9 sp10 sp11 sp12 sp13 sp14 sp15 sp16 sp17 sp18 sp19 sp20 sp21 sp22 sp23 sp24 sp25 sp26 sp27 sp28 sp29\n#> site1   0   1   1   0   0   0   0   0   0    1    2    0    0    0    0    1    1    0    4    0    0    0    1    0    0    1    0    4    0\n#> site2   0   2   1   0   0   0   0   1   1    0    0    2    0    1    1    0    0    0    0    0    0    1    2    0    1    0    1    1    0\n#> site3   0   0   0   1   0   1   5   0   2    1    1    2    0    4    0    1    0    0    1    0    0    0    0    1    0    2    1    0    0\n#> site4   0   1   0   4   0   0   0   1   5    0    5    0    0    5    0    1    0    1    0    0    2    0    0    0    0    2    1    0    0\n#> site5   0   2   0   0   0   0   0   0   0    0    4    0    0    2    1    0    0    0    0    1    1    0    0    2    0    5    0    1    0\n#> site6   1   0   0   1   0   0   0   0   0    0    1    0    0    2    0    0    0    0    0    0    0    0    4    0    0    2    0    0    0\n#>       sp30 sp31 sp32 sp33 sp34 sp35 sp36\n#> site1    0    0    1    5    0    5    0\n#> site2    0    1    0    0    1    0    0\n#> site3    0    0    1    1    0    0    2\n#> site4    0    0    0    0    5    1    0\n#> site5    1    0    0    2    0    0    0\n#> site6    0    0    0    0    0    4    0\n\n## Fixar a amostragem\nset.seed(1001) \n\n## Matrizes de distância de PCoA \nd_macro <- vegdist(macroinv, \"bray\")\npcoa_macro <- cmdscale(d_macro)\n\nd_fish <- vegdist(fish_comm, \"bray\")\npcoa_fish <- cmdscale(d_fish)\n\n## PROCRUSTES\nconcord <- procrustes(pcoa_macro, pcoa_fish)\nprotest(pcoa_macro, pcoa_fish)\n#> \n#> Call:\n#> protest(X = pcoa_macro, Y = pcoa_fish) \n#> \n#> Procrustes Sum of Squares (m12 squared):        0.6185 \n#> Correlation in a symmetric Procrustes rotation: 0.6176 \n#> Significance:  0.019 \n#> \n#> Permutation: free\n#> Number of permutations: 999\n\n## Gráfico\nplot(concord, main = \"\", \n     xlab = \"Dimensão 1\",\n     ylab = \"Dimensão 2\")"},{"path":"cap9.html","id":"métodos-multivariados-baseados-em-modelos","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.10 Métodos multivariados baseados em modelos","text":"Mais recentemente, alguns autores têm proposto métodos multivariados baseados em modelos. Estes métodos têm se diversificado hoje em dia e existem pacotes que realizam praticamente todas análises que vimos acima, com diferença que utilizam distribuições de probabilidade dado um modelo (e.g., Poisson) ao invés de coeficientes de dissimilaridades. Vamos exemplificar o uso de um desses métodos, mas não vamos fazer uma revisão extensiva. Caso o() leitor() deseje conhecer mais, recomendamos consultar principalmente os pacotes gllvm, ecoCopula e Hmsc.Um dos primeiros métodos foi proposto em 2012 pelo grupo de David Warton (Warton, Wright, Wang 2012), implementado pacote mvabund. Anteriormente neste mesmo ano foi publicado um artigo pelo mesmo grupo (Wang et al. 2012) em que os autores demonstraram que métodos baseados em dissimilaridade, especialmente PERMANOVA, não conseguem modelar adequadamente dados multivariados de contagem (e.g., abundância) por não levarem em conta relação monotônica entre média e variância. Vejamos o que isso quer dizer utilizando os dados artigo de da Silva et al. (2017) (Figura 9.24).\nFigura 9.24: Gráfico mostrando média-variância para dados de abundância multivariada.\nEste conjunto de dados contém abundâncias de anfíbios coletados num conjunto de cidades interior de São Paulo e também sua presença em museus. Aqui, vemos que há uma clara relação monotônica entre média e variância da abundância das espécies na matriz. Ou seja, à medida que espécies aumentam sua média de abundância, elas também aumentam quase que proporcionalmente sua variância. Esta é uma propriedade bastante comum de dados multivariados de contagem, mas que não é muito bem modelada por métodos baseados em dissimilaridade, já que nesse caso apenas espécies mais abundantes têm de fato um peso na análise.proposta implementada pacote mvabund é baseada em Modelos Lineares Generalizados (GLMs) que você já conhece Capítulo 8. Ou seja, este método permite que sejam modeladas abundâncias das espécies num contexto multivariado utilizando explicitamente distribuições de probabilidade, tais como aquelas que utilizamos (e.g., Poisson, Binomial negativa, etc.). Isso faz com que demos peso semelhante às espécies. Vejamos um exemplo de como o modelo é ajustado e como é feito teste de hipótese.Exemplo 1A principal pergunta artigo citado acima foi testar se composição de espécies que obtemos em campo e de coleção científicas é diferente. Isso tem implicações para levantamentos de fauna, já que podemos complementar um tipo de dado com outro ou saber se eles são redundantes. Aqui, grupos é apenas um fator que indica se aquela linha da matriz apresenta dados obtidos em campo ou coleção. Vamos testar hipótese nula de que não há diferença na composição entre duas fontes de dados ajustando um modelo com distribuição binomial negativa (Figura 9.25).\nFigura 9.25: Gráfico mostrando abundâncias para os dois grupos.\nO gráfico mostrando os dados possui log da abundância das espécies e o fator (campo vs. coleção). Com essa análise exploratória de dados já podemos identificar se há diferença ou não na abundância das espécies em cada local. Vemos que algumas espécies são bem mais abundantes campo que em coleções, tais como Leptodactylus mystacinus e Rhinella ornata.Uma grande vantagem desse tipo de método baseado em modelos é que, assim como um GLM típico, também podemos realizar diagnose dos resíduos utilizando plots de resíduos versus predito. Vejamos como isso funciona (Figura 9.26).\nFigura 9.26: Diagnose dos resíduos modelo ajustado.\nAqui vemos que os dados se distribuem de maneira mais ou menos homogênea e sem um padrão claro ao redor zero. Isso indica que distribuição Binomial negativa foi adequada para modelar estes dados. E agora sim podemos interpretar os resultados com mais segurança.Interpretação dos resultadosAqui vemos que há sim uma diferença significativa na abundância relativa das espécies entre o campo e coleção. Mas até então estamos levando em conta toda matriz numa abordagem multivariada. entanto, não sabemos qual() espécie(s) são responsáveis por este padrão. Para investigar isso, podemos realizar uma Análise de Deviance separada para cada espécie, isso é o que linha abaixo faz.Aqui vemos que algumas espécies já identificadas plot exploratório de fato são importantes para determinar o padrão, tais como: Leptodactylus mystacinus, Physalaemus nattereri e Rhinella ornata. Essas foram espécies cuja Deviance foi significativa.","code":"\n## Dados com seis primeiras localidades e espécies\nhead(anuros_permanova)[1:6]\n#>                   Chiasmocleis_albopunctata Dendropsophus_elianae Dendropsophus_jimi Dendropsophus_nanus Dendropsophus_minutus\n#> Araras                                    6                     0                  0                   0                     1\n#> Artur Nogueira                            0                     0                  0                   0                     6\n#> Conchal                                   0                     0                  0                   0                     0\n#> Engenheiro Coelho                         1                     0                  0                   0                     2\n#> Jaguari\\xfana                             8                     0                  0                   0                     0\n#> Limeira                                   0                    11                 15                   0                    15\n#>                   Dendropsophus_sanborni\n#> Araras                                 0\n#> Artur Nogueira                         0\n#> Conchal                                0\n#> Engenheiro Coelho                      0\n#> Jaguari\\xfana                          0\n#> Limeira                                0\n\n#retirando a coluna que contém o fator e deixando apenas dados de abundância\nanuros_abund <- as.data.frame(anuros_permanova[,-28])\n\n#incluindo apenas o fator a ser testado no modelo\ngrupos <- factor(anuros_permanova[,28])\n\n## Média-variância\n# criando um objeto da classe mvabund com os dados de abundância\nabund_tr <- mvabund(anuros_abund) \nmeanvar.plot(abund_tr)\n## Gráfico\nplot(abund_tr ~ grupos, cex.axis = 0.8, cex = 0.8)\n#> \n#>  PIPING TO 2nd MVFACTOR\n\n## Modelo\nmodelo1 <- manyglm(abund_tr ~ grupos, family = \"negative.binomial\")\n## Diagnose\nplot(modelo1)\n## Resultados\nsummary(modelo1)\n#> \n#> Test statistics:\n#>               wald value Pr(>wald)    \n#> (Intercept)       18.947     0.001 ***\n#> gruposCOLECAO      4.727     0.004 ** \n#> --- \n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n#> \n#> Test statistic:  4.727, p-value: 0.004 \n#> Arguments:\n#>  Test statistics calculated assuming response assumed to be uncorrelated \n#>  P-value calculated using 999 resampling iterations via pit.trap resampling (to account for correlation in testing).\n## ANOVA\nanova(modelo1, p.uni = \"adjusted\")\n#> Time elapsed: 0 hr 0 min 6 sec\n#> Analysis of Deviance Table\n#> \n#> Model: abund_tr ~ grupos\n#> \n#> Multivariate test:\n#>             Res.Df Df.diff   Dev Pr(>Dev)   \n#> (Intercept)      8                          \n#> grupos           7       1 94.08     0.01 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Univariate Tests:\n#>             Chiasmocleis_albopunctata          Dendropsophus_elianae          Dendropsophus_jimi          Dendropsophus_nanus         \n#>                                   Dev Pr(>Dev)                   Dev Pr(>Dev)                Dev Pr(>Dev)                 Dev Pr(>Dev)\n#> (Intercept)                                                                                                                           \n#> grupos                          3.308    0.467                 0.394    0.836               0.23    0.836               2.197    0.707\n#>             Dendropsophus_minutus          Dendropsophus_sanborni          Elachistocleis_cesari          Hypsiboas_albopunctatus         \n#>                               Dev Pr(>Dev)                    Dev Pr(>Dev)                   Dev Pr(>Dev)                     Dev Pr(>Dev)\n#> (Intercept)                                                                                                                               \n#> grupos                      4.752    0.309                  2.197    0.707                 2.012    0.762                   2.069    0.762\n#>             Hypsiboas_faber          Hypsiboas_lundii          Hypsiboas_prasinus          Itapotihyla_langsdorffii          Leptodactylus_fuscus\n#>                         Dev Pr(>Dev)              Dev Pr(>Dev)                Dev Pr(>Dev)                      Dev Pr(>Dev)                  Dev\n#> (Intercept)                                                                                                                                      \n#> grupos                0.913    0.822            0.915    0.822              0.913    0.822                    0.911    0.836                4.652\n#>                      Leptodactylus_latrans          Leptodactylus_mystaceus          Leptodactylus_mystacinus          Physalaemus_centralis\n#>             Pr(>Dev)                   Dev Pr(>Dev)                     Dev Pr(>Dev)                      Dev Pr(>Dev)                   Dev\n#> (Intercept)                                                                                                                                 \n#> grupos         0.309                 0.811    0.836                   1.869    0.783                    9.772    0.045                 2.639\n#>                      Physalaemus_cuvieri          Physalaemus_marmoratus          Physalaemus_nattereri          Pseudis_paradoxa         \n#>             Pr(>Dev)                 Dev Pr(>Dev)                    Dev Pr(>Dev)                   Dev Pr(>Dev)              Dev Pr(>Dev)\n#> (Intercept)                                                                                                                               \n#> grupos         0.670               8.221    0.062                  1.622    0.783                17.131    0.014             2.67    0.540\n#>             Rhinella_ornata          Rhinella_schneideri          Scinax_fuscomarginatus          Scinax_fuscovarius          Scinax_similis\n#>                         Dev Pr(>Dev)                 Dev Pr(>Dev)                    Dev Pr(>Dev)                Dev Pr(>Dev)            Dev\n#> (Intercept)                                                                                                                                 \n#> grupos               11.961    0.027               3.363    0.467                   2.64    0.622              1.655    0.783          2.646\n#>                      Trachycephalus_typhonius         \n#>             Pr(>Dev)                      Dev Pr(>Dev)\n#> (Intercept)                                           \n#> grupos         0.622                    1.622    0.783\n#> Arguments:\n#>  Test statistics calculated assuming uncorrelated response (for faster computation) \n#> P-value calculated using 999 iterations via PIT-trap resampling."},{"path":"cap9.html","id":"para-se-aprofundar-5","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.11 Para se aprofundar","text":"","code":""},{"path":"cap9.html","id":"livros-4","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.11.1 Livros","text":"Listamos seguir livros e artigos com material que recomendamos para seguir com sua aprendizagem em análises multivariadas em R: ) Legendre & Legendre (2012) Numerical Ecology, ii) Borcard e colaboradores (2018) Numerical Ecology R, iii) Thioulouse e colaboradores (2018) Multivariate Analysis Ecological Data ade4, iv) Ovaskainen & Abrego (2020) Joint Species Distribution Modelling, v) James & McCulloch (1990) Multivariate Analysis Ecology Systematics: Panacea Pandora’s Box? e vi) Dunstan e colaboradores (2011) Model based grouping species across environmental gradients.","code":""},{"path":"cap9.html","id":"links-4","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.11.2 Links","text":"Existem alguns tutoriais online bem interessantes, mas todos em inglês. Veja lista abaixo:Análises de ordenação - 1Análises de ordenação - 2Classificação numérica - agrupamento e kmeans","code":""},{"path":"cap9.html","id":"exercícios-5","chapter":"Capítulo 9 Análises Multidimensionais","heading":"9.12 Exercícios","text":"9.1\nUtilize os dados “mite” pacote vegan para testar o efeito de variáveis ambientais sobre composição de espécies de ácaros utilizando seguintes análises: RDA, RDAp (combinada com MEM), dbRDA e PERMANOVA. Após realizar cinco análises, responda às seguintes perguntas?. Quais são variáveis ambientais (mite.env) mais importantes para composição de ácaros em cada uma das análises?\nB. Os vetores espaciais obtidos com análise MEM explicam variação na composição de espécies? Eles são mais ou menos importantes que variáveis ambientais?\nC. Discuta diferenças de interpretação entre RDA, RDAp, dbRDA e PERMANOVA.Dados necessários:9.2\nEfetue uma análise de agrupamento pela função hclust(). Lembre-se de dar nome ao objeto para poder plotar o dendrograma depois. Utilize ajuda para encontrar como entrar com os argumentos da função.utilizando o método UPGMA e o índice de Bray-Curtis.Faça agora o dendrograma com outro índice de dissimilaridade e compare os resultados. São diferentes? que eles influenciariam interpretação resultado?9.3\nNa perspectiva de metacomunidades (Leibold et al., 2004), dispersão dos organismos tem um papel proeminente para entender como espécies estão distribuídas na natureza. Com o objetivo de testar se dispersão influencia composição de espécies de cladóceros e copépodos, e portanto estrutura da metacomunidade, um pesquisador selecionou dois conjuntos de lagos: em um deles todos os lagos são isolados e outro os lagos são conectados.Importe o conjunto de dados lagos pacote ecodados e responda pergunta se o fato de os lagos estarem conectados ou não influencia composição de espécies desses microcrustáceos. Utilize métodos baseados em modelos que você aprendeu ao longo capítulo para modelar abundância multivariada.Importe o conjunto de dados lagos pacote ecodados e responda pergunta se o fato de os lagos estarem conectados ou não influencia composição de espécies desses microcrustáceos. Utilize métodos baseados em modelos que você aprendeu ao longo capítulo para modelar abundância multivariada.Faça um plot mostrando abundância relativa das espécies com maior abundância e veja se elas são diferentes entre os tipos lagos. Combine este resultado com o item anterior para interpretar o resultado final.Faça um plot mostrando abundância relativa das espécies com maior abundância e veja se elas são diferentes entre os tipos lagos. Combine este resultado com o item anterior para interpretar o resultado final.9.4\nCarregue o pacote MASS para utilizar os dados crabs. Este conjunto traz medidas morfológicas de dois morfo-tipos da espécie de carangueijo Leptograpsus variegatus coletada em Fremantle, Austrália. Calcule uma PCA e veja se existe uma semelhança morfológica entre os dois morfo-tipos. Lembre-se de dar nome\nao objeto e use função biplot() para plotar o resultado teste. Dica: projeção de um objeto perpendicular à seta descritor fornece posição aproximada objeto ao longo desse descritor. distância dos objetos espaço cartesiano reflete distância euclidiana entre eles.Soluções dos exercícios.","code":"\ndata(mite)\ndata(mite.env)\ndata(mite.xy)"},{"path":"cap10.html","id":"cap10","chapter":"Capítulo 10 Rarefação","heading":"Capítulo 10 Rarefação","text":"","code":""},{"path":"cap10.html","id":"pré-requisitos-do-capítulo-6","chapter":"Capítulo 10 Rarefação","heading":"Pré-requisitos do capítulo","text":"Pacotes e dados que serão utilizados neste capítulo.","code":"\n## Pacotes\nlibrary(iNEXT)\nlibrary(ecodados)\nlibrary(ggplot2)\nlibrary(vegan)\nlibrary(nlme)\nlibrary(dplyr)\nlibrary(piecewiseSEM)\n\n## Dados\ndata(\"mite\")\ndata(\"mite.xy\")\ncoord <- mite.xy\ncolnames(coord) <- c(\"long\", \"lat\")\ndata(\"mite.env\")\nagua <- mite.env[, 2]\ndados_rarefacao <- ecodados::rarefacao_morcegos\nrarefacao_repteis <- ecodados::rarefacao_repteis\nrarefacao_anuros <- ecodados::rarefacao_anuros\ndados_amostras <- ecodados::morcegos_rarefacao_amostras"},{"path":"cap10.html","id":"aspectos-teóricos-1","chapter":"Capítulo 10 Rarefação","heading":"10.1 Aspectos teóricos","text":"Uma das grandes dificuldades na comparação da riqueza de espécies (número de espécies) entre comunidades é decorrente da diferença esforço amostral (e.g. diferença número de indivíduos, discrepância na quantidade de unidades amostrais ou área amostrada) que inevitavelmente influenciará número de espécies observadas (Gotelli Ellison 2012; Roswell, Dushoff, Winfree 2021). O método de rarefação nos permite comparar o número de espécies entre comunidades quando o tamanho da amostra (e.g. número de unidades amostrais), o esforço amostral (e.g. tempo de amostragem) ou número de indivíduos não são iguais. rarefação calcula o número esperado de espécies em cada comunidade tendo como base comparativa um valor em que todas amostras atinjam um tamanho padrão. Gotelli & Colwell (2001) descrevem dois tipos de curvas de rarefação: ) baseada em indivíduos (individual-based) - comparações são feitas considerando abundância da comunidade padronizada pelo menor número de indivíduos, e ii) baseada na amostra (sampled-based) - comparações são padronizadas pela comunidade com menor número de amostragens.O método foi formulado considerando seguinte pergunta: Se considerarmos n indivíduos ou amostras (n < N) para cada comunidade, quantas espécies registraríamos nas comunidades considerando o mesmo número de indivíduos ou amostras?Gotelli & Colwell (2001) descrevem este método e discutem em detalhes restrições sobre seu uso na ecologia.amostras serem comparadas devem ser consistentes ponto de vista taxonômico, ou seja, todos os indivíduos devem pertencer ao mesmo grupo taxonômicoAs comparações devem ser realizadas somente entre amostras com mesmas técnicas de coleta. Por exemplo, não é recomendado comparar amostras onde riqueza de espécies de anuros de uma amostra foi estimada utilizando armadilhas de interceptação e queda e outra foi estimada por vocalizações em sítios de reproduçãoOs tipos de hábitat onde amostras são obtidas devem ser semelhantesÉ um método para estimar riqueza de espécies em uma amostra menor – não pode ser usado para extrapolar riqueza para amostras maiores 📝 Importante \nEsta última restrição foi superada por Colwell et al. (2012) e Chao & Jost (2012), que desenvolveram uma nova abordagem onde os dados podem ser interpolados (rarefeito) para amostras menores e extrapolados para amostras maiores. Além disso, Chao & Jost (2012) propõem curva de rarefação coverage-based que padroniza amostras pela cobertura ou totalidade (completeness) da amostra ao invés tamanho. rarefações tradicionais apresentam limitações matemáticas que são superadas por essa nova abordagem (Chao Jost 2012).","code":""},{"path":"cap10.html","id":"curva-de-rarefação-baseada-no-indivíduo-individual-based","chapter":"Capítulo 10 Rarefação","heading":"10.2 Curva de rarefação baseada no indivíduo (individual-based)","text":"Exemplo prático 1 - MorcegosExplicação dos dadosUsaremos os dados de espécies de morcegos amostradas em três fragmentos florestais (Breviglieri 2008): ) Mata Ciliar Córrego Talhadinho com 12 hectares, ii) Mata Ciliar Córrego dos Tenentes com 10 hectares, e iii) Fazenda Experimental de Pindorama com 128 hectares.PerguntaA riqueza de espécies de morcegos é maior na Fazenda Experimental que nos fragmentos florestais menores?PrediçõesO número de espécies será maior em fragmentos florestais maioresVariáveisMatriz ou data frame com abundâncias das espécies de morcegos (variável resposta) registradas nos três fragmentos florestais (variável preditora)ChecklistVerificar se sua matriz ou data frame estão com espécies nas linhas e os fragmentos florestais nas colunasAnáliseVamos olhar os dados usando função head().Usaremos funções pacote iNEXT (iNterpolation e EXTrapolation) para o cálculo da rarefação (Hsieh, Ma, Chao 2016). Esta função permite estimar riqueza de espécies utilizando família Hill-numbers (Hill 1973; explicação dos conceitos da família Hill-numbers está detalhada Capítulo 12. O argumento q refere-se família Hill-numbers onde: 0 = riqueza de espécies, 1 = diversidade de Shannon e 2 = diversidade de Simpson. exemplo abaixo, utilizamos somente q = 0.Vamos visualizar os resultados (Figura 10.1).\nFigura 10.1: Curvas de rarefação baseada nos indivíduos de morcegos.\nInterpretação dos resultadosForam registrados 166 indivíduos na MC_Tenentes, 413 na MC_Talhadinho e 223 na FF_Experimental. Lembrando, você não pode comparar riqueza de espécies observada diretamente: 15 espécies na MC_Tenentes, 19 espécies na MC_Talhadinho, e 17 espécies FF_Experimental. comparação da riqueza de espécies entre comunidades deve ser feita com base na riqueza de espécies rarefeita, que é calculada com base número de indivíduos da comunidade com menor abundância (166 indivíduos - linha preta tracejada). Olhando o gráfico é possível perceber que riqueza de espécies de morcegos rarefeita não é diferente entre os três fragmentos florestais quando corrigimos o problema da diferença na abundância pela rarefação. interpretação é feita com base intervalo de confiança de 95%. curvas serão diferentes quando os intervalos de confiança não se sobreporem (Chao et al. 2014). Percebam que esta abordagem, além da interpolação (rarefação), também realiza extrapolações que podem ser usadas para estimar o número de espécies caso o esforço de coleta fosse maior. Este é o assunto Capítulo 11.Exemplo prático 2 - Anuros e RépteisExplicação dos dadosNeste exemplo, iremos comparar o número de espécies de anuros e répteis (serpentes e lagartos) usando informações dos indivíduos depositados em coleções científicas e coletas de campo (da Silva et al. 2017).PerguntaA riqueza de espécies estimada para uma mesma região é maior usando informações de coleções científicas que informações de coletas de campo?PrediçõesO número de espécies será maior em coleções científicas devido ao maior esforço amostral (.e. maior variação temporal para depositar os indivíduos e maior número de pessoas contribuindo com coletas esporádicas)VariáveisMatriz ou data frame com abundâncias das espécies de anuros e répteis (variável resposta) registradas em coleções científicas e coletas de campo (variável preditora)ChecklistVerificar se sua matriz ou data frame estão com espécies nas linhas e fonte dos dados nas colunasAnáliseOlhando os dados dos répteis.Análise usando o pacote iNEXT (Figura 10.2).\nFigura 10.2: Curvas de rarefação baseada nos indivíduos de répteis.\nInterpretação dos resultadosForam registradas oito espécies de répteis nas coletas de campo (48 indivíduos) e 28 espécies nas coleções científicas (77 indivíduos). Com base na rarefação, concluímos que riqueza de espécies de répteis obtida nas coleções científicas (20,5) é 2,5 vezes maior que obtida em coletas de campo.Olhando os dados dos anuros.Análise e visualização gráfico (Figura 10.3).\nFigura 10.3: Curvas de rarefação baseada nos indivíduos de anuros.\nInterpretação dos resultados - anurosForam registradas 21 espécies de anuros nas coletas de campo (675 indivíduos) e 12 espécies nas coleções científicas (37 indivíduos). Com base na rarefação, concluímos que não há diferença entre riqueza de espécies de anuros obtida em coletas de campo e coleções científicas.","code":"\n## Cabeçalho dos dados\nhead(dados_rarefacao)\n#>                       MC_Tenentes MC_Talhadinho FF_Experimental\n#> Chrotopterus_auritus            0             1               1\n#> Phyllostomus_hastatus           0             1               0\n#> Phyllostomus_discolor           0             2               2\n#> Artibeus_lituratus             17            26              26\n#> Artibeus_obscurus               1             4               6\n#> Artibeus_planirostris          34            72              52\n\n## Número de indivíduos por local\ncolSums(dados_rarefacao)\n#>     MC_Tenentes   MC_Talhadinho FF_Experimental \n#>             166             413             223\n## Rarefação\n# Datatype refere-se ao tipo de dados que você vai analisar (e.g. abundância, incidência).\n# Endpoint refere-se ao valor máximo que você determina para a extrapolação.\nresultados_morcegos <- iNEXT(dados_rarefacao, q = 0, \n                             datatype = \"abundance\", endpoint = 800)\n## Gráfico\n# type define o tipo de curva de rarefação\n# 1 = curva de rarefação baseada no indivíduo ou amostra\n# 2 = curva de representatividade da amostra\n# 3 = curva de rarefação baseada na representatividade (coverage-based)\n\nggiNEXT(resultados_morcegos, type = 1) +\n    geom_vline(xintercept = 166, lty = 2) +\n    scale_linetype_discrete(labels = c(\"Interpolado\", \"Extrapolado\")) +\n    scale_colour_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    scale_fill_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    labs(x = \"Número de indivíduos\", y = \" Riqueza de espécies\") +\n    tema_livro()\n## Cabeçalho\nhead(rarefacao_repteis)\n#>                         Coleta.Campo Colecoes.Cientificas\n#> Ameiva_ameiva                      1                    0\n#> Amphisbaena_mertensii              1                    0\n#> Apostolepis_dimidiata              0                    1\n#> Bothrops__itapetiningae            0                    2\n#> Bothrops__pauloensis               0                    1\n#> Bothrops_alternatus                0                    1\n## Análise\nresultados_repteis <- iNEXT(rarefacao_repteis, q = 0,\n                            datatype = \"abundance\", \n                            endpoint = 200)\n\n## Visualizar os resultados\nggiNEXT(resultados_repteis, type = 1) +\n    geom_vline(xintercept = 48, lty = 2) +\n    scale_linetype_discrete(labels = c(\"Interpolado\", \"Extrapolado\")) +\n    scale_colour_manual(values = c(\"darkorange\", \"cyan4\")) +\n    scale_fill_manual(values = c(\"darkorange\", \"cyan4\")) +\n    labs(x = \"Número de indivíduos\", y = \" Riqueza de espécies\") +\n    tema_livro()\n## Cabeçalho\nhead(rarefacao_anuros)\n#>                           Coleta.Campo Colecoes.Cientificas\n#> Chiasmocleis_albopunctata           15                    0\n#> Dendropsophus_elianae               11                    1\n#> Dendropsophus_jimi                  15                    2\n#> Dendropsophus_nanus                  0                    1\n#> Dendropsophus_minutus               24                    0\n#> Dendropsophus_sanborni               0                    1\n## Análise\nresultados_anuros <- iNEXT(rarefacao_anuros, q = 0, \n                           datatype = \"abundance\", endpoint = 800)\n\n## Visualizar os resultados\nggiNEXT(resultados_anuros, type = 1) + \n    geom_vline(xintercept = 37, lty = 2) +\n    scale_linetype_discrete(labels = c(\"Interpolado\", \"Extrapolado\")) +\n    scale_colour_manual(values = c(\"darkorange\", \"cyan4\")) +\n    scale_fill_manual(values = c(\"darkorange\", \"cyan4\")) +\n    labs(x = \"Número de indivíduos\", y = \" Riqueza de espécies\") +\n    tema_livro()"},{"path":"cap10.html","id":"curva-de-rarefação-baseada-em-amostras-sample-based","chapter":"Capítulo 10 Rarefação","heading":"10.3 Curva de rarefação baseada em amostras (sample-based)","text":"Exemplo prático 3 - MorcegosExplicação dos dadosUsaremos os mesmos dados de espécies de morcegos amostradas em três fragmentos florestais (Breviglieri 2008). Contudo, ao invés de padronizarmos riqueza de espécies pela abundância, iremos padronizar pelo número de amostras.VariáveisLista de vetores. Cada vetor deve conter como primeira informação o número total de amostras (variável preditora), seguido da frequência de ocorrência das espécies (.e. número de amostras em que cada espécie foi registrada - variável resposta)ChecklistVerificar se sua lista está com o número total de amostras e frequência de ocorrência das espéciesAnáliseVamos olhar os dados.Vamos criar uma lista com amostragens de cada comunidade e os códigos da análise.Visualizar os resultados (Figura 10.4).\nFigura 10.4: Curvas de rarefação baseada em amostras de morcegos.\nInterpretação dos resultadosOlhando o gráfico é possível perceber que riqueza de espécies de morcegos rarefeita não é diferente entre os três fragmentos florestais, quando corrigimos o problema da diferença número de amostras.","code":"\n## Cabeçalho\nhead(dados_amostras)\n#>          MC_Tenentes MC_Talhadinho FF_Experimental\n#> amostras          12            20              12\n#> sp1               12            20              12\n#> sp2               12            19              10\n#> sp3               10            15               8\n#> sp4                8            10               8\n#> sp5                6             7               7\n## Dados\n# Usamos [,] para excluir os NAs. Lembrando que valores antes da \n# vírgula representam as linhas e os posteriores representam as colunas.\nlista_rarefacao <- list(Tenentes = dados_amostras[1:18, 1],\n                        Talhadinho = dados_amostras[, 2],\n                        Experimental = dados_amostras[1:16, 3])\n\n## Análise\nres_rarefacao_amostras <- iNEXT(lista_rarefacao, q = 0, \n                                datatype = \"incidence_freq\")\n## Gráfico\nggiNEXT(res_rarefacao_amostras , type = 1, color.var = \"site\") + \n    geom_vline(xintercept = 12, lty = 2) +\n    scale_linetype_discrete(name = \"Método\", labels = c(\"Interpolado\", \"Extrapolado\")) +\n    scale_colour_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    scale_fill_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    labs(x = \"Número de amostras\", y = \" Riqueza de espécies\") +\n    tema_livro()"},{"path":"cap10.html","id":"curva-de-rarefação-coverage-based","chapter":"Capítulo 10 Rarefação","heading":"10.4 Curva de rarefação coverage-based","text":"Exemplo prático 4 - MorcegosExplicação dos dadosNeste exemplo, usaremos os mesmos dados de espécies de morcegos amostradas em três fragmentos florestais (Breviglieri 2008).AnáliseOs códigos para realização da rarefação coverage-based são idênticos aos utilizados para o cálculo das curvas de rarefações baseadas nas abundâncias e amostras. Portanto, não repetiremos linhas de código aqui e utilizaremos os resultados já calculados para visualização dos gráficos. Para isso, digitamos type = 3 que representa curva de rarefação coverage-based (Figura 10.5).\nFigura 10.5: Curvas de rarefação baseada coverage-based de morcegos.\nInterpretação dos resultadosCoverage é uma medida que determina proporção de amostras (sampled-based) ou número de indivíduos (abundance-based) da comunidade que representa espécies presentes na amostra. Um valor de coverage = 0,85 representa riqueza estimada com base em 85% das amostragens ou da abundância da comunidade. nosso exemplo, os valores de coverage foram acima de 0,93, indicando que precisamos de praticamente todas amostras para estimar riqueza observada em cada comunidade. Comparando comunidades considerando o mesmo valor de coverage 0,937 na comunidade Tenentes, identificamos que riqueza de espécies de morcegos estimada na comunidade Experimental é menor que estimada para comunidade de Talhadinho (não há sobreposição intervalo de confiança). Percebam que usando curva de rarefação coverage-based, interpretação dos resultados foi diferente das observadas usando curvas baseadas nos indivíduos ou amostras. Veja Chao & Jost (2012) e Roswell et al. (2021) para explicações mais detalhadas sobre esta metodologia.Exemplo prático 5 - Generalized Least Squares (GLS)Explicação dos dadosNeste exemplo, iremos refazer o exercício Capítulo 7 onde usamos Generalized Least Squares (GLS) para testar relação da riqueza de ácaros com quantidade de água substrato. Contudo, ao invés de considerar riqueza de espécies de ácaros observada como variável resposta, iremos utilizar riqueza rarefeita para controlar o efeito da amostragem (.e. diferentes abundâncias entre comunidades). Os dados que usaremos estão disponíveis pacote vegan e representam composição de espécies de ácaros amostradas em 70 amostras.PerguntaA riqueza rarefeita de espécies de ácaros é maior em comunidades localizadas em áreas com substratos secos?PrediçõesO número de espécies rarefeita será maior em substratos secos, uma vez que limitações fisiológicas impostas pela umidade limitam ocorrência de várias espécies de ácarosVariáveisMatriz ou data frame com abundâncias das espécies de ácaros (variável resposta) registradas em 70 comunidades (variável preditora)ChecklistVerificar se sua matriz ou data frame estão com espécies nas linhas e comunidades nas colunasAnáliseVamos calcular riqueza rarefeita com base na comunidade com menor abundância.Vamos calcular riqueza rarefeita de espécies para todas comunidades considerando menor abundância.Para padronizar e facilitar extração dos resultados, definimos os argumentos knots (.e. representa o intervalo igualmente espaçado que função irá utilizar para determinar riqueza estimada) e endpoint (.e. o valor final de amostras ou abundância extrapolados) com o valor de abundância = 8.Lembrando, estamos interessados valor rarefeito considerando abundância de 8 indivíduos. Esta informação está armazenada na linha 8 e na coluna 4 dos data frames salvos objeto resultados_rarefação$iNextEst. Assim, para obtermos o valor rarefeito de interesse, vamos criar um loop para facilitar extração da riqueza rarefeita para 70 comunidades.Vamos juntar esses resultados com os dados geográficos e ambientais.Agora, seguindo os passos descritos Capítulo 7, vamos identificar o melhor modelo que representa estrutura espacial dos dados da riqueza rarefeita.Agora vamos usar seleção de modelo por AIC para selecionar o modelo mais “provável” explicando distribuição da riqueza rarefeita das espécies de ácaros (Figura 10.6).\nFigura 10.6: Visualização dos resíduos modelo selecionado.\nPercebam que os pontos estão dispersos gráfico e não apresentam padrões que indiquem heterogeneidade de variância (Figura 10.7).\nFigura 10.7: Gráfico modelo GLS selecionado.\nInterpretação dos resultadosA concentração de água substrato explica 29,9% da variação na riqueza rarefeita das espécies de ácaros. Como predito, riqueza de espécies de ácaros foi maior em comunidades localizadas em áreas com substratos secos que em áreas com substratos úmidos (t = -4.71, df = 68, P < 0.01).","code":"\n## Gráfico\n# Visualizar os resultados da rarefação *coverage-based*. \nggiNEXT(res_rarefacao_amostras, type = 3, color.var = \"site\") + \n    geom_vline(xintercept = 0.937, lty = 2) +\n    scale_linetype_discrete(labels = c(\"Interpolado\", \"Extrapolado\")) +\n    scale_colour_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    scale_fill_manual(values = c(\"darkorange\", \"darkorchid\", \"cyan4\")) +\n    labs(x = \"Representatividade nas amostras\", y = \"Riqueza de espécies\") +\n    tema_livro()\n## Menos abundância\n# Os dados estão com as comunidades nas colunas e as espécies nas linhas. \n# Para as análises teremos que transpor a planilha.\ncomposicao_acaros <- as.data.frame(t(mite))\n\n# Verificar qual é a menor abundância registrada nas comunidades. \nabun_min <- min(colSums(composicao_acaros))\n## Riqueza rarefeita\nresultados_rarefacao <- iNEXT(composicao_acaros, \n                              q = 0, \n                              datatype = \"abundance\", \n                              knots = abun_min,\n                              endpoint = abun_min)\n## Riqueza rarefeita para cada comunidade\nriqueza_rarefeita <- c()\n\nfor (i in 1:70) {\n  resultados_comunidades <- resultados_rarefacao$iNextEst[[i]]\n  subset_res <- resultados_comunidades %>% dplyr::filter(m==abun_min)\n  riqueza_rarefeita[i] <- subset_res$qD\n}\n## Dados finais\n# Agrupando os dados em um data frame final.\ndados_combinado <- data.frame(riqueza_rarefeita, agua, coord)\n## Criando diferentes modelos usando a função gls\n## Sem estrutura espacial\nno_spat_gls <- gls(riqueza_rarefeita ~ agua, data = dados_combinado, \n                   method = \"REML\")\n\n## Covariância esférica \nespher_model <- gls(riqueza_rarefeita ~ agua, data = dados_combinado, \n                    corSpher(form = ~lat + long, nugget = TRUE))\n\n## Covariância exponencial \nexpon_model <- gls(riqueza_rarefeita ~ agua, data = dados_combinado, \n                   corExp(form = ~lat + long, nugget = TRUE))\n\n## Covariância Gaussiana \ngauss_model <- gls(riqueza_rarefeita ~ agua, data = dados_combinado, \n                   corGaus(form = ~lat + long, nugget = TRUE))\n\n## Covariância razão quadrática \nratio_model <- gls(riqueza_rarefeita ~ agua, data = dados_combinado, \n                   corRatio(form = ~lat + long, nugget = TRUE))\n## Seleção dos modelos\naic_fit <- AIC(no_spat_gls, espher_model, expon_model, \n               gauss_model, ratio_model)\naic_fit %>% arrange(AIC)\n#>   df      AIC\n#> 1  5 164.5840\n#> 2  5 165.7649\n#> 3  5 165.8698\n#> 4  3 166.7530\n#> 5  5 169.0242\n\n## Visualizando os resíduos do modelo selecionado\nplot(gauss_model)\n## Visualizando os resultados\nsummary(gauss_model)$tTable \n#>                    Value    Std.Error   t-value      p-value\n#> (Intercept)  6.086125990 0.2927633291 20.788553 3.550849e-31\n#> agua        -0.003142615 0.0006670097 -4.711498 1.258304e-05\n\n## Calculando o R-squared\nrsquared(gauss_model)\n#>            Response   family     link method R.squared\n#> 1 riqueza_rarefeita gaussian identity   none 0.2991059\n\n## Obtendo os valores preditos pelo modelo\npredito <- predict(gauss_model) \n\n## Plotando os resultados no gráfico\nggplot(data = dados_combinado, aes(x= agua, y= riqueza_rarefeita)) + \n    geom_point(size = 4, shape = 21, fill = \"gray\", alpha = 0.7) +\n    geom_line(aes(y = predito), size = 1) +\n    labs(x = \"Concentração de água no substrato\", \n         y = \"Riqueza rarefeita \\ndas espécies de ácaros\") +\n    tema_livro()"},{"path":"cap10.html","id":"para-se-aprofundar-6","chapter":"Capítulo 10 Rarefação","heading":"10.5 Para se aprofundar","text":"","code":""},{"path":"cap10.html","id":"livros-5","chapter":"Capítulo 10 Rarefação","heading":"10.5.1 Livros","text":"Recomendamos o livro Biological Diversity Frontiers Measurement Assessment (Magurran McGill 2011)","code":""},{"path":"cap10.html","id":"links-5","chapter":"Capítulo 10 Rarefação","heading":"10.5.2 Links","text":"Recomendamos também que acessem página EstimateS software e baixem o manual usuário que contém informações detalhadas sobre os índices de rarefação. Este site foi criado e é mantido pelo Dr. Robert K. Colwell, um dos maiores especialistas mundo em estimativas da biodiversidadeRecomendamos também que acessem página EstimateS software e baixem o manual usuário que contém informações detalhadas sobre os índices de rarefação. Este site foi criado e é mantido pelo Dr. Robert K. Colwell, um dos maiores especialistas mundo em estimativas da biodiversidadeRecomendamos página pessoal da pesquisadora Anne Chao que é uma das responsáveis pelo desenvolvimento da metodologia e pacote iNEXT. Nesta página, vocês irão encontrar exemplos e explicações detalhadas sobre análisesRecomendamos página pessoal da pesquisadora Anne Chao que é uma das responsáveis pelo desenvolvimento da metodologia e pacote iNEXT. Nesta página, vocês irão encontrar exemplos e explicações detalhadas sobre análises","code":""},{"path":"cap10.html","id":"exercícios-6","chapter":"Capítulo 10 Rarefação","heading":"10.6 Exercícios","text":"10.1\nUsando os dados Cap10_exercicio1 disponível pacote ecodados, avalie se diferentes tipos de uso da terra (fragmento florestal, borda de mata, área de pastagem e cana de açúcar) apresentam diferentes riquezas de espécies? Qual sua interpretação? Faça um gráfico com os resultados.10.2\nO estudo é o mesmo exercício anterior. Contudo, ao invés da rarefação baseada na abundância, faça rarefações baseadas número de amostras. Qual sua interpretação considerando os resultados exercício 1? Faça um gráfico com os resultados.10.3\nUse os dados dos exercícios anteriores e calcule rarefação baseada na cobertura de amostragem (coverage-based). Qual sua interpretação considerando os resultados anteriores? Faça um gráfico com os resultados.Soluções dos exercícios.","code":""},{"path":"cap11.html","id":"cap11","chapter":"Capítulo 11 Estimadores de riqueza","heading":"Capítulo 11 Estimadores de riqueza","text":"","code":""},{"path":"cap11.html","id":"pré-requisitos-do-capítulo-7","chapter":"Capítulo 11 Estimadores de riqueza","heading":"Pré-requisitos do capítulo","text":"Pacotes e dados que serão utilizados neste capítulo.","code":"\n## Pacotes\nlibrary(iNEXT)\nlibrary(devtools)\nlibrary(ecodados)\nlibrary(ggplot2)\nlibrary(vegan)\nlibrary(nlme)\nlibrary(dplyr)\nlibrary(piecewiseSEM)\n\n## Dados\ndados_coleta <- poca_anuros\ndata(mite)\ndata(mite.xy)\ncoord <- mite.xy\ncolnames(coord) <- c(\"long\", \"lat\") # altera o nome das colunas\ndata(mite.env)\nagua <- mite.env[, 2] # seleciona a variável de interesse"},{"path":"cap11.html","id":"aspectos-teóricos-2","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.1 Aspectos teóricos","text":"Uma vez que determinar o número total de espécies numa área é praticamente impossível, principalmente em regiões com alta riqueza de espécies, os estimadores são úteis para extrapolar riqueza observada e tentar estimar riqueza total através de uma amostra incompleta de uma comunidade biológica (Walther Moore 2005). Neste capítulo, serão considerados os estimadores não paramétricos que usam informações da frequência de espécies raras na comunidade (Gotelli Chao 2013). Isto porque tanto os testes paramétricos que tentam determinar os parâmetros de uma curva usando o formato da curva de acumulação de espécies (e.g. equação logística, Michaelis-Menten), quanto os testes que usam frequência número de indivíduos para enquadrá-las em uma das distribuições de abundância das espécies (e.g. distribuições log-séries, log-normal) não funcionam muito bem com dados empíricos (Gotelli Chao 2013). Para mais detalhes sobre os testes paramétricos veja Magurran & McGill (2011) e Colwell & Coddington (1994).Para que estimador de riqueza seja considera bom, ele precisa atender quatro características (Chazdon et al. 1998; Hortal, Borges, Gaspar 2006).Independência tamanho da amostra (quantidade de esforço amostral realizado)Insensibilidade diferentes padrões de distribuições (e.g. agrupado, disperso ou aleatório)Insensibilidade em relação à ordem das amostragensInsensibilidade à heterogeneidade entre amostras usadas entre os estudos","code":""},{"path":"cap11.html","id":"estimadores-baseados-na-abundância-das-espécies","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.2 Estimadores baseados na abundância das espécies","text":"","code":""},{"path":"cap11.html","id":"chao-1","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.2.1 CHAO 1","text":"Estimador simples número absoluto de espécies em uma comunidade. É baseado número de espécies raras dentro de uma amostra (Chao 1984, 1987).\\[Chao_{1} = S_{obs} + \\left(\\frac{n-1}{n}\\right)\\frac{F_1(F_1-1)}{2(F_2+1)}\\]onde:Sobs = número de espécies observadas na comunidaden = número de amostrasF1 = número de espécies observadas com abundância de um indivíduo (espécies singleton)F2 = número de espécies observadas com abundância de dois indivíduos (espécies doubletons)O valor de Chao 1 é máximo quando todas espécies menos uma são únicas (singleton). Neste caso, riqueza estimada é aproximadamente o dobro da riqueza observada.Exemplo práticoExplicação dos dadosUsaremos os dados hipotéticos de 17 espécies de anuros amostradas em 14 dias de coletas de campo em um habitat reprodutivo localizado na região noroeste estado de São Paulo, Brasil.PerguntaQuantas espécies mais poderiam ser amostradas caso aumentássemos o esforço amostral até o infinito?PrediçõesO número de espécies estimadas é similar ao número de espécies observadasO número de espécies estimadas é maior que o número de espécies observadasVariáveisdata frame com abundâncias das espécies de anuros (variável resposta) registradas em 14 dias de amostragens (variável preditora) em um habitat reprodutivoChecklistVerificar se sua matriz está com espécies nas colunas e amostragens nas linhasVerificar se os dados são de abundância e não de incidência (presença e ausência)AnáliseVamos olhar os dados.Cálculo estimador de riqueza - Chao 1.Percebam que função retorna: N = número de amostragens, Chao = valor médio da estimativa índice de Chao, 2.5% e 97.5% = intervalo de confiança de 95%, e Std.Dev = desvio padrão. Esses dados são obtidos usando permutações, sem reposição, que alteram ordem das amostragens. Neste exemplo, usamos 100 permutações.Vamos visualizar os resultados com intervalo de confiança de 95% (Figura 11.1).\nFigura 11.1: Resultados com intervalo de confiança de 95% para o estimador Chao 1.\nInterpretação dos resultadosCom base número de espécies raras (singletons e doubletons), o estimador Chao 1 indicou possibilidade de encontrarmos mais três espécies, caso o esforço amostral fosse maior e não estima tendência de estabilização da curva em uma assíntota.","code":"\n## Cabeçalho\nhead(dados_coleta)\n#>       Boana_albopunctata Boana_faber Boana_raniceps Dendropsophus_eliane Dendropsophus_melanargyrius Dendropsophus_minutus Dendropsophus_nanus\n#> Dia_1                  5           0              2                    0                           0                     0                   4\n#> Dia_2                  0           0              0                    0                           0                     2                   0\n#> Dia_3                  0           0              0                    6                           0                     1                   3\n#> Dia_4                  0           0              0                   15                           0                    15                  15\n#> Dia_5                  0           0              0                    2                           1                     8                   2\n#> Dia_6                  1           0              0                    2                           0                     2                   2\n#>       Dermatonotus_muelleri Elachistocleis_bicolor Elachistocleis_sp Leptodactylus_chaquensis Leptodactylus_fuscus Leptodactylus_labyrinthicus\n#> Dia_1                     0                      0                 0                        0                    8                           0\n#> Dia_2                     0                      0                 0                        0                    3                           0\n#> Dia_3                     0                      0                 0                        0                    2                           0\n#> Dia_4                     3                      0                 0                        0                    5                           0\n#> Dia_5                    12                      0                 0                       11                    4                           0\n#> Dia_6                     0                      0                 0                        0                    2                           0\n#>       Physalameus_cuvieri Physalaemus_nattereri Rhinella_schneideri Scinax_fuscovarius\n#> Dia_1                   5                     0                   2                  0\n#> Dia_2                   3                     4                   1                  0\n#> Dia_3                   4                     1                   2                  0\n#> Dia_4                   2                     0                   2                  0\n#> Dia_5                   2                    17                   1                  0\n#> Dia_6                   2                     0                   1                  0\n## Análise\nest_chao1 <- estaccumR(dados_coleta, permutations = 100)\nsummary(est_chao1, display = \"chao\")\n#> $chao\n#>         N      Chao   2.5%    97.5%  Std.Dev\n#> Dia_13  1  7.161667  3.000 12.33333 2.786019\n#> Dia_7   2 10.255429  6.000 18.81250 3.465467\n#> Dia_12  3 11.614500  7.475 19.52500 3.104127\n#> Dia_5   4 13.078167  9.000 20.00000 3.037566\n#> Dia_6   5 14.093333  9.000 22.00000 3.259457\n#> Dia_4   6 14.621667 10.000 22.00000 3.129084\n#> Dia_10  7 15.308333 10.475 22.00000 3.048741\n#> Dia_1   8 16.140000 12.000 22.00000 2.863106\n#> Dia_11  9 16.773333 12.000 22.00000 2.777931\n#> Dia_8  10 17.606667 13.000 22.00000 2.754093\n#> Dia_3  11 18.205000 13.000 22.00000 2.453708\n#> Dia_9  12 18.815000 14.975 22.00000 2.034618\n#> Dia_14 13 19.585000 15.500 22.00000 1.494189\n#> Dia_2  14 20.000000 20.000 20.00000 0.000000\n#> \n#> attr(,\"class\")\n#> [1] \"summary.poolaccum\"\n## Preparando os dados para fazer o gráfico\nresultados <- summary(est_chao1, display = c(\"S\", \"chao\"))\nres_chao <- cbind(resultados$chao[, 1:4], resultados$S[, 2:4])\nres_chao <- as.data.frame(res_chao)\ncolnames(res_chao) <- c(\"Amostras\", \"Chao\", \"C_inferior\", \"C_superior\", \n                        \"Riqueza\", \"R_inferior\", \"R_superior\")\n\n## Gráfico\nggplot(res_chao, aes(y = Riqueza, x = Amostras)) +\n    geom_point(aes(y = Chao, x = Amostras + 0.1), size = 4, \n               color = \"darkorange\", alpha = 0.7) +\n    geom_point(aes(y = Riqueza, x = Amostras), size = 4, \n               color = \"cyan4\", alpha = 0.7) +\n    geom_point(y = 7.5, x = 9, size = 4, color = \"darkorange\", alpha = 0.7) + \n    geom_point(y = 5.9, x = 9, size = 4, color = \"cyan4\", alpha = 0.7) + \n    geom_label(y = 7.5, x = 12, label = \"Riqueza estimada - Chao 1\") +\n    geom_label(y = 5.9, x = 11.3, label = \"Riqueza observada\") + \n    geom_line(aes(y = Chao, x = Amostras), color = \"darkorange\") +\n    geom_line(aes(y = Riqueza, x = Amostras), color = \"cyan4\") +\n    geom_linerange(aes(ymin = C_inferior, ymax = C_superior,\n                       x = Amostras + 0.1), color = \"darkorange\") +\n    geom_linerange(aes(ymin = R_inferior, ymax = R_superior,\n                       x = Amostras), color = \"cyan4\") +\n    scale_x_continuous(limits = c(1, 15), breaks = seq(1, 15, 1)) +\n    labs (x = \"Número de amostras\", y = \"Riqueza estimada - Chao 1\") +\n    tema_livro()"},{"path":"cap11.html","id":"ace---abundance-based-coverage-estimator","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.2.2 ACE - Abundance-based Coverage Estimator","text":"Este método trabalha com abundância das espécies raras (.e. abundância baixa) (Chao Lee 1992; Chao et al. 2000). Entretanto, diferente estimador anterior, esse método permite ao pesquisador determinar os limites para os quais uma espécie seja considerada rara. Em geral, são consideradas raras espécies com abundância entre 1 e 10 indivíduos. riqueza estimada pode variar conforme se aumente ou diminua o limiar de abundância, e infelizmente não existem critérios biológicos definidos para escolha melhor intervalo.\\[ACE = S_{abund} + \\frac{S_{rare}}{C_{ace}} + \\frac{F_1}{C_{ace}}Y_{ace}^2\\]onde:\\[Y_{ace}^2 = max \\left[\\frac{S_{rare}}{C_{ace}}\\frac{\\sum_{=}^{10}(-1)F1}{(N_{rare})({N_{rare} - 1)}}-1,0\\right]\\]\\[Y_{ace}^2 = max \\left[\\frac{S_{rare}}{C_{ace}}\\frac{\\sum_{=}^{10}(-1)F1}{(N_{rare})({N_{rare} - 1)}}-1,0\\right]\\]\\[C_{ace} = 1 - \\frac{F1}{N_{rare}}\\]\\[C_{ace} = 1 - \\frac{F1}{N_{rare}}\\]\\[N_{rare} = \\sum_{=1}^{10}iF_i\\]\\[N_{rare} = \\sum_{=1}^{10}iF_i\\]Exemplo práticoExplicação dos dadosUsaremos os mesmos dados hipotéticos de 17 espécies de anuros amostradas em 14 dias de coletas de campo em um habitat reprodutivo localizado na região Noroeste Estado de São Paulo, Brasil.AnáliseCálculo estimador de riqueza - ACE.Visualizar os resultados com intervalo de confiança de 95% (Figura 11.2).\nFigura 11.2: Resultados com intervalo de confiança de 95% para o estimador ACE.\nInterpretação dos resultadosCom base número de espécies raras (abundância menor que 10 indivíduos - default), o estimador ACE indica possibilidade de encontrarmos mais sete espécies, caso o esforço amostral fosse maior e não estimou tendência de estabilização da curva em uma assíntota.","code":"\n## Análise\nest_ace <- estaccumR(dados_coleta, permutations = 100)\nsummary(est_ace, display = \"ace\")\n#> $ace\n#>         N       ACE      2.5%    97.5%  Std.Dev\n#> Dia_11  1  7.123899  3.545190 13.71429 2.768212\n#> Dia_3   2  9.832864  6.000000 18.42880 2.876526\n#> Dia_2   3 11.395043  7.619618 18.01220 2.668935\n#> Dia_14  4 12.442264  8.000000 17.13587 2.398428\n#> Dia_7   5 13.512461  9.328990 19.24111 2.482220\n#> Dia_8   6 14.249301 10.179603 19.70014 2.608287\n#> Dia_6   7 15.272604 10.712067 21.68808 2.950251\n#> Dia_5   8 16.269161 11.419992 22.61582 3.000033\n#> Dia_12  9 17.584889 12.635634 24.20307 3.149600\n#> Dia_1  10 19.491955 13.410767 25.28994 3.732346\n#> Dia_13 11 21.058884 13.923335 25.72368 3.607014\n#> Dia_9  12 22.452802 15.911357 25.72368 3.249493\n#> Dia_4  13 23.796512 17.676471 25.72368 2.243847\n#> Dia_10 14 24.703704 24.703704 24.70370 0.000000\n#> \n#> attr(,\"class\")\n#> [1] \"summary.poolaccum\"\n## Preparando os dados para fazer o gráfico\nresultados_ace <- summary(est_ace, display = c(\"S\", \"ace\"))\nres_ace <- cbind(resultados_ace$ace[, 1:4], resultados_ace$S[, 2:4])\nres_ace <- as.data.frame(res_ace)\ncolnames(res_ace) <- c(\"Amostras\", \"ACE\", \"ACE_inferior\", \"ACE_superior\", \n                       \"Riqueza\", \"R_inferior\", \"R_superior\")\n\n## Gráfico\nggplot(res_ace, aes(y = Riqueza, x = Amostras)) +\n    geom_point(aes(y = ACE, x = Amostras + 0.1), size = 4, \n               color = \"darkorange\", alpha = 0.7) +\n    geom_point(aes(y = Riqueza, x = Amostras), size = 4, \n               color = \"cyan4\", alpha = 0.7) +\n    geom_point(y = 7.5, x = 9, size = 4, color = \"darkorange\", alpha = 0.7) + \n    geom_point(y = 5.9, x = 9, size = 4, color = \"cyan4\", alpha = 0.7) + \n    geom_label(y = 7.5, x = 11.7, label = \"Riqueza estimada - ACE\") +\n    geom_label(y = 5.9, x = 11.3, label = \"Riqueza observada\") +\n    geom_line(aes(y = ACE, x = Amostras), color = \"darkorange\") +\n    geom_line(aes(y = Riqueza, x = Amostras), color = \"cyan4\") +\n    geom_linerange(aes(ymin = ACE_inferior, ymax = ACE_superior, \n                       x = Amostras + 0.1), color = \"darkorange\") +\n    geom_linerange(aes(ymin = R_inferior, ymax = R_superior,\n                       x = Amostras), color = \"cyan4\") +\n    scale_x_continuous(limits = c(1, 15), breaks = seq(1, 15, 1)) +\n    labs(x = \"Número de amostras\", y = \"Riqueza estimada - ACE\") +\n    tema_livro()"},{"path":"cap11.html","id":"estimadores-baseados-na-incidência-das-espécies","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.3 Estimadores baseados na incidência das espécies","text":"","code":""},{"path":"cap11.html","id":"chao-2","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.3.1 CHAO 2","text":"De acordo com Anne Chao, o estimador Chao 1 pode ser modificado para uso com dados de presença/ausência levando em conta distribuição das espécies entre amostras (Chao 1987). Neste caso é necessário somente conhecer o número de espécies encontradas em somente uma amostra e o número de espécies encontradas exatamente em duas amostras. Essa variação ficou denominada como Chao 2.\\[Chao_{2} = S_{obs} + \\left(\\frac{m-1}{m}\\right)\\left(\\frac{Q_1(Q_1-1)}{2(Q_2 + 1}\\right)\\]onde:Sobs = o número de espécies observada na comunidadem = número de amostrasQ1 = número de espécies observadas em uma amostra (espécies uniques)Q2 = número de espécies observadas em duas amostras (espécies duplicates)O valor de Chao2 é máximo quando espécies menos uma são únicas (uniques). Neste caso, riqueza estimada é aproximadamente o dobro da riqueza observada.Colwell & Coddington (1994) encontraram que o valor de Chao 2 mostrou ser o estimador menos enviesado para amostras com tamanho pequeno. 📝 Importante \nVocê perceberá que ao longo capítulo recomendações sobre qual é o melhor índice varia entre estudos (e.g. Palmer 1990; Walther Moore 2005).Exemplo práticoExplicação dos dadosUsaremos novamente os mesmos dados hipotéticos de 17 espécies de anuros amostradas em 14 dias de coletas de campo em um habitat reprodutivo localizado na região Noroeste Estado de São Paulo, Brasil.AnáliseCálculo estimador de riqueza - Chao 2.Visualizar os resultados com intervalo de confiança de 95% (Figura 11.3).\nFigura 11.3: Resultados com intervalo de confiança de 95% para o estimador Chao 2.\nInterpretação dos resultadosCom base número de espécies raras (uniques e duplicates), o estimador Chao 2 encontrou possibilidade de acharmos mais dezesseis espécies, caso o esforço amostral fosse maior e não estimou tendência de estabilização da curva em uma assíntota.","code":"\n## Análise\nest_chao2 <- poolaccum(dados_coleta, permutations = 100)\nsummary(est_chao2, display = \"chao\")\n#> $chao\n#>        N     Chao      2.5%    97.5%  Std.Dev\n#>  [1,]  3 14.31571  9.211111 24.35000 3.909186\n#>  [2,]  4 15.12125  8.796875 26.50000 4.676977\n#>  [3,]  5 17.18113 10.485000 34.12500 5.116710\n#>  [4,]  6 18.40042 11.336806 34.50625 5.802968\n#>  [5,]  7 20.06214 12.142857 34.00000 6.145670\n#>  [6,]  8 21.28187 12.391927 38.11562 6.923969\n#>  [7,]  9 22.60556 12.538889 36.82500 6.845149\n#>  [8,] 10 25.94025 14.327500 42.20000 7.446691\n#>  [9,] 11 27.58773 15.401136 39.27273 6.931535\n#> [10,] 12 29.55229 19.933333 39.45833 6.111500\n#> [11,] 13 31.51769 22.384615 39.61538 4.586452\n#> [12,] 14 33.71429 33.714286 33.71429 0.000000\n#> \n#> attr(,\"class\")\n#> [1] \"summary.poolaccum\"\n## Preparando os dados para fazer o gráfico\nresultados_chao2 <- summary(est_chao2, display = c(\"S\", \"chao\"))\nres_chao2 <- cbind(resultados_chao2$chao[, 1:4], resultados_chao2$S[, 2:4])\nres_chao2 <- as.data.frame(res_chao2)\ncolnames(res_chao2) <- c(\"Amostras\", \"Chao2\", \"C_inferior\", \"C_superior\",\n                         \"Riqueza\", \"R_inferior\", \"R_superior\")\n\n## Gráfico\nggplot(res_chao2, aes(y = Riqueza, x = Amostras)) +\n    geom_point(aes(y = Chao2, x = Amostras + 0.1), size = 4, \n               color = \"darkorange\", alpha = 0.7) +\n    geom_point(aes(y = Riqueza, x = Amostras), size = 4, \n               color = \"cyan4\", alpha = 0.7) +\n    geom_point(y = 9.8, x = 10, size = 4, color = \"darkorange\", alpha = 0.7) + \n    geom_point(y = 7.7, x = 10, size = 4, color = \"cyan4\", alpha = 0.7) + \n    geom_label(y = 9.8, x = 12.95, label = \"Riqueza estimada - Chao 2\") +\n    geom_label(y = 7.7, x = 12.3, label = \"Riqueza observada\") +\n    geom_line(aes(y = Chao2, x = Amostras), color = \"darkorange\") +\n    geom_line(aes(y = Riqueza, x = Amostras), color = \"cyan4\") +\n    geom_linerange(aes(ymin = C_inferior, ymax = C_superior, \n                       x = Amostras + 0.1), color = \"darkorange\") +\n    geom_linerange(aes(ymin = R_inferior, ymax = R_superior,\n                       x = Amostras), color = \"cyan4\") +\n    scale_x_continuous(limits = c(1, 15), breaks = seq(1, 15, 1)) +\n    labs(x = \"Número de amostras\", y = \"Riqueza estimada - Chao 2\") +\n    tema_livro()"},{"path":"cap11.html","id":"jackknife-1","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.3.2 JACKKNIFE 1","text":"Este estimador baseia-se número de espécies que ocorrem em somente uma amostra (Q1) (K. P. Burnham Overton 1978, 1979).\\[S_{jack1} = S_{obs} + Q_1\\left(\\frac{m - 1}{m}\\right)\\]onde:Sobs = o número de espécies observadas na comunidadeQ1 = número de espécies observadas em uma amostra (espécies uniques)m = número de amostrasJackknife é um método de reamostragem (sem repetição) não paramétrico usado para estimar riqueza de espécies e variância associada com estimativa. Para isso, o método: ) exclui uma amostra e contabiliza o valor da riqueza estimada usando fórmula acima, ii) repete este processo n vezes até que todas amostras tenham sido excluídas, e iii) estima média e variância da riqueza de espécie (Smith van Belle 1984).Palmer (1990) verificou que Jackknife 1 foi o estimador mais preciso e menos enviesado comparado outros métodos de extrapolação.Exemplo práticoExplicação dos dadosUsaremos os mesmos dados hipotéticos de 17 espécies de anuros amostradas em 14 dias de coletas de campo em um habitat reprodutivo localizado na região Noroeste Estado de São Paulo, Brasil.AnáliseCálculo estimador de riqueza - Jackknife 1.Visualizar os resultados com 95% intervalo de confiança (Figura 11.4).\nFigura 11.4: Resultados com intervalo de confiança de 95% para o estimador Jackknife 1.\nInterpretação dos resultadosCom base número de espécies raras, o estimador Jackknife 1 estimou possibilidade de encontrarmos mais seis espécies, caso o esforço amostral fosse maior e não estimou tendência de estabilização da curva em uma assíntota.","code":"\n## Análise\nest_jack1 <- poolaccum(dados_coleta, permutations = 100)\nsummary(est_jack1, display = \"jack1\")\n#> $jack1\n#>        N Jackknife 1      2.5%    97.5%  Std.Dev\n#>  [1,]  3    13.64667  8.333333 19.17500 2.798035\n#>  [2,]  4    14.75000  9.750000 20.01250 2.788188\n#>  [3,]  5    15.32800  9.800000 20.42000 2.780091\n#>  [4,]  6    16.01500 11.229167 21.96250 2.802425\n#>  [5,]  7    17.07714 12.782143 22.93214 2.575868\n#>  [6,]  8    18.11625 14.165625 23.12500 2.536306\n#>  [7,]  9    18.77778 14.719444 23.22222 2.609373\n#>  [8,] 10    19.56100 14.800000 23.77250 2.586909\n#>  [9,] 11    20.42455 16.727273 23.36364 2.190412\n#> [10,] 12    21.16000 17.185417 23.41667 1.863867\n#> [11,] 13    21.92846 18.692308 23.46154 1.335047\n#> [12,] 14    22.57143 22.571429 22.57143 0.000000\n#> \n#> attr(,\"class\")\n#> [1] \"summary.poolaccum\"\n## Preparando os dados para fazer o gráfico\nresultados_jack1 <- summary(est_jack1, display = c(\"S\", \"jack1\"))\nres_jack1 <- cbind(resultados_jack1$jack1[, 1:4], resultados_jack1$S[, 2:4])\nres_jack1 <- as.data.frame(res_jack1)\ncolnames(res_jack1) <- c(\"Amostras\", \"JACK1\", \"JACK1_inferior\", \"JACK1_superior\", \n                         \"Riqueza\", \"R_inferior\", \"R_superior\")\n\n## Gráfico\nggplot(res_jack1, aes(y = Riqueza, x = Amostras)) +\n    geom_point(aes(y = JACK1, x = Amostras + 0.1), size = 4, \n               color = \"darkorange\", alpha = 0.7) +\n    geom_point(aes(y = Riqueza, x = Amostras), size = 4, \n               color = \"cyan4\", alpha = 0.7) +\n    geom_point(y = 9.9, x = 9, size = 4, color = \"darkorange\", alpha = 0.7) + \n    geom_point(y = 8.6, x = 9, size = 4, color = \"cyan4\", alpha = 0.7) + \n    geom_label(y = 9.9, x = 12.5, label = \"Riqueza estimada - Jackknife 1\") +\n    geom_label(y = 8.6, x = 11.5, label = \"Riqueza observada\") +\n    geom_line(aes(y = JACK1, x = Amostras), color = \"darkorange\") +\n    geom_line(aes(y = Riqueza, x = Amostras), color = \"cyan4\") +\n    geom_linerange(aes(ymin = JACK1_inferior, ymax = JACK1_superior, \n                       x = Amostras + 0.1), color = \"darkorange\") +\n    geom_linerange(aes(ymin = R_inferior, ymax = R_superior, \n                       x = Amostras), color = \"cyan4\") +\n    scale_x_continuous(limits = c(1, 15), breaks = seq(1, 15, 1)) +\n    labs(x = \"Número de amostras\", y = \"Riqueza estimada - Jackknife 1\") +\n    tema_livro()"},{"path":"cap11.html","id":"jackknife-2","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.3.3 JACKKNIFE 2","text":"Este método baseia-se número de espécies que ocorrem em apenas uma amostra e número de espécies que ocorrem em exatamente duas amostras (K. P. Burnham Overton 1978, 1979; Palmer 1991).\\[S_{jack2} = S_{obs} + \\left[\\frac{Q_1(2m - 3)}{m}-\\frac{Q_2(m - 2)^2}{m(m-1)}\\right]\\]onde:Sobs = o número de espécies observadas na comunidadem = número de amostrasQ1 = número de espécies observadas em uma amostra (espécies uniques)Q2 = número de espécies observadas em duas amostras (espécies duplicates)Exemplo práticoExplicação dos dadosUsaremos os mesmos dados hipotéticos de 17 espécies de anuros amostradas em 14 dias de coletas de campo em um habitat reprodutivo localizado na região Noroeste Estado de São Paulo, Brasil.AnáliseCálculo estimador de riqueza - Jackknife 2.Visualizar os resultados com intervalo de confiança de 95% (Figura 11.5).\nFigura 11.5: Resultados com intervalo de confiança de 95% para o estimador Jackknife 2.\nInterpretação dos resultadosCom base número de espécies raras, o estimador Jackknife 2 estimou possibilidade de encontrarmos mais dez espécies, caso o esforço amostral fosse maior e não estimou tendência estabilização da curva em uma assíntota.","code":"\n## Análise\nest_jack2 <- poolaccum(dados_coleta, permutations = 100)\nsummary(est_jack2, display = \"jack2\")\n#> $jack2\n#>        N Jackknife 2      2.5%    97.5%  Std.Dev\n#>  [1,]  3    14.27667  7.904167 21.50000 3.750886\n#>  [2,]  4    15.73500  8.566667 23.77292 4.127584\n#>  [3,]  5    16.36650  9.150000 24.68625 4.262113\n#>  [4,]  6    18.16900 10.631667 27.46667 4.274110\n#>  [5,]  7    19.57786 12.339881 27.21250 3.875295\n#>  [6,]  8    20.78679 14.133929 28.58527 3.778223\n#>  [7,]  9    21.82028 13.972222 27.98611 3.960469\n#>  [8,] 10    22.84444 14.977778 28.18889 3.770196\n#>  [9,] 11    24.04227 17.445455 28.35455 3.123930\n#> [10,] 12    24.99455 20.242424 28.49242 2.618660\n#> [11,] 13    26.09481 21.301282 28.60897 1.953804\n#> [12,] 14    26.92308 26.923077 26.92308 0.000000\n#> \n#> attr(,\"class\")\n#> [1] \"summary.poolaccum\"\n## Preparando os dados para fazer o gráfico\nresultados_jack2 <- summary(est_jack2, display = c(\"S\", \"jack2\"))\nres_jack2 <- cbind(resultados_jack2$jack2[, 1:4], resultados_jack2$S[, 2:4])\nres_jack2 <- as.data.frame(res_jack2)\ncolnames(res_jack2) <- c(\"Amostras\", \"JACK2\", \"JACK2_inferior\", \"JACK2_superior\", \n                         \"Riqueza\", \"R_inferior\", \"R_superior\")\n\n## Gráfico\nggplot(res_jack2, aes(y = Riqueza, x = Amostras)) +\n    geom_point(aes(y = JACK2, x = Amostras + 0.1), size = 4, \n               color = \"darkorange\", alpha = 0.7) +\n    geom_point(aes(y = Riqueza, x = Amostras), size = 4, \n               color = \"cyan4\", alpha = 0.7) +\n    geom_point(y = 9.9, x = 9, size = 4, color = \"darkorange\", alpha = 0.7) + \n    geom_point(y = 8.2, x = 9, size = 4, color = \"cyan4\", alpha = 0.7) + \n    geom_label(y = 9.9, x = 12.5, label = \"Riqueza estimada - Jackknife 2\") +\n    geom_label(y = 8.2, x = 11.5, label = \"Riqueza observada\") +\n    geom_line(aes(y = JACK2, x = Amostras), color = \"darkorange\") +\n    geom_line(aes(y = Riqueza, x = Amostras), color = \"cyan4\") +\n    geom_linerange(aes(ymin = JACK2_inferior, ymax = JACK2_superior, \n                       x = Amostras + 0.1), color = \"darkorange\") +\n    geom_linerange(aes(ymin = R_inferior, ymax = R_superior, \n                       x = Amostras), color = \"cyan4\") +\n    scale_x_continuous(limits = c(1, 15), breaks = seq(1, 15, 1)) +\n    labs(x = \"Número de amostras\", y = \"Riqueza estimada - Jackknife 2\") +\n    tema_livro()"},{"path":"cap11.html","id":"bootstrap","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.3.4 BOOTSTRAP","text":"Este método difere dos demais por utilizar dados de todas espécies coletadas para estimar riqueza total, não se restringindo às espécies raras (Smith van Belle 1984). Ele requer somente dados de incidência. estimativa pelo bootstrap é calculada somando-se riqueza observada à soma inverso da proporção de amostras em que cada espécie ocorre.\\[S_{boot} = S_{obs} + \\sum_{k=1}^{S_{obs}}(1-P_k)^m\\]onde:Sobs = o número de espécies observadas na comunidadem = número de amostragensPk = proporção número de amostras em que cada espécie foi registradaBootstrap é um método não paramétrico usado para estimar os parâmetros de uma população por reamostragem. premissa é que reamostragens podem ser entendidas como pseudo-populações, com características similares da população original. Para isso, o método: ) seleciona ao acaso um conjunto de amostras (nosso exemplo 14 amostras) com reposição, ii) repete este processo n vezes, e iii) estima média e variância da riqueza de espécie (Smith van Belle 1984).Exemplo práticoExplicação dos dadosUsaremos os mesmos dados hipotéticos de 17 espécies de anuros amostradas em 14 dias de coletas de campo em um habitat reprodutivo localizado na região Noroeste Estado de São Paulo, Brasil.AnáliseCálculo estimador de riqueza - Bootstrap.Visualizar os resultados com intervalo de confiança de 95% (Figura 11.6).\nFigura 11.6: Resultados com intervalo de confiança de 95% para o estimador Bootstrap.\nInterpretação dos resultadosCom base na frequência de ocorrência das espécies, o estimador bootstrap estimou possibilidade de encontrarmos mais duas espécies, caso o esforço amostral fosse maior e não estimou tendência de estabilização da curva em uma assíntota.","code":"\n## Análise\nest_boot <- poolaccum(dados_coleta, permutations = 100)\nsummary(est_boot, display = \"boot\")\n#> $boot\n#>        N Bootstrap      2.5%    97.5%  Std.Dev\n#>  [1,]  3  11.87407  8.425000 15.51852 2.011130\n#>  [2,]  4  13.14934  9.771387 16.77627 2.001598\n#>  [3,]  5  13.83898 10.822584 17.43899 1.964793\n#>  [4,]  6  14.66163 10.705144 18.37757 1.977243\n#>  [5,]  7  15.47596 11.981076 18.68644 1.879223\n#>  [6,]  8  16.18653 12.599602 19.68927 1.830489\n#>  [7,]  9  16.61378 13.042764 19.61772 1.779771\n#>  [8,] 10  17.19346 13.077245 19.70960 1.788670\n#>  [9,] 11  17.82230 14.315556 19.81162 1.558281\n#> [10,] 12  18.35152 15.374464 19.58721 1.270158\n#> [11,] 13  18.75381 16.570376 19.59107 1.027014\n#> [12,] 14  19.27832 19.278321 19.27832 0.000000\n#> \n#> attr(,\"class\")\n#> [1] \"summary.poolaccum\"\n## Preparando os dados para fazer o gráfico\nresultados_boot <- summary(est_boot, display = c(\"S\", \"boot\"))\nres_boot <- cbind(resultados_boot$boot[,1:4], resultados_boot$S[,2:4])\nres_boot <- as.data.frame(res_boot)\ncolnames(res_boot) <- c(\"Amostras\", \"BOOT\", \"BOOT_inferior\", \"BOOT_superior\", \n                        \"Riqueza\", \"R_inferior\", \"R_superior\")\n\n## Gráfico\nggplot(res_boot, aes(y = Riqueza, x = Amostras)) +\n    geom_point(aes(y = BOOT, x = Amostras + 0.1), size = 4, \n               color = \"darkorange\", alpha = 0.7) +\n    geom_point(aes(y = Riqueza, x = Amostras), size = 4, \n               color = \"cyan4\", alpha = 0.7) +\n    geom_point(y = 10.4, x = 9, size = 4, color = \"darkorange\", alpha = 0.7) + \n    geom_point(y = 9.3, x = 9, size = 4, color = \"cyan4\", alpha = 0.7) + \n    geom_label(y = 10.4, x = 12.3, label = \"Riqueza estimada - Bootstrap\") +\n    geom_label(y = 9.3, x = 11.5, label = \"Riqueza observada\") +\n    geom_line(aes(y = BOOT, x = Amostras), color = \"darkorange\") +\n    geom_line(aes(y = Riqueza, x = Amostras), color = \"cyan4\") +\n    geom_linerange(aes(ymin = BOOT_inferior, ymax = BOOT_superior, \n                       x = Amostras + 0.1), color = \"darkorange\") +\n    geom_linerange(aes(ymin = R_inferior, ymax = R_superior, \n                       x = Amostras), color = \"cyan4\") +\n    scale_x_continuous(limits = c(1, 15), breaks = seq(1, 15, 1)) +\n    labs(x = \"Número de amostras\", y = \"Riqueza estimada - Bootstrap\") +\n    tema_livro()"},{"path":"cap11.html","id":"interpolação-e-extrapolação-baseadas-em-rarefação-usando-amostragens-de-incidência-ou-abundância","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.4 Interpolação e extrapolação baseadas em rarefação usando amostragens de incidência ou abundância","text":"Este método utiliza teoria de amostragem (e.g. modelos multinomial, Poisson e Bernoulli) para conectar rarefação (interpolação) e predição (extrapolação) com base tamanho da amostra (Chao Jost 2012; Colwell et al. 2012). Ele utiliza uma abordagem com bootstrap para calcular o intervalo de confiança de 95%. 📝 Importante \nextrapolação torna-se altamente incerta quando estendida para o dobro ou mais tamanho da amostragem.Exemplo prático 1Explicação dos dadosUsaremos os mesmos dados hipotéticos de 17 espécies de anuros amostradas em 14 dias de coletas de campo em um habitat reprodutivo localizado na região Noroeste Estado de São Paulo, Brasil.AnáliseCálculo da extrapolação da riqueza com base número de indivíduos (Figura 11.7).\nFigura 11.7: Resultados com intervalo de confiança de 95% para extrapolação.\nInterpretação dos resultadosVeja que o ponto final da linha contínua representa 17 espécies de anuros (eixo Y) observadas entre os 304 indivíduos (eixo X). extrapolação máxima (600 indivíduos nosso exemplo) estima um aumento de até oito espécies (intervalo de confiança) caso amostrássemos mais 296 indivíduos.Cálculo da extrapolação da riqueza com base número de amostras (Figura 11.8).\nFigura 11.8: Resultados com intervalo de confiança de 95% para extrapolação da riqueza.\nInterpretação dos resultadosVeja que o ponto final da linha contínua representa 17 espécies de anuros (eixo Y) observadas nos 14 dias de coleta (eixo X - amostras). extrapolação máxima (28 dias de coleta nosso exemplo) estima um aumento de até 12 espécies (intervalo de confiança) caso amostrássemos mais 14 dias.Exemplo prático 2Explicação dos dadosNeste exemplo, iremos refazer o exemplo Capítulo 7 que usa Generalized Least Squares (GLS) para testar relação da riqueza de ácaros com quantidade de água substrato. Contudo, ao invés de considerar riqueza de espécies de ácaros observada como variável resposta, iremos utilizar riqueza extrapolada. Os dados que usaremos estão disponíveis pacote vegan e representam composição de espécies de ácaros amostradas em 70 comunidades/amostras.PerguntaA riqueza extrapolada de espécies de ácaros é maior em comunidades localizadas em áreas com substratos secos?PrediçõesO número de espécies extrapolada será maior em substratos secos uma vez que limitações fisiológicas impostas pela umidade limitam ocorrência de várias espécies de ácarosVariáveismatriz ou data frame com abundâncias das espécies de ácaros (variável resposta) registradas em 70 comunidades/amostras (variável preditora)ChecklistVerificar se sua matriz ou data frame estão com espécies nas linhas e comunidades nas colunasAnáliseVamos iniciar calculando riqueza extrapolada com base na comunidade com maior abundância.Lembrando, estamos interessados valor rarefeito considerando abundância total de 781 indivíduos. Esta informação está armazenada na linha 40 e na coluna 4 dos data frames salvos objeto resultados_extrapolacao$iNextEst. Assim, para obtermos o valor rarefeito de interesse, vamos criar um loop para facilitar extração da riqueza rarefeita para 70 comunidades.Agora, seguindo os passos descritos Capítulo 7, vamos identificar o melhor modelo que representa estrutura espacial dos dados extrapolados.Vamos usar seleção de modelos por AIC para selecionar o modelo mais “provável” explicando distribuição da riqueza extrapolada das espécies de ácaros (Figura 11.9).\nFigura 11.9: Visualização dos resíduos modelo selecionado.\nDe forma geral, distribuição dos resíduos está adequada com apenas dois pontos fugindo da nuvem. Contudo, eles podem influenciar os resultados (Figura 11.10).\nFigura 11.10: Gráfico modelo GLS selecionado.\nInterpretação dos resultadosA riqueza extrapolada das espécies de ácaros foi maior em comunidades localizadas em áreas com substratos secos que em áreas com substratos úmidos. Contudo, apesar modelo apresentar uma relação significativa entre variáveis, concentração de água explica apenas 5,9% da variação da riqueza extrapolada das espécies de ácaros. O padrão observado, valor de P < 0.05 e o baixo valor de R2, provavelmente está relacionado com duas comunidades com altos valores de riqueza extrapolada (e.g. outilers). Refaça análises sem os dois pontos e vejam o padrão dos novos resultados.","code":"\n## Preparando os dados para análises considerando a abundância\ndados_inext_abu <- colSums(dados_coleta) \nresultados_abundancia <- iNEXT(dados_inext_abu, q = 0, datatype = \"abundance\", \n                               endpoint = 600)\n\n## Gráfico\nggiNEXT(resultados_abundancia, type = 1) + \n    scale_linetype_discrete(labels = c(\"Interpolado\", \"Extrapolado\")) +\n    scale_colour_manual(values = \"darkorange\") +\n    scale_fill_manual(values = \"darkorange\") +\n    labs(x = \"Número de indivíduos\", y = \" Riqueza de espécies\") +\n    tema_livro()\n## Preparando os dados para análises considerando a incidência\n# Precisa transpor o data frame.\ndados_inext <- as.incfreq(t(dados_coleta)) \nresultados_incidencia <- iNEXT(dados_inext, q = 0, datatype = \"incidence_freq\", \n                               endpoint = 28)\n\n## Gráfico\nggiNEXT(resultados_incidencia, type = 1) +\n    scale_linetype_discrete(labels = c(\"Interpolado\", \"Extrapolado\")) +\n    scale_colour_manual(values = \"darkorange\") +\n    scale_fill_manual(values = \"darkorange\") +\n    labs(x = \"Número de amostras\", y = \" Riqueza de espécies\") +\n    tema_livro()\n## Transposição\n# Os dados estão com as comunidades nas colunas e as espécies nas linhas. \n# Para as análises teremos que transpor a planilha.\ncomposicao_acaros <- as.data.frame(t(mite))\n\n## Dados\n# A comunidade com maior abundância tem 781 indivíduos. \nmax(colSums(composicao_acaros))\n#> [1] 781\n\n# Calcular a riqueza extrapolada de espécies para todas as comunidades \n# considerando a maior abundância. \nresultados_extrapolacao <- iNEXT(composicao_acaros, q = 0, \n                                 datatype = \"abundance\", \n                                 endpoint = 781)\n## Dados\n# Criando um data.frame vazio para salvar os dados\nresultados_comunidades_ext <- data.frame()\n\n# Criando um vetor vazio para salvar os resultados\nriqueza_extrapolada <- c()\n\n## Loop\n# Loop repetindo as análises para as 70 comunidades\n# O objetivo é extrair a riqueza estimada extrapolada para 781 individuos\nfor (i in 1:70){\n    resultados_comunidades_ext <- data.frame(resultados_extrapolacao$iNextEst[i])\n    riqueza_extrapolada[i] <- resultados_comunidades_ext[40, 4] # 8 abundância e 4 riqueza rarefeita\n}\n## Dados\n# Criando data frame com todas as variáveis\ndados_combinado_ext <- data.frame(riqueza_extrapolada, agua, coord)\n\n## Modelo gls sem estrutura espacial\nno_spat_gls <- gls(riqueza_extrapolada ~ agua, \n                   data = dados_combinado_ext, \n                   method = \"REML\")\n\n## Covariância esférica\nespher_model <- gls(riqueza_extrapolada ~ agua, \n                    data = dados_combinado_ext, \n                    corSpher(form = ~lat + long, nugget = TRUE))\n\n## Covariância exponencial\nexpon_model <- gls(riqueza_extrapolada ~ agua, \n                   data = dados_combinado_ext, \n                   corExp(form = ~lat + long, nugget = TRUE))\n\n## Covariância Gaussiana\ngauss_model <- gls(riqueza_extrapolada ~ agua,\n                   data = dados_combinado_ext, \n                   corGaus(form = ~lat + long, nugget = TRUE))\n\n## Covariância razão quadrática\nratio_model <- gls(riqueza_extrapolada ~ agua, \n                   data = dados_combinado_ext, \n                   corRatio(form = ~lat + long, nugget = TRUE))\n## Seleção dos modelos\naic_fit_ext <- AIC(no_spat_gls, espher_model, expon_model, gauss_model, ratio_model)\naic_fit_ext %>% arrange(AIC)\n#>   df      AIC\n#> 1  5 467.9349\n#> 2  3 469.3103\n#> 3  5 473.2815\n#> 4  5 473.3086\n#> 5  5 473.3103\n\n## Visualizando os resíduos do modelo com menor valor de AIC (veja Capítulo 7)\nplot(ratio_model)\n## Visualizando os resultados e calculando pseudo-R-squared.\nsummary(ratio_model)$tTable \n#>                   Value   Std.Error   t-value      p-value\n#> (Intercept) 24.09577588 4.816461582  5.002796 4.227862e-06\n#> agua        -0.01181425 0.006977381 -1.693221 9.499017e-02\nrsquared(ratio_model)\n#>              Response   family     link method  R.squared\n#> 1 riqueza_extrapolada gaussian identity   none 0.05977552\n\n## Gráfico\npredito_ext <- predict(ratio_model) \n\nggplot(data = dados_combinado_ext, aes(x= agua, y= riqueza_extrapolada)) + \n    geom_point(size = 4, shape = 21, fill = \"darkorange\", alpha = 0.7) +\n    geom_line(aes(y = predito_ext), size = 1) +\n    labs(x = \"Concentração de água no substrato\", \n         y = \"Riqueza extrapolada \\ndas espécies de ácaros\") +\n    tema_livro()"},{"path":"cap11.html","id":"para-se-aprofundar-7","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.5 Para se aprofundar","text":"","code":""},{"path":"cap11.html","id":"livros-6","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.5.1 Livros","text":"Recomendamos o livro Biological Diversity Frontiers Measurement Assessment (Magurran McGill 2011)","code":""},{"path":"cap11.html","id":"links-6","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.5.2 Links","text":"Recomendamos também que acessem página EstimateS software e baixem o manual usuário que contém informações detalhadas sobre os índices de rarefação e estimadores de riqueza. Este site foi criado e é mantido pelo Dr. Robert K. Colwell, um dos maiores especialistas mundo em estimativas da biodiversidadeRecomendamos também que acessem página EstimateS software e baixem o manual usuário que contém informações detalhadas sobre os índices de rarefação e estimadores de riqueza. Este site foi criado e é mantido pelo Dr. Robert K. Colwell, um dos maiores especialistas mundo em estimativas da biodiversidadeRecomendamos página pessoal da pesquisadora Anne Chao que é uma das responsáveis pelo desenvolvimento da metodologia e pacote iNEXT. Nesta página, vocês irão encontrar exemplos e explicações detalhadas sobre análisesRecomendamos página pessoal da pesquisadora Anne Chao que é uma das responsáveis pelo desenvolvimento da metodologia e pacote iNEXT. Nesta página, vocês irão encontrar exemplos e explicações detalhadas sobre análises","code":""},{"path":"cap11.html","id":"exercícios-7","chapter":"Capítulo 11 Estimadores de riqueza","heading":"11.6 Exercícios","text":"11.1\nCarregue os dados - Cap11_exercicio1 - que está pacote ecodados. Este conjunto de dados representa abundância de 50 espécies de besouros coletados em 30 amostras. Calcule os estimadores de riqueza - Chao1 e ACE - e faça um gráfico contendo riqueza observada e os dois estimadores de riqueza. Qual sua interpretação sobre o esforço amostral?11.2\nUtilize o mesmo conjunto de dados exercício anterior. Calcule os estimadores de riqueza - Jackknife 1 e bootstrap. Faça um gráfico contendo riqueza observada e os dois estimadores de riqueza. Qual sua interpretação sobre o esforço amostral? Compare com os resultados exercício anterior que utilizam estimadores baseados na abundância das espécies.11.3\nVamos refazer o exercício 10 Capítulo 7 que usa Generalized Least Squares (GLS) para testar relação da riqueza de anuros em 44 localidades na Mata Atlântica com precipitação anual. Contudo, ao invés de considerar riqueza de espécies de anuros observada como variável resposta, iremos utilizar riqueza extrapolada. Utilize os dados anuros_composicao para estimar riqueza extrapolada e o dados anuros_ambientais para acessar os dados de precipitação anual e coordenadas geográficas. Qual sua interpretação dos resultados utilizando riqueza observada e extrapolada?Soluções dos exercícios.","code":""},{"path":"cap12.html","id":"cap12","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"Capítulo 12 Diversidade Taxonômica","text":"","code":""},{"path":"cap12.html","id":"pré-requisitos-do-capítulo-8","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"Pré-requisitos do capítulo","text":"Pacotes e dados que serão utilizados neste capítulo.","code":"\n## Pacotes\nlibrary(devtools)\nlibrary(ecodados)\nlibrary(vegan)\nlibrary(ggplot2)\nlibrary(BiodiversityR)\nlibrary(hillR)\nlibrary(betapart)\n\n## Dados\ncomposicao_especies <- ecodados::composicao_anuros_div_taxonomica\nprecipitacao        <- ecodados::precipitacao_div_taxonomica"},{"path":"cap12.html","id":"aspectos-teóricos-3","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.1 Aspectos teóricos","text":"diversidade biológica é um conceito multifacetado que pode ser definido e analisado de diferentes maneiras (e.g. diversidade genética, taxonômica, funcional, filogenética, ecossistêmica, etc.) (Magurran McGill 2011; Gotelli Chao 2013). Whittaker (1960, 1972) particionou diversidade em três componentes: ) diversidade alfa que é caracterizada pela diversidade dentro habitat ou unidade amostral, ii) diversidade beta que é caracterizada pela variação na diversidade entre habitats ou unidades amostrais, e iii) diversidade gama que é caracterizada pela combinação da diversidade alfa e beta ou definida como diversidade regional englobando todos os habitat ou unidades amostrais. Portanto, não existe um método que quantifique todos os parâmetros associados à diversidade biológica. Consequentemente, escolha da métrica de diversidade dependerá: ) objetivo estudo, e ii) das informações disponíveis para o pesquisador.Neste capítulo, iremos abordar diversidade taxonômica que ignora relação de parentesco entre espécies (e.g. diversidade filogenética - Capítulo 13) e diferentes funções que espécies realizam ecossistema (e.g. diversidade funcional - Capítulo 14). Na diversidade taxonômica, pesquisadores estão interessados na riqueza de espécies (e.g. número de espécies), na distribuição de abundância das espécies (e.g. fato que algumas espécies são comuns e outras raras) e/ou diversidade de espécies (e.g. índices que descrevem relação entre riqueza e distribuição da abundância relativa das espécies) nas localidades.","code":""},{"path":"cap12.html","id":"diversidade-alfa","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.2 Diversidade alfa","text":"","code":""},{"path":"cap12.html","id":"riqueza-de-espécies-ou-número-de-espécies","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.2.1 Riqueza de espécies ou número de espécies","text":"Riqueza de espécies é uma métrica intuitiva e de fácil compreensão, uma vez que se refere ao número de espécies observadas em uma localidade. É importante ter em mente que riqueza de espécies é influenciada pelo esforço amostral e sua estimativa real é um imenso desafio (Magurran McGill 2011). Comparações entre comunidades com diferenças número de amostragens ou abundância das espécies devem ser realizadas por meio de rarefações (veja Capítulo 10), enquanto que o número de espécies não detectadas pode ser estimado pelos estimadores de riqueza (veja Capítulo 11). Embora raramente usados como alternativas à rarefação, existem alguns índices que calculam riqueza de espécies ponderando abundância total (.e. tamanho da amostra) dentro de cada comunidade.Esses índices são:1. Índice de Margalef\\[D_{Mg} = \\frac{S-1}{ln (N)}\\]onde:S = o número de espécies na comunidadeln = logaritmo naturalN = número total de indivíduos na comunidadeDMg não tem um valor máximo e sua interpretação é comparativa, com valores maiores indicando maior riqueza de espécies2. Índice de Menhinick\\[D_{Mn} = \\frac{S}{\\sqrt{N}}\\]onde:S = o número de espécies na comunidadeN = número total de indivíduos na comunidadeDMn não tem um valor máximo e sua interpretação é comparativa, com valores maiores indicando maior riqueza de espéciesExemplo prático 1Explicação dos dadosNeste exemplo, avaliaremos riqueza de espécies de 10 comunidades. Os dados de ocorrência das espécies nas comunidades foram simulados para demonstrar propriedades das métricas de diversidade taxonômicas. Utilizaremos este conjunto de dados para todos os exemplos deste capítulo.PerguntaA variação espacial na riqueza de espécies nas comunidades está associada com variação na precipitação?PrediçõesOs valores de riqueza de espécies serão maiores nas comunidades localizadas em regiões que recebem grande volume de precipitação que em regiões mais secasVariáveisData frame com comunidades (unidade amostral) nas linhas e espécies (variável resposta) nas colunasData frame com comunidades (unidade amostral) nas linhas e precipitação anual (variável preditora) na colunaChecklistVerificar se os data frames de composição de espécies e variáveis ambientais estão com unidades amostrais nas linhas e variáveis preditores nas colunasVerificar se os data frames de composição de espécies e variáveis ambientais estão com unidades amostrais nas linhas e variáveis preditores nas colunasVerificar se comunidades nos data frames de composição de espécies e variáveis ambientais estão distribuídos na mesma sequência/ordem nos dois arquivosVerificar se comunidades nos data frames de composição de espécies e variáveis ambientais estão distribuídos na mesma sequência/ordem nos dois arquivosAnáliseAbaixo, demonstramos os códigos R para determinar riqueza de espécies para cada comunidade partir dos dados de composição de espécies. Os dados estão disponíveis pacote ecodados.Vamos ver riqueza de espécies para cada comunidade.Vamos ver abundância total de cada comunidade.Calculando o Índice de Margalef.Calculando o Índice de Menhinick.Agora vamos analisar relação entre riqueza de espécies e precipitação anual.Há uma relação positiva entre riqueza de espécies e precipitação anual (F1,8 = 8,91, P = 0,01).Analisar relação entre o Índice de Margalef e precipitação anual.Não há uma relação positiva entre o índice de Margalef e precipitação anual (F1,8 = 2,12, P = 0,18).Agora vamos analisar relação entre o índice de Menhinick e precipitação anual.Não há uma relação positiva entre o índice de Menhinick e precipitação anual (F1,8 = 1,09, P = 0,32).Com base nos resultados, vamos plotar apenas o gráfico com os resultados da riqueza de espécies ao longo gradiente de precipitação anual (Figura 12.1).\nFigura 12.1: Gráfico modelo linear da relação entre riqueza e precipitação.\nInterpretação dos resultadosO número de espécies é maior em comunidades com maior precipitação. Contudo, quando ponderamos pela abundância (índices de Margalef ou Menhinick), relação com precipitação não é significativa. 📝 Importante \nPercebam que ponderar riqueza de espécies pela abundância altera interpretação dos resultados.","code":"\n## Ver os dados das comunidades\nhead(composicao_especies)\n#>       sp1 sp2 sp3 sp4 sp5 sp6 sp7 sp8 sp9 sp10\n#> Com_1  10  10  10  10  10  10  10  10  10   10\n#> Com_2  91   1   1   1   1   1   1   1   1    1\n#> Com_3   1   3   6  25   1   0   0   0   0    0\n#> Com_4   0   0   0   0   0  15  15  18  17   16\n#> Com_5   0   9   0   6   0  11   0   2  12    0\n#> Com_6   3   0   5   0  12   1   0  13  12    0\n## Calculando a riqueza observada de espécies para cada comunidade\nriqueza_sp <- specnumber(composicao_especies)\nriqueza_sp\n#>  Com_1  Com_2  Com_3  Com_4  Com_5  Com_6  Com_7  Com_8  Com_9 Com_10 \n#>     10     10      5      5      5      6      2      4      6      4\n## Calculamos a abundância total para cada comunidade\nabundancia <- apply(composicao_especies, 1, sum)\nabundancia\n#>  Com_1  Com_2  Com_3  Com_4  Com_5  Com_6  Com_7  Com_8  Com_9 Com_10 \n#>    100    100     36     81     40     46      4     20     15     11\n## Índice de Margalef\n# A função round é para limitar o resultado para duas casas decimais.\nMargalef <- round((riqueza_sp - 1)/log(abundancia), 2)\nMargalef\n#>  Com_1  Com_2  Com_3  Com_4  Com_5  Com_6  Com_7  Com_8  Com_9 Com_10 \n#>   1.95   1.95   1.12   0.91   1.08   1.31   0.72   1.00   1.85   1.25\n## Índice de Menhinick\nMenhinick <- round(riqueza_sp/sqrt(abundancia), 2)\nMenhinick\n#>  Com_1  Com_2  Com_3  Com_4  Com_5  Com_6  Com_7  Com_8  Com_9 Com_10 \n#>   1.00   1.00   0.83   0.56   0.79   0.88   1.00   0.89   1.55   1.21\n## Juntando todos os dados em um único data frame\ndados <- data.frame(precipitacao$prec, riqueza_sp, Margalef, Menhinick)\n\n## Renomenado as colunas\ncolnames(dados) <- c(\"Precipitacao\", \"Riqueza\", \"Margalef\", \"Menhinick\")\n\n## ANOVA\nanova_riq <- lm(Riqueza ~ Precipitacao, data = dados)\nanova(anova_riq)\n#> Analysis of Variance Table\n#> \n#> Response: Riqueza\n#>              Df Sum Sq Mean Sq F value  Pr(>F)  \n#> Precipitacao  1 30.622 30.6224  8.9156 0.01744 *\n#> Residuals     8 27.478  3.4347                  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## ANOVA\nanova_marg <- lm(Margalef ~ Precipitacao, data = dados)\nanova(anova_marg)\n#> Analysis of Variance Table\n#> \n#> Response: Margalef\n#>              Df  Sum Sq Mean Sq F value Pr(>F)\n#> Precipitacao  1 0.37865 0.37865  2.1201 0.1835\n#> Residuals     8 1.42879 0.17860\n## ANOVA\nanova_menh <- lm(Menhinick ~ Precipitacao, data = dados)\nanova(anova_menh)\n#> Analysis of Variance Table\n#> \n#> Response: Menhinick\n#>              Df  Sum Sq  Mean Sq F value Pr(>F)\n#> Precipitacao  1 0.07626 0.076262  1.0992 0.3251\n#> Residuals     8 0.55503 0.069378\n## Gráfico\nggplot(data = dados, aes(x= Precipitacao, y= Riqueza)) + \n    labs(x = \"Precipitação anual (mm)\", y = \"Riqueza de espécies\") +\n    geom_point(size = 4, shape = 21, fill = \"darkorange\", alpha = 0.7) +\n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    tema_livro()"},{"path":"cap12.html","id":"diversidade-de-espécies","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.2.2 Diversidade de espécies","text":"Diferente dos índices de riqueza de espécies que não levam em consideração abundância relativa das espécies (.e. todas espécies tem o mesmo peso), os índices de diversidade avaliam além da riqueza, dominância ou raridade das espécies nas comunidades. Assim, quando comparamos duas comunidades com mesma riqueza de espécies, e uma das comunidades é dominada por uma única espécie e outra comunidade apresenta espécies com abundâncias parecidas, consideramos segunda comunidade mais diversa. Os índices de diversidade variam porque eles dão pesos diferentes para riqueza e equitabilidade das espécies. Assim, um determinado índice de diversidade pode indicar que uma comunidade X é mais diversa que uma comunidade Y, enquanto outro índice indica o oposto (Melo 2008). Portanto, uma maneira de determinar qual índice de diversidade usar é saber se você quer dar maior peso para riqueza ou equitabilidade das espécies nas comunidades. 📝 Importante \nRessaltamos que há várias críticas em relação ao uso dos índices de diversidade que são abstratos e difíceis de se interpretar (Hurlbert 1971). Por exemplo, dizer que o valor X estimado por índices de diversidade é alto ou baixo é irrelevante se não tivermos uma base comparativa (para mais detalhes veja Melo (2008)).Os dois índices de diversidade mais usados em Ecologia são:1. Índice de Shannon-WienerQuantifica incerteza associada em predizer identidade de uma espécie dado o número de espécies e distribuição de abundância para cada espécie. Este índice é mais sensível às mudanças nas espécies raras da comunidade.\\[H' = -\\sum_{=1}^{S}p_i * ln p_i\\]onde:pi = abundância relativa de cada espécie, calculada pela proporção dos indivíduos de uma espécie pelo número total dos indivíduos na comunidadeln = logaritmo natural, mas outras bases logarítmicas podem ser utilizadasH’ = não tem um valor máximo e sua interpretação é comparativa, com valores maiores indicando maior diversidade2. Índice de SimpsonQuantifica probabilidade de dois indivíduos retirados ao acaso da comunidade pertencerem à mesma espécie. Este índice é na verdade uma medida de dominância. Assim como probabilidade dos indivíduos serem da mesma espécie diminui com o aumento da riqueza de espécies, o índice de Simpson também diminui com riqueza.\\[D = \\sum_{=1}^{S}p_i^2\\]onde:Pi = abundância relativa de cada espécie, calculada pela proporção dos indivíduos de uma espécie pelo número total dos indivíduos na comunidadePi = abundância relativa de cada espécie, calculada pela proporção dos indivíduos de uma espécie pelo número total dos indivíduos na comunidadeD = varia de 0 1, com valores próximos de 1 indicando menor diversidade enquanto valores próximos de 0 indicam maior diversidade. Para evitar confusão nas interpretações, normalmente o índice de Simpson é expressado como o valor inverso (1 - D) para que os maiores valores representem maior diversidade. Neste caso, o valor inverso é conhecido na literatura como índice Gini-Simpson. Para o índice Gini-Simpson estamos avaliando probabilidade de dois indivíduos retirados ao acaso da comunidade sejam de espécies diferentes.D = varia de 0 1, com valores próximos de 1 indicando menor diversidade enquanto valores próximos de 0 indicam maior diversidade. Para evitar confusão nas interpretações, normalmente o índice de Simpson é expressado como o valor inverso (1 - D) para que os maiores valores representem maior diversidade. Neste caso, o valor inverso é conhecido na literatura como índice Gini-Simpson. Para o índice Gini-Simpson estamos avaliando probabilidade de dois indivíduos retirados ao acaso da comunidade sejam de espécies diferentes.Exemplo prático 2Explicação dos dadosUsaremos os mesmos dados simulados exemplo prático 1.PerguntaA variação espacial na diversidade de espécies das comunidades está associada com o gradiente de precipitação?PrediçõesOs valores de diversidade de espécies serão maiores nas comunidades localizadas em regiões maior volume de precipitação que em regiões mais secasAnáliseAbaixo demonstramos os códigos R para determinar diversidade de espécies para cada comunidade partir da planilha de composição de espécies.O argumento index = \"simpson\", calcula o índice Gini-Simpson (1-D).Interpretação dos resultadosA comunidade 1 foi comunidade que apresentou maior diversidade de espécies (Shannon-Wiener = 2,3 e Gini-Simpson = 0.9), enquanto comunidade 2 foi comunidade que apresentou menor diversidade (Shannon-Wiener = 0,5 e Gini-Simpson = 0,17). Gostaríamos de chamar atenção para importância da distribuição da abundância relativa das espécies dentro das comunidades. Percebam que tanto comunidade 1 quanto comunidade 2 abrigam o mesmo número de espécies (10 espécies) e abundância total (100 indivíduos), mas os padrões de distribuição da abundância relativa entre espécies dentro das comunidades são bem discrepantes. Na comunidade 1 espécies apresentam abundâncias semelhantes entre elas (.e. alta equitabilidade), enquanto na comunidade 2 uma espécie é dominante e outras raras (.e. baixa equitabilidade). Essa diferença na distribuição da abundância relativa entre comunidades é um fator muito importante para os índices de diversidade.Dentro desta perspectiva, alguns índices fornecem uma estimativa sobre equitabilidade da distribuição da abundância nas comunidades. Entre eles, o mais conhecido foi proposto por Pielou (1966).Índice de Equabilidade (ou Equitabilidade) de PielouÉ uma métrica derivada índice de Shannon-Wiener que descreve o padrão de distribuição da abundância relativa das espécies na comunidade.\\[J = \\frac{H'}{Hmax} = \\frac{H'}{ln (S)}\\]onde:H’ = índice de Shannon-WienerHmax = todas espécies teriam mesma abundância relativaHmax é calculado aplicando o logaritmo natural (ln) para riqueza de espécies (S)Se todas espécies apresentam mesma abundância relativa, então J = 1. Se uma espécie apresenta forte dominância, J aproxima-se de zeroNão há uma função R que calcule o índice de Pielou, mas ele pode facilmente ser calculado usando os valores de diversidade de Shannon-Wiener e o logaritmo da riqueza de espécies de cada comunidade.Agora que temos uma ideia de como riqueza de espécies e distribuição da abundância relativa são importantes para quantificar os valores dos índices de diversidade, vamos testar se há alguma relação entre os índices de diversidade e precipitação anual nas comunidades.Vamos realizar uma regressão simples para verificar relação entre o índice de Shannon-Wiener e precipitação anual nas comunidades.Agora, vamos realizar uma regressão simples para verificar relação entre o índice de Simpson e precipitação anual nas comunidades.Por fim, vamos fazer regressão simples para verificar relação entre o índice de Pielou e precipitação anual nas comunidades. 📝 Importante \nanálises acima são apenas ilustrativas. Não estamos avaliando premissas de normalidade e homogeneidade da variância dos resíduos (veja Capítulo 7). Além disso, é importante estar ciente das críticas e limitações de usar índices de Shannon e Simpson como nesses exemplos (Lou Jost 2007).Interpretação dos resultadosA variação espacial na diversidade de espécies, obtida através dos índices de Shannon-Wiener e Simpson, e equitabilidade de Pielou não foram associados com variação na precipitação anual entre áreas (P > 0,05).","code":"\n## Índice de Shannon\n# MARGIN = 1 significa que a função irá calcular o índice considerando \n# as linhas do data.frame (comunidades).\nshannon_res <- diversity(composicao_especies, index = \"shannon\", MARGIN = 1)\nshannon_res\n#>     Com_1     Com_2     Com_3     Com_4     Com_5     Com_6     Com_7     Com_8     Com_9    Com_10 \n#> 2.3025851 0.5002880 0.9580109 1.6068659 1.4861894 1.5607038 0.6931472 1.1058899 1.7140875 1.2636544\n## Índice de Simpson\nsimpson_res <- diversity(composicao_especies, index = \"simpson\", MARGIN = 1) \nsimpson_res\n#>     Com_1     Com_2     Com_3     Com_4     Com_5     Com_6     Com_7     Com_8     Com_9    Com_10 \n#> 0.9000000 0.1710000 0.4814815 0.7989636 0.7587500 0.7674858 0.5000000 0.5850000 0.8088889 0.6942149\n## Índice de Pielou\nPielou <- shannon_res/log(specnumber(composicao_especies))\n## Juntando todos os dados em um único data frame\ndados_div <- data.frame(precipitacao$prec, shannon_res, \n                        simpson_res, Pielou)\n\n## Renomeando as colunas\ncolnames(dados_div) <- c(\"Precipitacao\", \"Shannon\", \"Simpson\", \"Pielou\")\n## ANOVA\nanova_shan <- lm(Shannon ~ Precipitacao, data = dados_div)\nanova(anova_shan)\n#> Analysis of Variance Table\n#> \n#> Response: Shannon\n#>              Df  Sum Sq Mean Sq F value Pr(>F)\n#> Precipitacao  1 0.10989 0.10989  0.3627 0.5637\n#> Residuals     8 2.42366 0.30296\n## ANOVA\nanova_simp <- lm(Simpson ~ Precipitacao, data = dados_div)\nanova(anova_simp)\n#> Analysis of Variance Table\n#> \n#> Response: Simpson\n#>              Df  Sum Sq  Mean Sq F value Pr(>F)\n#> Precipitacao  1 0.00132 0.001325  0.0252 0.8778\n#> Residuals     8 0.42064 0.052580\n## ANOVA\nanova_piel <- lm(Pielou ~ Precipitacao, data = dados_div)\nanova(anova_piel)\n#> Analysis of Variance Table\n#> \n#> Response: Pielou\n#>              Df  Sum Sq  Mean Sq F value Pr(>F)\n#> Precipitacao  1 0.09080 0.090798  1.5792 0.2443\n#> Residuals     8 0.45997 0.057496"},{"path":"cap12.html","id":"diagramas-de-whittaker-ou-curva-de-dominância","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.2.3 Diagramas de Whittaker ou Curva de Dominância","text":"Embora os índices de diversidade de espécies englobem os componentes de riqueza e abundância relativa das espécies nas suas estimativas, não é possível conhecer o número de espécies ou quais são espécies dominantes ou raras dentro das comunidades. Por exemplo, duas comunidades podem ter o mesmo valor de diversidade e ainda assim apresentarem diferenças na riqueza e equitabilidade (Melo 2008). O Diagrama de Whittaker é um método que lida com essas questões utilizando informações visuais número de espécies e abundância relativa de cada espécie nas comunidades. Este método plota espécies ranqueadas eixo X da mais abundante para menos abundante, enquanto eixo Y abundâncias relativas das espécies são plotadas em escala logarítmica (log10). Este gráfico permite ao leitor reconhecer: ) riqueza de espécies observando o eixo X, ii) equitabilidade da abundância relativa das espécies pela inclinação da reta, e iii) quais são espécies dominantes, intermediárias e raras nas comunidades através da observação em relação ao eixo Y. partir destas curvas, vários autores propuseram modelos matemáticos para explicar distribuição de abundância das espécies gerando diferentes modelos teóricos (e.g. série geométrica, broken-stick, log-series e log-normal). Cada modelo possui predições distintas: o modelo geométrico prediz distribuição de abundâncias desiguais, broken-stick prediz distribuição de abundâncias uniformes, enquanto log-normal e log-series são intermediárias com predições distintas sobre proporções de espécies raras - alta em log-series, baixa em log-normal (veja McGill et al. (2007) para revisão).Para análises exploratórias onde você tem interesse em visualizar o padrão da distribuição relativa das espécies por comunidade, função rankabundance pacote BiodiversityR é uma opção interessante (Figura 12.2).\nFigura 12.2: Diagramas de Whittaker para duas comunidades.\nInterpretação dos resultadosPercebam que olhando os eixos gráfico conseguimos determinar que Comunidade 2 (círculo laranja) abriga 10 espécies total (.e. comprimento eixo X), com espécie sp1 apresentando alta dominância e outras espécies apresentando abundâncias muito baixas. Comunidade 3 (círculo ciano) abriga cinco espécies total, sendo que espécie sp4 apresenta alta dominância, duas espécies apresentam abundâncias intermediárias e outras duas abundâncias baixas.","code":"\n## Cálculo da curva para as comunidades 2 e 3\nrank_com2 <- rankabundance(composicao_especies[2, composicao_especies[2,] > 0])\nrank_com3 <- rankabundance(composicao_especies[3, composicao_especies[3,] > 0])\n\n## Gráfico \n# Veja a ajuda da função rankabundplot para outros exemplos de gráficos.\nrankabunplot(rank_com2, scale = \"logabun\", specnames = c(1), \n             pch = 19, col = \"darkorange\")\nrankabunplot(rank_com3, scale = \"logabun\", specnames = c(1), pch = 19, \n             xlim = c(0,10), addit = TRUE, col = \"cyan4\" , legend = TRUE)\nlegend(5, 40, legend = c(\"Comunidade 2\", \"Comunidade 3\"),\n       col = c(\"darkorange\", \"cyan4\"), lty = 1, cex = 0.8, box.lty = 0)"},{"path":"cap12.html","id":"curvas-de-distribuição-de-abundâncias","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.2.4 Curvas de distribuição de abundâncias","text":"Caso o interesse seja avaliar qual dos modelos teóricos melhor explica distribuição das abundâncias das espécies, função radift() pacote vegan é melhor opção.função radfit avalia cinco modelos teóricos para determinar qual deles melhor se ajustam aos dados. Os modelos teóricos avaliados na função são:Null = modelo broken-stickpreemption = série geométricalog-normalZipfZipf-MandelbrotVocê pode realizar análises separadamente para cada comunidade ou para todas comunidades ao mesmo tempo.Vamos começar avaliando separadamente Comunidade 2.Agora vamos fazer um gráfico com predições dos modelos (Figura 12.3).\nFigura 12.3: Teste de cinco Diagramas de Whittaker para Comunidade 2.\nInterpretação dos resultadosOs pontos brancos representam espécies ranqueadas de acordo com abundância e linhas representam predições dos modelos matemáticos. Com base nos valores de AIC (veja Capítulo 7), o modelo Zipf é o melhor modelo que explica distribuição da abundância relativa das espécies na Comunidade 2.Agora vamos analisar os dados considerando todas comunidades (Figura 12.4).\nFigura 12.4: Testes de cinco Diagramas de Whittaker para todas comunidades.\nInterpretação dos resultadosA Comunidade 1 foi associada com o modelo log-normal, Comunidades 2 e 4 com o modelo Zipf, Comunidade 3 com o modelo série geométrica e outras comunidades com o modelo nulo. Para explorar explicação biológica por trás destes modelos veja os artigos (Wilson 1991; McGill et al. 2007; Magurran McGill 2011). 📝 Importante \nligação entre o modelo matemático e explicação biológica precisa ser interpretada com cuidado porque diferentes modelos matemáticos podem levar ao mesmo padrão de distribuição de abundância.","code":"\n## Teste das curvas de distribuição de abundâncias\ncurvas_dominancia_com2 <- radfit(composicao_especies[2,])\ncurvas_dominancia_com2\n#> \n#> RAD models, family poisson \n#> No. of species 10, total abundance 100\n#> \n#>            par1     par2    par3        Deviance AIC     BIC    \n#> Null                                    175.242  199.592 199.592\n#> Preemption  0.68962                      79.560  105.910 106.213\n#> Lognormal  -0.65366  3.2485              47.350   75.701  76.306\n#> Zipf        0.83829 -3.0254              26.612   54.963  55.568\n#> Mandelbrot  0.83829 -3.0254  1.6442e-07  26.612   56.963  57.871\nplot(curvas_dominancia_com2, \n     ylab = \"Abundância\", \n     xlab = \"Ranqueamento das espécies\")\n## Teste das curvas de distribuição de abundâncias\ncurvas_dominancia_todas <- radfit(composicao_especies)\ncurvas_dominancia_todas\n#> \n#> Deviance for RAD models:\n#> \n#>                  Com_1       Com_2       Com_3       Com_4       Com_5       Com_6       Com_7       Com_8       Com_9 Com_10\n#> Null        8.2193e+01  1.7524e+02  8.9085e+00  4.2265e+01  4.9719e+00  4.7099e+00  1.1507e+00  1.8998e+00  2.7703e+00 1.1146\n#> Preemption  2.2878e+01  7.9560e+01  1.5423e+00  1.4332e+01  3.0438e+00  4.5536e+00  7.7259e-01  1.7847e+00  9.2518e-01 0.7428\n#> Lognormal  -1.7764e-15  4.7350e+01  1.0161e+00  2.9441e-02  1.9303e+00  4.8898e+00 -2.2053e-25  1.4556e+00  2.0626e-01 0.5079\n#> Zipf        8.8818e-15  2.6612e+01  2.1659e-01  1.5846e-02  3.6094e+00  8.3245e+00 -2.2073e-25  6.6938e-01  4.7931e-01 0.8730\n#> Mandelbrot -1.7764e-15  2.6612e+01  2.0926e-01  1.1390e-02  1.8740e+00  4.1131e+00  0.0000e+00  6.6938e-01  2.3634e-01 0.4456\n\n# Vamos fazer um gráfico para cada comunidade\nplot(curvas_dominancia_todas, log = \"y\")"},{"path":"cap12.html","id":"números-de-hill-ou-série-de-hill","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.2.5 Números de Hill ou Série de Hill","text":"Embora os índices de Shannon-Wiener e Gini-Simpson sejam amplamente usados em estudos ecológicos e de conservação, eles sofrem de propriedades matemáticas e não representam diversidade propriamente dita (L. Jost 2006). Portanto, quando o objetivo é avaliar diversidade, os índices de Shannon-Wiener e Gini-Simpson não deveriam ser utilizados na sua forma padrão, mas transformados em números efetivos de espécies ou diversidade verdadeira (L. Jost 2006). O número efetivo de espécies é o número de espécies igualmente abundantes (.e. todas espécies com mesma abundância) necessárias para produzir o valor observado para um determinado índice. Por exemplo, uma comunidade com índice de Shannon-Wiener estimado de 4,5 teria um número efetivo de 90 espécies igualmente abundantes. Jost et al. (2006) usam o seguinte exemplo para explicar o conceito número efetivo de espécies - uma comunidade com 16 espécies igualmente abundantes é duas vezes mais diversa que uma comunidade com 8 espécies igualmente abundantes. Neste caso, diversidade deveria ser proporcional ao número de espécies. Contudo, quando aplicamos os índices de diversidade para estas comunidades com 16 e 8 espécies (cada espécie com 5 indivíduos), o índice de Shannon-Wiener é 2,772 e 2,079, respectivamente, e o índice de Gini-Simpson é 0,937 e 0,875, respectivamente. Claramente, os valores estimados pelos índices de diversidade não representam diferença entre comunidades porque eles carecem de uma particularidade matemática conhecida como propriedade de duplicação.O próximo exemplo (modificado website de Lou Jost; http://www.loujost.com/), demostra importância da transformação dos índices de diversidade em números efetivos de espécies. Imagine que você foi contratado para avaliar diversidade de peixes em um riacho antes e depois da instalação de uma usina hidrelétrica. Suponha que os valores estimados pelo índice de Gini-Simpson foi de 0,99 antes da instalação e de 0,97 depois da instalação. princípio, você poderia concluir que diversidade diminuiu somente 2% e que instalação da hidrelétrica não afetou diversidade de peixes riacho. Contudo, transformando os valores índice de diversidade em números efetivos, percebemos que antes da instalação diversidade riacho equivale 100 espécies igualmente abundantes, enquanto após instalação, diversidade equivale 33 espécies igualmente abundantes. Portanto, queda da diversidade foi de 66% e não 2%.Hill (1973) derivou uma equação geral para o cálculo número efetivo de espécies ou diversidade verdadeira que depende apenas valor de q e da abundância relativa das espécies.\\[^qD = (\\sum_{=1}^{S}p_i^q)^{1/(1-q)}\\]Onde:q = é um parâmetro conhecido como ordem da diversidade e é usado para dar peso às espécies comuns ou raras. q = 0 não considera frequência das espécies e representa riqueza observada de espécies, q = 1 equivale transformação índice de Shannon-Wiener (.e. exp(H')) e atribui pesos às espécies com base na proporção das suas frequências e, q = 2 equivale à transformação índice de Gini-Simpson (.e. 1/(1-D)) e atribui peso às espécies mais comuns. Valores de q \\<1 favorecem espécies raras, enquanto valores de q \\> 1 favorecem espécies comuns.q = é um parâmetro conhecido como ordem da diversidade e é usado para dar peso às espécies comuns ou raras. q = 0 não considera frequência das espécies e representa riqueza observada de espécies, q = 1 equivale transformação índice de Shannon-Wiener (.e. exp(H')) e atribui pesos às espécies com base na proporção das suas frequências e, q = 2 equivale à transformação índice de Gini-Simpson (.e. 1/(1-D)) e atribui peso às espécies mais comuns. Valores de q \\<1 favorecem espécies raras, enquanto valores de q \\> 1 favorecem espécies comuns.pi = abundância relativa de cada espécie, calculada pela proporção dos indivíduos de uma espécie pelo número total dos indivíduos na comunidadepi = abundância relativa de cada espécie, calculada pela proporção dos indivíduos de uma espécie pelo número total dos indivíduos na comunidadeVamos calcular o Número de Hill para comunidades nosso exemplo.Calculando o Número de Hill com q = 0.Calculando o Número de Hill com q = 1.Calculando o Número de Hill com q = 2.Criando um data frame com os três resultados anteriores.Interpretação dos resultadosComo na comunidade 1 todas espécies são igualmente abundantes, alterar os valores de q não altera o número efetivo de espécies que permanece sempre 10. Contudo, na comunidade 2, que apresenta alta dominância de uma espécie, alterar os valores de q diminui consideravelmente estimativa da diversidade. vantagem dos Números de Hill é que eles são de fácil interpretação e comparação entre comunidades. Fator ausente para os índices de diversidade. 📝 Importante \nNeste ponto, esperamos que tenha ficado claro que mais que riqueza de espécies, abundância relativa das espécies (e.g. comuns ou raras) tem um papel fundamental na estimativa da diversidade de espécies.","code":"\n## Número de Hill para q = 0\nhill_res_q_0 <- hill_taxa(composicao_especies, q  = 0)\nhill_res_q_0\n#>  Com_1  Com_2  Com_3  Com_4  Com_5  Com_6  Com_7  Com_8  Com_9 Com_10 \n#>     10     10      5      5      5      6      2      4      6      4\n## Número de Hill para q = 1\nhill_res_q_1 <- hill_taxa(composicao_especies, q  = 1)\nhill_res_q_1\n#>     Com_1     Com_2     Com_3     Com_4     Com_5     Com_6     Com_7     Com_8     Com_9    Com_10 \n#> 10.000000  1.649196  2.606507  4.987156  4.420220  4.762172  2.000000  3.021912  5.551608  3.538328\n## Número de Hill para q = 2\nhill_res_q_2 <- hill_taxa(composicao_especies, q  = 2)\nhill_res_q_2\n#>     Com_1     Com_2     Com_3     Com_4     Com_5     Com_6     Com_7     Com_8     Com_9    Com_10 \n#> 10.000000  1.206273  1.928571  4.974223  4.145078  4.300813  2.000000  2.409639  5.232558  3.270270\n## Resultados\nres_hill <- data.frame(hill_res_q_0, hill_res_q_1, hill_res_q_2)\ncolnames(res_hill) <- c(\"q=0\", \"q=1\", \"q=2\")\nhead(res_hill)\n#>       q=0       q=1       q=2\n#> Com_1  10 10.000000 10.000000\n#> Com_2  10  1.649196  1.206273\n#> Com_3   5  2.606507  1.928571\n#> Com_4   5  4.987156  4.974223\n#> Com_5   5  4.420220  4.145078\n#> Com_6   6  4.762172  4.300813"},{"path":"cap12.html","id":"diversidade-beta","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.3 Diversidade beta","text":"O termo diversidade beta foi proposto por Whittker (1960) e foi definido como razão entre diversidade gama e diversidade alfa (.e. diversidade beta multiplicativa), quantificando não só relação entre diversidade regional e local, mas também o grau de diferenciação entre comunidades. Para demonstrar como diversidade beta varia entre comunidades locais dentro de uma região usaremos explicação Baselga (http://webspersoais.usc.es/persoais/andres.baselga/beta.html). Imagine três comunidades, cada comunidade abrigando mesmas cinco espécies. Neste caso, média da diversidade alfa = 5, diversidade gama = 5 e razão entre elas (gama/alfa) indica uma diversidade beta = 1. Isso significa que na região existe apenas uma unidade distinta de composição. Quando composição de espécies das três comunidades é completamente diferente (.e. diferenciação máxima), temos que média da diversidade alfa = 5, diversidade gama = 15 e razão entre elas indica uma diversidade beta = 3. Neste caso, existem três unidades distintas dentro da região. Assim, diversidade beta multiplicativa varia de 1 até o número de comunidades dentro da região.maioria dos índices de (dis)similaridade utilizadas na ecologia (e.g. índices de Jaccard e Sørensen) são índices que padronizam diversidade beta e geram valores independentes número de comunidades. Eles podem ser calculados para dados de incidência (presença e ausência) ou abundância (Legendre Legendre 2012) e considerando comparações par--par entre comunidades ou comparação entre múltiplas comunidades (.e. multiple-site). Por muito tempo, os valores de (dis)similaridade foram interpretados como sinônimo de substituição de espécies (turnover) entre comunidades. Contudo, índices de (dis)similaridade como Jaccard e Sørensen geram valores de (dis)similaridade para comunidades que não apresentam diferenças na composição de espécies, mas apresentam diferenças na riqueza de espécies (.e. comunidades aninhadas). Pensando nestes fatores, Baselga (2010, 2012) propôs uma abordagem que particiona diversidade beta total (\\(\\beta\\)jac) em dois componentes: o componente resultante da substituição de espécies (turnover - \\(\\beta\\)tur) e o componente resultante aninhamento (nestedness, .e. diferença na riqueza de espécies - \\(\\beta\\)nes). Vejam (Figura 12.5) onde temos 3 comunidades (X, Y e Z). primeiro exemplo, temos apenas diferença número de espécies entre comunidades. Neste caso, o componente subsituição de espécies (\\(\\beta\\)tur) é zero porque espécies na comunidade Z são um sub-grupo das espécies nas comunidades X e Y. O mesmo para espécies na comunidade Y que são um sub-grupo das espécies da comunidade X. segundo exemplo, temos o cenário oposto com comunidades abrigando mesma riqueza de espécies e assim, o componente resultante aninhamento (\\(\\beta\\)nes) é zero, temos um valor máximo para o \\(\\beta\\)tur. Percebam que somando \\(\\beta\\)tur com \\(\\beta\\)nes temos o valor da diversidade beta total (\\(\\beta\\)jac). Baselga (2013) também propôs partição da diversidade beta para índices de dissimilaridade que lidam com dados de abundância. Neste caso os componentes da diversidade beta são chamados de variação balanceada na abundância (similar ao componente substituição de espécies) e gradiente de abundância (similar ao componente aninhamento). Reconhecer estes componentes da diversidade beta é importante porque eles apresentam padrões distintos (substituição de espécies versus perda ordenada de espécies), que provavelmente estão sendo gerados por processos ecológicos diferentes (Baselga 2010, 2012, 2013).\nFigura 12.5: Partição da diversidade beta taxonômica. Os três cenários apresentados representam diversidade beta explicada somente por substituição, aninhamento e uma combinação dos dois.\nAqui, vamos demonstrar alguns exemplos de como calcular partição da diversidade beta para os dados deste capítulo.Para isso, primeiro vamos transformar nossos dados de abundância em presença e ausência.Calculando diversidade beta par par usando os dados de presença e ausência. 📝 Importante \nfunção beta.pair() gera três listas com matrizes triangulares:Diversidade beta total = índice de Sørensen (beta.sor)Componente de substituição = índice de Simpson (beta.sim)Componente de aninhamento = diferença na riqueza (beta.sne)Vamos olhar os resultados da diversidade beta total.Vamos montar um data frame com os resultados 📝 Importante \nPercebam que primeira linha e primeira coluna data frame (.e., 0.00) representa dissimilaridade de Sørensen entre Com1 e Com2 (compare com os valores da matriz triangular acima). linhas subsequentes representam dissimilaridade da Com1 com todas outras comunidades, depois da Com2 com todas comunidades e assim sucessivamente. Lembrem-se que os componentes, subsituição (Simpson) e aninhamento, são um desdobramento da diversidade beta total (Sørensen). Assim, soma da dissimilaridade de Simpson e aninhamento é igual ao valor de dissimilaridade de Sørensen (Baselga 2010, 2012).Vamos calcular dissimilaridade entre precipitação anual das comunidades usando o índice de distância euclidiana. Vejam ajuda da função vegdist() que calcula 17 índices diferentes de dissimilaridade.Agora vamos juntar os resultados. 📝 Importante \ncomunidades devem estar dispostas na mesma ordem nas duas planilhas (composição de espécies e precipitação) para que os resultados representem dissimilaridades par par para mesmas comunidades data frame.Criando data frame.Vamos testar relação entre diferenças na composição de espécies e\nprecipitação nas comunidades.Interpretação dos resultadosHá uma relação positiva entre o componente aninhado da diversidade beta e diferença na precipitação entre comunidades (F1,43 = 6,4, P = 0,01). Contudo, não há relação entre diversidade beta total (Sørensen) e o componente substituição de espécies (Simpson) com precipitação (P > 0,05).Agora vamos fazer um gráfico com o componente aninhamento da diversidade\nbeta (Figura 12.6).\nFigura 12.6: Relação entre o componente de aninhamento da diversidade beta e diferença da precipitação.\nInterpretação dos resultadosAs comunidades localizadas em locais com baixa precipitação anual apresentam espécies que são um subgrupo das espécies presentes nas comunidades com alta precipitação anual.Agora vamos fazer um exemplo considerando os dados de abundância das espécies.função beta.pair.abund() gera três listas com matrizes triangulares:Diversidade beta total = índice de Bray-Curtis (beta.bray)Componente variação balanceada (beta.bray.bal)Componente gradiente de abundância (beta.bray.gra)Análise.Cria um data frame com os resultados.Testar relação da dissimilaridade considerando abundância com diferença na precipitação entre comunidades.Interpretação dos resultadosHá uma relação positiva entre os componentes variação balanceada (F1,43 = 7,07, P = 0,01) e gradiente (F1,43 = 18,7, P < 0,001) de abundância da diversidade beta com diferença na precipitação entre comunidades. Contudo, não há relação entre diversidade beta total (Bray) com precipitação (F1,43 = 0,84, P = 0,36).Vamos fazer um gráfico para variação balanceada da diversidade beta (Figura 12.7).\nFigura 12.7: Relação entre o componente variação balanceada da diversidade beta e diferença da precipitação.\nInterpretação dos resultadosOlhando o início eixo X onde comunidades apresentam precipitação anual similares (.e. baixa diferença na precipitação), o componente variação balanceada indica que há uma tendência das espécies com maiores abundâncias não serem mesmas quando comparamos duas comunidades (.e. maiores valores de dissimilaridade). Por outro lado, quando diferença na precipitação entre duas comunidades é alta, o componente variação balanceada é baixo, indicando que mesmas espécies estão dominando abundância nas comunidades comparadas.Vamos fazer agora um gráfico para variação gradiente da diversidade beta (Figura 12.8).\nFigura 12.8: Relação entre o componente variação gradiente da diversidade beta e diferença da precipitação.\nInterpretação dos resultadosOlhando o início eixo X onde comunidades apresentam precipitação anual similares (.e. baixa diferença na precipitação), o componente gradiente indica que há uma tendência das espécies apresentarem abundâncias parecidas (.e. menor valor de dissimilaridade). Por outro lado, quando diferença na precipitação entre duas comunidades é alta, o componente gradiente é alto, indicando que mesmas espécies têm valores discrepantes de abundâncias entre comunidades.","code":"\n## Transformando dados em presencia e ausência.\ncomposicao_PA <- decostand(composicao_especies, method = \"pa\")\n## Diversidade beta\nresultado_PA <- beta.pair(composicao_PA, index.family = \"sorensen\")\n## Resultados\nresultado_PA$beta.sor\n#>            Com_1     Com_2     Com_3     Com_4     Com_5     Com_6     Com_7     Com_8     Com_9\n#> Com_2  0.0000000                                                                                \n#> Com_3  0.3333333 0.3333333                                                                      \n#> Com_4  0.3333333 0.3333333 1.0000000                                                            \n#> Com_5  0.3333333 0.3333333 0.6000000 0.4000000                                                  \n#> Com_6  0.2500000 0.2500000 0.4545455 0.4545455 0.4545455                                        \n#> Com_7  0.6666667 0.6666667 0.7142857 0.7142857 1.0000000 0.7500000                              \n#> Com_8  0.4285714 0.4285714 0.7777778 0.3333333 0.3333333 0.2000000 1.0000000                    \n#> Com_9  0.2500000 0.2500000 0.4545455 0.4545455 0.2727273 0.5000000 0.7500000 0.4000000          \n#> Com_10 0.4285714 0.4285714 0.3333333 0.7777778 0.5555556 0.4000000 0.6666667 0.7500000 0.6000000\n## Data frame com os resultados\ndata.frame_PA <- data.frame(round(as.numeric(resultado_PA$beta.sor), 2),\n                            round(as.numeric(resultado_PA$beta.sim), 2),\n                            round(as.numeric(resultado_PA$beta.sne), 2))\ncolnames(data.frame_PA) <- c(\"Sorensen\", \"Simpson\", \"Aninhamento\")\nhead(data.frame_PA)\n#>   Sorensen Simpson Aninhamento\n#> 1     0.00       0        0.00\n#> 2     0.33       0        0.33\n#> 3     0.33       0        0.33\n#> 4     0.33       0        0.33\n#> 5     0.25       0        0.25\n#> 6     0.67       0        0.67\n## Dissimilaridade\nprec_dis <- vegdist(precipitacao, method = \"euclidian\")\ndados_prec <- as.numeric(prec_dis) \n## Data frame\ndados_dis <- data.frame(dados_prec, data.frame_PA)\nhead(dados_dis)\n#>   dados_prec Sorensen Simpson Aninhamento\n#> 1         88     0.00       0        0.00\n#> 2        400     0.33       0        0.33\n#> 3       1400     0.33       0        0.33\n#> 4        294     0.33       0        0.33\n#> 5        195     0.25       0        0.25\n#> 6       2270     0.67       0        0.67\n## ANOVA\n# Avaliar a relação entre os valores de diversidade beta total (Sørensen) e precipitação.\nanova_sore <-lm(Sorensen ~ dados_prec, data = dados_dis)\nanova(anova_sore)\n#> Analysis of Variance Table\n#> \n#> Response: Sorensen\n#>            Df  Sum Sq  Mean Sq F value Pr(>F)\n#> dados_prec  1 0.00188 0.001877  0.0358 0.8508\n#> Residuals  43 2.25264 0.052387\n\n# Avaliar a relação entre os valores do componente substituição (Simpson) e precipitação\nanova_simp <-lm(Simpson ~ dados_prec, data = dados_dis)\nanova(anova_simp)\n#> Analysis of Variance Table\n#> \n#> Response: Simpson\n#>            Df Sum Sq  Mean Sq F value Pr(>F)\n#> dados_prec  1 0.1403 0.140342  1.4905 0.2288\n#> Residuals  43 4.0488 0.094157\n\n# Avaliar a relação entre os valores do componente aninhamento e precipitação\nanova_anin <-lm(Aninhamento ~ dados_prec, data = dados_dis)\nanova(anova_anin)\n#> Analysis of Variance Table\n#> \n#> Response: Aninhamento\n#>            Df  Sum Sq Mean Sq F value  Pr(>F)  \n#> dados_prec  1 0.17467 0.17467  6.4006 0.01515 *\n#> Residuals  43 1.17349 0.02729                  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Gráfico\nggplot(data = dados_dis, aes(x = dados_prec, y = Aninhamento)) + \n    geom_point(size = 4, shape = 21, fill = \"darkorange\") +\n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    labs(x = \"Diferença precipitação (mm)\", \n         y = \"Componente aninhamento da\\n diversidade beta\") +\n    tema_livro()\n## Diversidade beta para abundância\nresultado_AB <- beta.pair.abund(composicao_especies, index.family = \"bray\")\n## Data frame\n# Vamos montar um data.frame com os resultados.\ndata.frame_AB <- data.frame(round(as.numeric(resultado_AB$beta.bray), 2),\n                            round(as.numeric(resultado_AB$beta.bray.bal), 2),\n                            round(as.numeric(resultado_AB$beta.bray.gra), 2))\ncolnames(data.frame_AB) <- c(\"Bray\", \"Balanceada\", \"Gradiente\")\nhead(data.frame_AB)\n#>   Bray Balanceada Gradiente\n#> 1 0.81       0.81      0.00\n#> 2 0.69       0.42      0.27\n#> 3 0.45       0.38      0.06\n#> 4 0.47       0.07      0.40\n#> 5 0.47       0.15      0.31\n#> 6 0.92       0.00      0.92\n\n## Agora vamos juntar os resultados com a precipitação\ndados_dis_AB <- data.frame(dados_prec, data.frame_AB)\n## ANOVA\n# Avaliar a relação entre os valores de diversidade beta total e precipitação\nanova_dis_AB <- lm(Bray ~ dados_prec, data = dados_dis_AB)\nanova(anova_dis_AB)\n#> Analysis of Variance Table\n#> \n#> Response: Bray\n#>            Df  Sum Sq  Mean Sq F value Pr(>F)\n#> dados_prec  1 0.01782 0.017815  0.8441 0.3634\n#> Residuals  43 0.90755 0.021106\n\n# Avaliar a relação entre os valores do componente balanceada e precipitação\nanova_balan <- lm(Balanceada ~ dados_prec, data = dados_dis_AB)\nanova(anova_balan)\n#> Analysis of Variance Table\n#> \n#> Response: Balanceada\n#>            Df  Sum Sq Mean Sq F value  Pr(>F)  \n#> dados_prec  1 0.48761 0.48761  7.0742 0.01094 *\n#> Residuals  43 2.96391 0.06893                  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Avaliar a relação entre os valores do componente gradiente e precipitação\nanova_grad <- lm(Gradiente ~ dados_prec, data = dados_dis_AB)\nanova(anova_grad)\n#> Analysis of Variance Table\n#> \n#> Response: Gradiente\n#>            Df  Sum Sq Mean Sq F value    Pr(>F)    \n#> dados_prec  1 0.68981 0.68981  18.705 8.903e-05 ***\n#> Residuals  43 1.58575 0.03688                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nggplot(data = dados_dis_AB, aes(x = dados_prec, y = Balanceada)) + \n    geom_point(size = 4, shape = 21, fill = \"darkorange\") +\n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    labs(x = \"Diferença precipitação (mm)\",\n         y = \"Componente variação balanceada\\n da diversidade beta\") +\n    tema_livro() \nggplot(data = dados_dis_AB, aes(x = dados_prec, y = Gradiente)) + \n    geom_point(size = 4, shape = 21, fill = \"darkorange\") +\n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    labs(x = \"Diferença precipitação anual (mm)\", \n         y = \"Componente gradiente de abundância\\n da diversidade beta\") +\n    tema_livro()"},{"path":"cap12.html","id":"para-se-aprofundar-8","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.4 Para se aprofundar","text":"","code":""},{"path":"cap12.html","id":"livros-7","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.4.1 Livros","text":"Recomendamos leitura dos artigos citados capítulo e os livros de Magurran & McGill (2011) - Biological Diversity Frontiers Measurement Assessment e Legendre & Legendre (2012) - Numerical Ecology.","code":""},{"path":"cap12.html","id":"links-7","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.4.2 Links","text":"Recomendamos página pessoal pesquisador Lou Jost que apresenta e discute diversas medidas de diversidade taxonômica:Medidas de diversidade e similaridadeMedindo diversidade de uma única localidadeComparando diversidade entre duas comunidadesNúmero efetivo de espécies","code":""},{"path":"cap12.html","id":"exercícios-8","chapter":"Capítulo 12 Diversidade Taxonômica","heading":"12.5 Exercícios","text":"12.1\nCarregue os dados - anuros_composicao - que está pacote ecodados. Este conjunto de dados representa abundância de 211 espécies de anuros coletados em 44 localidades na Mata Atlântica. Calcule riqueza de espécies para cada comunidade e os índices de Margalef, Menhinich, Shannon-Wiener, Gini-Simpson e Equitabilidade de Pielou. Salve todos os resultados em novo data frame. Faça um gráfico usando o função ggpairs para ver correlação entre métricas. Qual sua interpretação sobre os resultados?12.2\nUsando os resultados anteriores, selecione duas comunidades com os maiores e menores valores de Shannon-Wiener. Em seguida, faça um Diagrama de Whittaker. Por fim, interprete curvas considerando curvas teóricas (.e., geométrica, broken-stick, etc.) descritas nos livros de ecologia.12.3\nUsando os dados - anuros_composicao - calcule partição da diversidade beta considerando os dados de abundância e presença e ausência. ) Faça um gráfico boxplot com os resultados. Discuta se os resultados usando abundância ou presença e ausência são congruentes ou discrepantes. b) Calcule distância geográfica (use planilha anuros_ambientais) entre localidades (use Distância Euclidiana). Em seguida, faça uma análise de regressão para verificar se localidades que estão próximas apresentam maior similaridade na composição de espécies (use componente turnover - Bsim) que comunidades que estão distantes (e.g., Decaimento da similaridade).Soluções dos exercícios.","code":""},{"path":"cap13.html","id":"cap13","chapter":"Capítulo 13 Diversidade Filogenética","heading":"Capítulo 13 Diversidade Filogenética","text":"","code":""},{"path":"cap13.html","id":"pré-requisitos-do-capítulo-9","chapter":"Capítulo 13 Diversidade Filogenética","heading":"Pré-requisitos do capítulo","text":"Pacotes e dados que serão utilizados neste capítulo.","code":"\n## Pacotes\nlibrary(devtools)\nlibrary(ecodados)\nlibrary(V.PhyloMaker)\nlibrary(vegan)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(ggpubr)\nlibrary(picante)\nlibrary(phytools)\nlibrary(ape)\nlibrary(geiger)\nlibrary(phyloregion)\nlibrary(pez)\nlibrary(reshape2)\nlibrary(betapart)\n\n## Dados\nminha_arvore <- ecodados::filogenia_aves\nespecies_plantas <- ecodados::sp_list\ncomunidade <- ecodados::comm\ncomposicao_especies <- ecodados::composicao_aves_filogenetica\nfilogenia_aves <- ecodados::filogenia_aves\nprecipitacao <- precipitacao_filogenetica"},{"path":"cap13.html","id":"aspectos-teóricos-4","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.1 Aspectos teóricos","text":"diversidade filogenética captura ancestralidade compartilhada entre espécies em termos de quantidade da história evolutiva e o grau de parentesco entre espécies. Pesquisadores têm utilizado diferentes métricas de diversidade filogenética em duas linhas de investigações principais: ) incorporar história evolutiva das espécies na seleção das áreas prioritárias para conservação visando minimizar perda da diversidade evolutiva (Vane-Wright, Humphries, Williams 1991; Faith 1992; Véron et al. 2019), e ii) produzir explicações sobre os processos atuando na montagem das comunidades (Webb et al. 2002; Helmus et al. 2007). quantidade de artigos abordando ecologia, macroecologia e conservação com diversidade filogenética cresceram exponencialmente nas últimas décadas (Véron et al. 2019). Seguindo esta tendência, o número de métricas de diversidade filogenética propostas não param de aumentar. Tucker et al. (2017) revisaram 70 métricas de diversidade filogenética e classificaram estas métricas em três dimensões: ) riqueza - representa soma da diferença filogenética acumulada entre táxons, ii) divergência - representa o padrão de diferença filogenética entre táxons de uma assembleia, e iii) regularidade - representa o grau de variação das diferenças filogenéticas entre táxons em uma assembleia. Outros autores utilizaram diferentes classificações (Pavoine Bonsall 2011; Vellend et al. 2011; Garamszegi 2014). Neste capítulo, iremos seguir classificação de Tucker et al. (2017) e mostrar algumas das principais métricas dentro de cada uma dessas dimensões. 📝 Importante \nAlguns autores recomendam que os pesquisadores não foquem em apenas uma dimensão, mas comparem métricas de diferentes dimensões (Tucker et al. 2017).","code":""},{"path":"cap13.html","id":"manipulação-de-filogenias","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.2 Manipulação de filogenias","text":"Nesta seção, iremos descrever os códigos em R para carregar, plotar, acessar os dados, e excluir e adicionar espécies em filogenias. Estes são códigos introdutórios e necessários para realizarmos análises de diversidade filogenética. Não iremos descrever os comandos necessários para construir uma filogenia. Estamos assumindo que já existe uma filogenia disponível para os organismos de interesse.Mas antes, vamos entender principais terminologias de uma filogenia e analisá-las graficamente (Figura 13.1).Árvore filogenética: são hipóteses que representam relação de parentesco entre espécies (pode ser também indivíduos, genes, etc.) com informações sobre quais espécies compartilham um ancestral comum e distância (tempo, genética, ou diferenças nos caracteres) que separamÁrvore filogenética: são hipóteses que representam relação de parentesco entre espécies (pode ser também indivíduos, genes, etc.) com informações sobre quais espécies compartilham um ancestral comum e distância (tempo, genética, ou diferenças nos caracteres) que separamNó: o ponto onde uma linhagem dá origem duas ou mais linhagens descendentesNó: o ponto onde uma linhagem dá origem duas ou mais linhagens descendentesPolitomia: três ou mais linhagens descendendo de um único nóPolitomia: três ou mais linhagens descendendo de um único nóRamo: uma linha orientada ao longo de um eixo terminais-raiz que conecta os nós na filogeniaRamo: uma linha orientada ao longo de um eixo terminais-raiz que conecta os nós na filogeniaTerminal (inglês tip): o final ramo representando uma espécie atual ou extinta (pode também representar gêneros, indivíduos, genes, etc.)Terminal (inglês tip): o final ramo representando uma espécie atual ou extinta (pode também representar gêneros, indivíduos, genes, etc.)Raiz: representa o ancestral comum de todas espécies na filogeniaRaiz: representa o ancestral comum de todas espécies na filogeniaClado: um grupo de espécies aparentadas descendendo de um único nó na filogeniaClado: um grupo de espécies aparentadas descendendo de um único nó na filogeniaUltramétrica: distância de todos os terminais até raiz são idênticas. Característica requerida pela maioria dos índices de diversidade filogenéticaUltramétrica: distância de todos os terminais até raiz são idênticas. Característica requerida pela maioria dos índices de diversidade filogenética\nFigura 13.1: Ilustrações de diferentes árvores filogenéticas. ) Árvore enraizada e ultramétrica indicando raiz da árvore, nós, ramos, comprimento ramo, politomias e terminais. B) Árvore não enraizada que mostra relações entre espécies, mas não define história evolutiva. C) Árvore não ultramétrica onde espécies apresentam diferentes distâncias até raiz.\nAgora vamos plotar filogenia para visualizar relações entre 37 espécies de aves endêmicas da Mata Atlântica. Essa filogenia foi extraída de Jetz et al. (2012). Os dados estão disponíveis pacote ecodados (Figura 13.2).\nFigura 13.2: Filogenia de 37 espécies de aves endêmicas da Mata Atlântica.\nPodemos alterar o formato de apresentação da filogenia usando o argumento type e cor dos ramos usando o argumento edge.color (Figura 13.3).\nFigura 13.3: Filogenia de 37 espécies de aves endêmicas da Mata Atlântica, com alterações de parametros de visualização.\nPercebam que existem vários argumentos para modificar largura e cor dos ramos, tamanho da fonte, distância entre filogenia e os nomes das espécies e muito mais. Uma sugestão é visitar o blog professor Liam Revell (http://blog.phytools.org/) que é o criador e mantenedor pacote phytools R.Acessar informações da filogeniaUma das características mais interessantes R é que podemos acessar informações objeto que contém filogenia. Neste caso, o nosso objeto é filogenia e, muitas vezes, temos interesse nas informações que estão inseridas dentro da filogenia. Para sabermos quais são informações que podemos acessar na filogenia, vamos usar função names().Temos acesso quatro componentes da filogenia: ) ramo (edge), ii) comprimento ramo (edge.length), iii) número de nós (Nnode), e iv) nome das espécies (tip.label). Podemos usar o operador $ para acessar estes componentes. Veja abaixo como acessar o nome das 37 espécies de aves na filogenia ou o comprimento de cada um dos ramos da filogenia.Remover espécies da filogeniaNas análises de diversidade filogenética, espécies que estarão presentes na filogenia normalmente são aquelas que foram amostradas seu estudo. Contudo, muitas vezes utilizamos filogenias contendo espécies que não estão presentes nosso estudo. Neste caso, precisamos excluir essas espécies da filogenia. função drop.tip() faz essa tarefa.Vejam que agora filogenia tem 35 espécies de aves. duas espécies que selecionamos foram excluídas da filogenia.Adicionar espécies na filogeniaOutra situação bem comum é quando precisamos inserir espécies que foram amostradas nosso estudo, mas não estão presente na filogenia. Para isso, vamos usar função add.species..genus(). função force.ultrametric() é usada para que filogenia continue sendo ultramétrica (sem essa função árvore perde os comprimentos dos ramos) 📝 Importante \nO comprimento ramo que espécie irá receber dependerá de onde você indicar inserção da espécie.opções são:\n- root: insere espécie ancestral comum mais recente (MRCA) de todas espécies gênero (default)\n- random: insere espécie aleatoriamente dentro clado MRCA contendo todos espécies gêneroAgora vamos inserir várias espécies dentro mesmo gênero.Vamos plotar essa nova filogenia (Figura 13.4).\nFigura 13.4: Filogenia de espécies de aves endêmicas da Mata Atlântica, com adição de espécies.\nVamos fazer outro exemplo usando função phylo.maker() pacote V.PhyloMaker que adiciona espécies nos gêneros ou os gêneros nas famílias usando uma filogênia backbone.Essa função permite adição dos gêneros ou espécies considerando três cenários diferentes:Cenário 1: adiciona gêneros ou espécies como politomias basais dentro das famílias ou gêneros da filogenia respectivamenteCenário 2: adiciona gêneros e espécies aleatoriamente nas famílias ou gêneros da filogenia respectivamenteCenário 3: adiciona gêneros e espécies nas famílias ou gêneros da filogenia respectivamente usando abordagens implementadas Phylomatic e BLADJVamos essa filogenia criada pelo pacote V.PhyloMaker (Figura 13.5).\nFigura 13.5: Filogenia de espécies de plantas criada pelo pacote V.PhyloMaker.\n","code":"\n## Gráfico\nplot.phylo (minha_arvore, type = \"phylogram\", show.tip.label = TRUE, \n            show.node.label = TRUE, edge.color = \"black\", edge.width = 1.5, \n            tip.color = \"black\", cex = 0.45, label.offset = 2) \n## Gráfico\nplot.phylo (minha_arvore, type = \"fan\", show.tip.label = TRUE, \n            show.node.label = TRUE, edge.color = \"blue\", edge.width = 1.5, \n            tip.color = \"black\", cex = 0.45, label.offset = 2) \n## Nomes\nnames(minha_arvore)\n#> [1] \"edge\"        \"edge.length\" \"Nnode\"       \"tip.label\"\n## Nome das espécies\nminha_arvore$tip.label\n#>  [1] \"Cranioleuca_pallida\"          \"Synallaxis_ruficapilla\"       \"Phacellodomus_ferrugineigula\" \"Cinclodes_pabsti\"            \n#>  [5] \"Conopophaga_melanops\"         \"Herpsilochmus_pileatus\"       \"Pyriglena_leucoptera\"         \"Formicivora_serrana\"         \n#>  [9] \"Chiroxiphia_caudata\"          \"Neopelma_aurifrons\"           \"Carpornis_cucullata\"          \"Mionectes_rufiventris\"       \n#> [13] \"Phylloscartes_kronei\"         \"Dacnis_nigripes\"              \"Ramphocelus_bresilius\"        \"Sporophila_frontalis\"        \n#> [17] \"Tangara_seledon\"              \"Euphonia_pectoralis\"          \"Cyanocorax_caeruleus\"         \"Brotogeris_tirica\"           \n#> [21] \"Pionopsitta_pileata\"          \"Pyrrhura_frontalis\"           \"Ramphastos_dicolorus\"         \"Pteroglossus_bailloni\"       \n#> [25] \"Veniliornis_maculifrons\"      \"Melanerpes_flavifrons\"        \"Malacoptila_striata\"          \"Strix_hylophila\"             \n#> [29] \"Pulsatrix_koeniswaldiana\"     \"Megascops_sanctaecatarinae\"   \"Leucopternis_polionotus\"      \"Buteogallus_lacernulatus\"    \n#> [33] \"Thalurania_glaucopis\"         \"Stephanoxis_lalandi\"          \"Aramides_saracura\"            \"Ortalis_guttata\"             \n#> [37] \"Tinamus_solitarius\"\n\n## Comprimento dos ramos\nminha_arvore$edge.length\n#>  [1]   8.3802647  18.8669712   1.7333865   3.6642170  10.6732942  15.2239228  11.0917270   1.7755983  28.3607791   2.9678911   1.7546545\n#> [12]   8.0910030   8.0910030   9.8456576  12.8135486  41.1743278  25.4606915   0.5546030  16.9346316  16.9346316  17.4892346  18.4363079\n#> [23]   4.6581580  13.5498048  17.3960309  17.3960309  30.9471871  12.4567724  23.1472214  23.1472214  17.4065182  24.1723764  12.4712971\n#> [34]   2.4600303  12.6082491  12.5529673  12.6146793  15.2153841  27.6866812  51.8590576  50.4721698   1.0975337  28.3691666  28.3691666\n#> [45]  29.4667004   0.4805624   1.9967144   2.7687118  17.0272986  23.4955884  20.5338341  17.3003772  17.3003772  22.2636579  15.5705534\n#> [56]  15.5705534  61.3297997  50.6621138   5.3479249  22.3470597  22.3470597  27.6949846  66.5513507  14.5744595  14.5744595  67.7448719\n#> [67]  15.3776527  15.3776527  85.3364736 104.2034448 112.5837095\n## Remover espécies da filogenia\n# Vamos criar um novo nome para o objeto e excluir as espécies Leucopternis polionotus e Aramides saracura da filogenia\nfilogenia_cortada <- drop.tip(minha_arvore, c(\"Leucopternis_polionotus\", \"Aramides_saracura\"))\nfilogenia_cortada\n#> \n#> Phylogenetic tree with 35 tips and 33 internal nodes.\n#> \n#> Tip labels:\n#>   Cranioleuca_pallida, Synallaxis_ruficapilla, Phacellodomus_ferrugineigula, Cinclodes_pabsti, Conopophaga_melanops, Herpsilochmus_pileatus, ...\n#> \n#> Rooted; includes branch lengths.\n## Adicionar espécies à filogenia\n# Vamos inserir as espécies Megascops_sp1, Carponis_sp, Strix_sp1, Strix_sp2 e\n# Strix_sp3 na filogenia\nMegascops <- c(\"Megascops_sp1\")\nCarpornis <- c(\"Carpornis_sp1\")\nStrix <- c(\"Strix_sp1\", \"Strix_sp2\", \"Strix_sp3\")\n\n# Inserindo espécies como politomias\nfilogenia_nova <- add.species.to.genus(force.ultrametric(minha_arvore, message = FALSE), Megascops)\nfilogenia_nova <- add.species.to.genus(force.ultrametric(filogenia_nova, message = FALSE), Carpornis)\n## Adicionar várias espécies à filogenia\n# Para inserir mais de uma espécie dentro do gênero, vamos utilizar um loop.  \nfor(i in 1:length(Strix)) \n    filogenia_nova <- add.species.to.genus(force.ultrametric(filogenia_nova, message = FALSE),\n                                           Strix[i], where = \"root\")\n## Gráfico\nplot(filogenia_nova, cex = 0.5, no.margin = TRUE)\n## phylo.maker\n# A função phylo.maker usa uma filogenia default de plantas (i.e. GBOTB.extended).\n# Caso você queira utilizar outra filogenia, é só alterar o argumento tree\nnovas_filogenias <- phylo.maker(especies_plantas,\n                                tree = GBOTB.extended,\n                                scenarios = c(\"S1\",\"S2\",\"S3\"))\n#> [1] \"Note: 2 taxa fail to be binded to the tree,\"\n#> [1] \"Genus7_sp1\" \"Genus8_sp1\"\n## Gráfico\npar(mfrow = c(1, 2))\nplot.phylo(novas_filogenias$scenario.1, cex = 0.5, main = \"Cenário 1\")\nplot.phylo(novas_filogenias$scenario.3, cex = 0.5, main = \"Cenário 3\")\ndev.off()\n#> null device \n#>           1"},{"path":"cap13.html","id":"métricas-de-diversidade-alfa-filogenética","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.3 Métricas de diversidade alfa filogenética","text":"Métricas de diversidade alfa utilizam os dados de incidência (presença e ausência) ou abundância das espécies para determinar um valor de diversidade para cada comunidade ou sítio de interesse.Exemplo prático 1Explicação dos dadosAvaliaremos diversidade filogenética de 10 comunidades de aves amostradas ao longo de um gradiente de precipitação. Utilizaremos este conjunto de dados para todos os exemplos deste capítulo.PerguntaA variação na distribuição espacial dos valores de diversidade filogenética das comunidades está associada com o gradiente de precipitação?PrediçõesOs valores de diversidade filogenética serão maiores nas comunidades localizadas em regiões com altas precipitações que em regiões mais secasVariáveisData frame com comunidades (unidade amostral) nas linhas e espécies de aves nas colunas (variável resposta)Data frame com comunidades (unidade amostral) nas linhas e espécies de aves nas colunas (variável resposta)Data frame com comunidades (unidade amostral) nas linhas e variável precipitação anual na coluna (variável preditora)Data frame com comunidades (unidade amostral) nas linhas e variável precipitação anual na coluna (variável preditora)Arquivo com filogenia das 37 espécies de aves (variável resposta)Arquivo com filogenia das 37 espécies de aves (variável resposta)ChecklistVerificar se os data frames de composição de espécies e variáveis ambientais estão com unidades amostrais nas linhas e variáveis preditoras nas colunasVerificar se os data frames de composição de espécies e variáveis ambientais estão com unidades amostrais nas linhas e variáveis preditoras nas colunasVerificar se comunidades nos data frames de composição de espécies e variáveis ambientais estão distribuídos na mesma sequência/ordem nos dois arquivos.Verificar se comunidades nos data frames de composição de espécies e variáveis ambientais estão distribuídos na mesma sequência/ordem nos dois arquivos.Verificar se o nome das espécies de aves data frame de composição de espécies é idêntico ao nome das espécies na filogenia.Verificar se o nome das espécies de aves data frame de composição de espécies é idêntico ao nome das espécies na filogenia.","code":""},{"path":"cap13.html","id":"riqueza-da-diversidade-alfa-filogenética","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.3.1 Riqueza da diversidade alfa filogenética","text":"métricas de riqueza somam quantidade da diferença filogenética presente em uma comunidade (Tucker et al. 2017).Phylogenetic diversity (PD)Esta métrica é definida pela soma comprimento dos ramos conectando todas espécies na comunidade. É métrica mais conhecida e usada nos estudos de conservação e comunidade (Faith 1992).Vamos conferir se os nomes das espécies de aves data frame de composição são os mesmos da filogenia. O resultado OK indica que os nomes estão corretos. Caso contrário, você deve verificar e arrumar.Agora vamos colocar os nomes das espécies data frame na mesma ordem que os nomes aparecem na filogenia. Isso é obrigatório para algumas funções.Abaixo, demonstramos os códigos R para o cálculo de PD para comunidades de aves.comunidade 2 abriga maior diversidade filogenética com composição de espécies contemplando 1293,15 milhões de anos (.e. soma comprimento dos ramos ligando todas espécies da comunidade). Por outro lado, comunidade 10 abriga menor diversidade filogenética contemplando 599,69 milhões de anos. 📝 Importante \nEste índice é correlacionado com riqueza de espécies. Discutiremos essa questão na seção de modelos nulos.Phylogenetic Species Richness (PSR)Esta métrica é calculada multiplicando riqueza de espécies registrada na comunidade pela Phylogenetic Species Variability (PSV) da comunidade (Helmus et al. 2007). PSR é diretamente comparável ao número de espécies na comunidade, mas inclui o parentesco filogenético entre espécies.Abaixo, demonstramos os códigos R para o cálculo PSR utilizando os dados das comunidades de aves.comunidade 2 abriga o maior valor de PSR enquanto comunidade 10 abriga o menor valor. Vejam que PSR é fortemente correlacionado com número de espécies nas comunidades (r = 0.99, p <0.0001). Contudo, existe uma variabilidade residual PSR em relação ao número de espécies que afeta o ranqueamento das comunidades quando utilizando PSR ou número de espécies. Consequentemente, escolha da métrica pode gerar diferentes delineamentos de áreas prioritárias para conservação (Helmus et al. 2007).Phylogenetic Endemism (PE)Esta métrica calcula fração dos ramos restritas regiões específicas. PE identifica áreas ou comunidades que abrigam componentes restritos da diversidade filogenética. PE é uma métrica proposta para auxiliar estudos de conservação estabelecendo critérios para priorizar regiões serem conservadas com base na importância evolutiva (.e. partes da filogênia com distribuição espacial limitada) das espécies que ocorrem nestes locais (Rosauer et al. 2009).Abaixo, demonstramos os códigos R para o cálculo PE utilizando os dados das comunidades de aves.O índice PE considera 10 comunidades como o range espacial máximo. Se todas espécies ocorressem nas 10 comunidades, o valor de PE seria 1, indicando baixo endemismo filogenético. comunidade 2 abriga um conjunto de espécies cujo os ramos com distribuição espacial restrita contemplam 272,6 milhões de anos. Por outro lado, comunidade 10 abriga um conjunto de espécies cujo os ramos com distribuição espacial restrita contemplam 68,5 milhões de anos. Assim, comunidades 1, 2 e 4 são áreas que abrigam os maiores endemismo filogenéticos.Species Evolutionary Distinctiveness (ED)Esta métrica calcula qual é fração da árvore filogenética que é atribuída para uma espécie. ED reflete quão evolutivamente isolada uma espécie é comparada com outras espécies na filogenia (Redding Mooers 2006). ED é uma métrica proposta para auxiliar estudos de conservação estabelecendo critérios para priorizar espécies serem conservadas com base na sua importância evolutiva (exclusividade comprimento ramo) que não é compartilhada com outras espécies. Portanto, apenas informações da filogenia são utilizadas para o cálculo de ED.Abaixo, demonstramos os códigos R para o cálculo ED utilizando os dados das comunidades de aves.Com base na filogenia estudada, Tinamus solitarius (112,58 milhões de anos), Ortalis guttata (108,39 m..) e Aramides saracura (96,86 m..) são espécies com maior distinção evolutiva devido elevada fração dos ramos não compartilhado com outras espécies.","code":"\n## Conferir os nomes das espécies\nname.check(filogenia_aves, t(composicao_especies))\n#> [1] \"OK\"\n## Colocar os nomes das espécies do data frame na mesma ordem que aparecem na filogenia\ncomposicao_especies_P <- match.phylo.comm(phy = filogenia_aves, comm = composicao_especies)$comm\n## Phylogenetic diversity (PD)\n# Calculando a métrica de diversidade filogenética proposta por Faith (1992).\nresultados_PD <- pd(composicao_especies_P, filogenia_aves)\n\n# Mostra o valor de PD e riqueza de espécies para cada comunidade.\nresultados_PD\n#>               PD SR\n#> Com_1  1259.3151 27\n#> Com_2  1293.1521 26\n#> Com_3  1222.3102 25\n#> Com_4  1254.5410 25\n#> Com_5  1021.9670 22\n#> Com_6   856.7810 18\n#> Com_7   930.6452 15\n#> Com_8   678.9394 12\n#> Com_9   673.6288 13\n#> Com_10  599.6924  9\n## Phylogenetic Species Richness (PSR)\n# Análise com dados de composição das espécies nas comunidades.\nresultados_PSR <- psr(composicao_especies_P,filogenia_aves)\n\n# Mostra os valores de PSR para cada comunidade.\nresultados_PSR \n#>              PSR SR       vars\n#> Com_1  18.084236 27 0.04537904\n#> Com_2  18.167183 26 0.04881734\n#> Com_3  16.230938 25 0.05205832\n#> Com_4  17.153972 25 0.05205832\n#> Com_5  13.981597 22 0.06060866\n#> Com_6  11.287030 18 0.06933707\n#> Com_7  10.279983 15 0.07398666\n#> Com_8   7.538134 12 0.07721118\n#> Com_9   8.060933 13 0.07627517\n#> Com_10  5.720063  9 0.07948474\n## Phylogenetic Endemism (PE)\n# Transformando data.frame em matriz.\ndados_matriz <- as.matrix(composicao_especies_P)\n\n# Análise.\nresultados_PE <- phylo_endemism(dados_matriz, filogenia_aves, \n                                weighted = TRUE)\n\n# Mostra os valores de PE para cada comunidade.\nresultados_PE \n#>     Com_1     Com_2     Com_3     Com_4     Com_5     Com_6     Com_7     Com_8     Com_9    Com_10 \n#> 232.09145 272.60106 210.22647 218.89037 146.99281 135.06423 148.65234  79.22402  77.95458  68.50266\n## Species Evolutionary Distinctiveness (ED)\n# Análise.\nresultados_ED <- evol.distinct(filogenia_aves)\n\n# Mostra os valores de ED para cada espécie.\nhead(resultados_ED)\n#>                        Species        w\n#> 1          Cranioleuca_pallida 14.07447\n#> 2       Synallaxis_ruficapilla 14.07447\n#> 3 Phacellodomus_ferrugineigula 20.05793\n#> 4             Cinclodes_pabsti 30.27020\n#> 5         Conopophaga_melanops 47.72685\n#> 6       Herpsilochmus_pileatus 26.40947"},{"path":"cap13.html","id":"divergência-da-diversidade-alfa-filogenética","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.3.2 Divergência da diversidade alfa filogenética","text":"métricas de divergência utilizam média da distribuição das unidades extraídas da árvore filogenética (Tucker et al. 2017).Mean Pairwise Distance (MPD)Esta métrica utiliza matriz de distância filogenética para quantificar distância média parentesco entre pares de espécies em uma comunidade. Este índice pode ser calculado considerando dados de incidência ou considerando dados de abundância das espécies. Importante, o MPD é uma métrica que pesa estrutura interna da filogenia (e.g., relações entre espécies de famílias diferentes) (Webb et al. 2002).Abaixo, demonstramos os códigos R para o cálculo MPD utilizando os dados das comunidades de aves.Vamos iniciar com dados de incidência (presença e ausência) das espécies nas comunidades. função cophenetic() gera uma matriz com distâncias par par entre espécies. Essas distâncias são utilizadas para computar distância média parentesco das espécies dentro das comunidades.comunidade 9 abriga composição de espécies mais aparentada (.e. menor diversidade filogenética) com distância média entre espécies de 139,62 milhões de anos. Por outro lado, comunidade 2 abriga composição de espécies menos aparentada (.e. maior diversidade filogenética) com distância média de 157,31 milhões anos.Vamos refazer análise MPD, mas desta vez, considerando abundância das espécies de aves nas comunidades. Para isso, alteramos o argumento abundance.weighted = TRUE.Percebam que pesando o comprimento ramo pela abundância das espécies altera-se os valores índice de diversidade filogenética. Neste caso, comunidade 10 passa ser comunidade que abriga composição de espécies mais aparentada (.e. menor diversidade filogenética) com distância média entre espécies de 117,88 milhões de anos.Mean Nearest Taxon Distance (MNTD)Esta métrica utiliza matriz de distância filogenética para quantificar média dos valores mínimos de parentesco entre pares de espécies em uma comunidade. Ou seja, qual o valor médio da distância para o vizinho mais próximo. Este índice pode ser calculado considerando dados de incidência (presença e ausência) ou considerando dados de abundância das espécies. Diferente MPD, o MNTD é uma métrica terminal que pesa relações nas pontas da filogenia (e.g. espécies dentro mesmo gênero) (Webb et al. 2002).Abaixo, demonstramos os códigos R para o cálculo MNTD utilizando os dados das comunidades de aves.comunidade 9 abriga composição de espécies com distância média vizinho mais próximo de 62,34 milhões de anos. Esse resultado indica que espécies terminais são mais aparentadas (e.g. espécies mesmo gênero) que composição de espécies da comunidade 10 onde distância média vizinho mais próximo é 112,23 milhões de anos (e.g. espécies de gêneros diferentes).Vamos refazer análise MNTD, mas desta vez, considerando abundância das espécies de aves nas comunidades.Como nos resultados MPD, pesar o comprimento ramo pela abundância das espécies altera os valores MNTD. Neste caso, ao invés da comunidade 9, comunidade 2 passa ser comunidade que abriga composição de espécies com menor distância média vizinho mais próximo (53,02 milhões de anos). 📝 Importante \nPerceba que ao determinar análises com base na incidência ou abundância das espécies, você pode também alterar interpretação dos padrões encontrados.Phylogenetic Species Variability (PSV)Esta métrica estima quantidade relativa dos comprimentos dos ramos não compartilhados entre comunidades. Quando todas espécies em uma amostra não são aparentadas (.e. filogenia em estrela), o valor PSV é 1 (um), indicando máxima variabilidade. Quando espécies tornam-se mais aparentadas, o valor de PSV aproxima-se de 0 (zero), indicando reduzida variabilidade. Os valores esperados de PSV são estatisticamente independentes da riqueza de espécies (Helmus et al. 2007). 📝 Importante \nOs valores de PSV são idênticos ao MPD quando filogenia é ultramétrica.Abaixo, demonstramos os códigos R para o cálculo PSV utilizando os dados das comunidades de aves.comunidade 2 abriga maior variabilidade filogenética (0,69) enquanto comunidade 9 abriga menor variabilidade (0,62). Perceba que os valores de PSV não são correlacionados com número de espécies nas comunidades (r = 0,59, p = 0,07).","code":"\n## Mean Pairwise Distance (MPD)\n# Análise com dados de incidência das espécies nas comunidades.\nresultados_MPD_PA <- mpd(composicao_especies_P, cophenetic(filogenia_aves), \n                         abundance.weighted = FALSE)\n\n# Mostra os valores de MPD para cada comunidade.\nresultados_MPD_PA \n#>  [1] 150.7914 157.3158 146.1622 154.5005 143.0727 141.1926 154.3145 141.4292 139.6198 143.0862\n## Mean Pairwise Distance (MPD)\n# Análise com dados de abundância das espécies nas comunidades.\nresultados_MPD_AB <- mpd(composicao_especies_P, cophenetic(filogenia_aves), \n                         abundance.weighted = TRUE)\n\n# Mostra os valores de MPD para cada comunidade.\nresultados_MPD_AB \n#>  [1] 135.0704 143.3156 129.1940 142.8127 131.4027 128.7733 134.0380 132.6389 133.4041 117.8787\n## Mean Nearest Taxon Distance (MNTD)\n# Análise com dados de presença e ausência das espécies nas comunidades.\nresultados_MNTD_PA <- mntd(composicao_especies_P, cophenetic(filogenia_aves), \n                           abundance.weighted = FALSE)\n\n# Mostra os valores de MPD para cada comunidade.\nresultados_MNTD_PA \n#>  [1]  63.89727  66.15828  72.96912  67.67170  64.93477  63.72337  93.54980  78.24876  62.34565 112.23127\n# Análise com dados de abundância das espécies nas comunidades.\nresultados_MNTD_AB <- mntd(composicao_especies_P, cophenetic(filogenia_aves), \n                           abundance.weighted = TRUE)\n\n# Mostra os valores de MPD para cada comunidade.\nresultados_MNTD_AB \n#>  [1]  57.11745  53.02212  70.47864  59.12049  61.23225  60.26180 110.13043  97.35404  82.12099 127.70084\n## Phylogenetic Species Variability (PSV)\n# Análise com dados de presença e ausência das espécies nas comunidades.\nresultados_PSV <- psv(composicao_especies_P,filogenia_aves)\n\n# Mostra os valores de PSV para cada comunidade.\nresultados_PSV \n#>             PSVs SR         vars\n#> Com_1  0.6697865 27 6.224834e-05\n#> Com_2  0.6987378 26 7.221499e-05\n#> Com_3  0.6492375 25 8.329332e-05\n#> Com_4  0.6861589 25 8.329332e-05\n#> Com_5  0.6355271 22 1.252245e-04\n#> Com_6  0.6270572 18 2.140033e-04\n#> Com_7  0.6853322 15 3.288296e-04\n#> Com_8  0.6281778 12 5.361887e-04\n#> Com_9  0.6200717 13 4.513324e-04\n#> Com_10 0.6355626  9 9.812931e-04"},{"path":"cap13.html","id":"regularidade-da-diversidade-alfa-filogenética","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.3.3 Regularidade da diversidade alfa filogenética","text":"métricas de regularidade caracterizam variação das distâncias entre espécies em uma comunidade (Tucker et al. 2017).Variance Pairwise Distance (VPD)Esta métrica utiliza matriz de distância filogenética para quantificar variância parentesco entre pares de espécies em uma comunidade (Clarke Warwick 2001).Abaixo, demonstramos os códigos R para o cálculo VPD utilizando os dados das comunidades de aves.comunidade 2 abriga maior variância na distância filogenética entre pares de espécies dentro da comunidade (1828,19 milhões de anos) enquanto comunidade 7 abriga menor variância entre os pares de espécies (825,53 m..).","code":"\n## Variance of Pairwise Distance (VPD)\n# Transformando data frame em matriz.\ndados_matriz <- as.matrix(composicao_especies_P)\n\n# Transformar os dados para o formato requerido pelo pacote pez.\ndados <- comparative.comm(filogenia_aves, dados_matriz)\n\n# Análise.\nresultados_VPD <- .vpd(dados, cophenetic(filogenia_aves))\n\n# Mostra os valores de VPD para cada comunidade.\nresultados_VPD \n#>     Com_1    Com_10     Com_2     Com_3     Com_4     Com_5     Com_6     Com_7     Com_8     Com_9 \n#> 1619.4697 1031.8887 1828.1930 1630.4026 1317.9919 1465.1728 1519.6115  825.5349 1278.0076 1508.0495"},{"path":"cap13.html","id":"correlação-entre-as-métricas-de-diversidade-alfa-filogenética","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.3.4 Correlação entre as métricas de diversidade alfa filogenética","text":"Vamos avaliar correlação entre os valores das métricas de diversidade alfa filogenética. Vamos criar um data frame com os resultados das métricas separados para dimensões de riqueza e divergência. Não iremos fazer para regularidade, pois só apresentamos uma métrica de diversidade filogenética nesta dimensão (Figura 13.6).\nFigura 13.6: Correlação entre métricas de riqueza da diversidade alfa filogenética.\nPercebam que três métricas apresentam correlações pareadas acima de 94%. Isso indica que métricas são redundantes. Portanto, não há necessidade de calcular mais de uma métrica dentro da dimensão da riqueza filogenética. Além disso, três métricas de diversidade alfa filogenética também apresentam alta correlação com riqueza de espécies. Veja abaixo na seção de modelos nulos como controlar o efeito da riqueza de espécies nas métricas de diversidade filogenética.Vamos avaliar correlação entre os valores das métricas de diversidade alfa filogenética para dimensão divergência (Figura 13.7).\nFigura 13.7: Correlação entre métricas de divergência da diversidade alfa filogenética.\nComo mencionado, métricas MPD e PSV são idênticas quando usamos uma filogenia ultramétrica. Contudo, métricas de divergência não apresentam correlações tão altas como métricas da dimensão riqueza, com exceção MNTD usando dados de incidência e abundância que foram fortemente correlacionados (r = 0,9). Além disso, estas métricas não são tão afetadas pela riqueza de espécies das comunidades como métricas da dimensão riqueza.","code":"\n## Data frame\n# Vamos criar um data.frame com os resultados das métricas da dimensão riqueza.\nmetricas_riqueza <- data.frame(riqueza = resultados_PD$SR,\n                               PD = resultados_PD$PD,\n                               PSR = resultados_PSR$PSR,\n                               PE = resultados_PE)\n\n## Gráfico\n# Gráfico mostrando na parte:\n# i) inferior a distribuição dos pontos considerando as métricas pareadas\n# ii) superior o valor da correlação de pearson\n# iii) diagonal a curva de densidade\nggpairs(metricas_riqueza, upper = list(continuous = wrap(\"cor\", size = 4))) +\n    tema_livro()\n## Data frame\n# Vamos criar um data.frame com os resultados das métricas da dimensão divergência.\nmetricas_divergencia <- data.frame(riqueza = resultados_PD$SR,\n                                   MPD = resultados_MPD_PA,\n                                   MPD_AB = resultados_MPD_AB,\n                                   MNTD = resultados_MNTD_PA,\n                                   MNTD_AB = resultados_MNTD_AB,\n                                   PSV = resultados_PSV$PSVs)\n\n## Gráfico\nggpairs(metricas_divergencia, upper = list(continuous = wrap(\"cor\", size = 4))) +\n    tema_livro()"},{"path":"cap13.html","id":"associação-entre-a-diversidade-alfa-filogenética-e-o-ambiente","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.3.5 Associação entre a diversidade alfa filogenética e o ambiente","text":"Vamos avaliar e plotar relação entre os valores de algumas métrica de diversidade alfa filogenética (variável resposta) e os valores de precipitação (variável preditora) (Figura 13.8).\nFigura 13.8: Relação de algumas métrica de diversidade alfa filogenética e valores de precipitação.\nO MPD, que avalia relações de parentesco mais internas da filogenia (.e., relações entre espécies de famílias diferentes) não apresentou associação com o gradiente de precipitação. Por outro lado, o MNTD que avalia relações mais terminais da filogenia (.e., espécies dentro mesmo gênero) apresentou uma relação negativa com o gradiente de precipitação. Interessante que associação só foi significativa quando pesamos análise pela abundância das espécies nas comunidades. Esses resultados demonstram importância da seleção das métricas de diversidade filogenética e tipos de dados (e.g., incidência ou abundância) utilizados na interpretação dos padrões observados na natureza.Vamos ver os gráficos das métricas da dimensão riqueza da diversidade alfa filogenética (Figura 13.9).\nFigura 13.9: Relação de algumas métrica de diversidade alfa filogenética e valores de precipitação.\ntrês métricas de diversidade filogenética foram relacionadas com o gradiente de precipitação. Esse resultado indica que comunidades localizadas em áreas com maior precipitação anual abrigaram maior diversidade filogenética que comunidades localizadas em áreas mais secas. Contudo, estas métricas são dependentes da riqueza de espécies nas comunidades. Veja abaixo seção de modelos nulos para entender como lidar com essa dependência.","code":"\n## Dados\n# Vamos inserir os dados de precipitação na planilha metrica_divergencia.\nmetricas_divergencia$precipitacao <- precipitacao_filogenetica$prec\n\n## Gráficos\nMPD_PA_plot <- ggplot(metricas_divergencia, aes(precipitacao, MPD)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") + \n    labs(x = \"Precipitação (mm)\", \n         y = \"Mean Pairwise Distance\\n (MPD - Ausência e Presença)\") +\n    tema_livro()\n\nMPD_AB_plot <- ggplot(metricas_divergencia, aes(precipitacao, MPD_AB)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") + \n    labs(x = \"Precipitação (mm)\", \n         y = \"Mean Pairwise Distance\\n (MPD - Abundância)\", size = 8) +\n    tema_livro() \n\nMNTD_AP_plot <- ggplot(metricas_divergencia, aes(precipitacao, MNTD)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") + \n    labs(x = \"Precipitação (mm)\", \n         y = \"Mean Nearest Taxon Distance\\n (MNTD - Ausência e Presença)\", \n         size = 8) +\n    tema_livro() \n\nMNTD_AB_plot <- ggplot(metricas_divergencia, aes(precipitacao, MNTD_AB)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") + \n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    labs(x = \"Precipitação (mm)\", \n         y = \"Mean Nearest Taxon Distance\\n (MNTD - Abundância)\", \n         size = 8) +\n    tema_livro()\n\nggarrange(MPD_PA_plot, MPD_AB_plot, MNTD_AP_plot, MNTD_AB_plot,\n          ncol = 2, nrow = 2)\n## Dados\n# Vamos inserir os dados de precipitação na planilha metrica_riqueza.\nmetricas_riqueza$precipitacao <- precipitacao$prec\n\n## Gráficos\nRiqueza_plot <- ggplot(metricas_riqueza, aes(precipitacao, riqueza)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") +\n    geom_smooth(method = lm, se = FALSE, color = \"black\") + \n    labs(x = \"Precipitação (mm)\", y = \"Riqueza de espécies\") +\n    tema_livro() \n\nPD_plot <- ggplot(metricas_riqueza, aes(precipitacao, PD)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") + \n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    labs(x = \"Precipitação (mm)\", \n         y = \"Diversidade Filogenética\\n (Faith)\", size = 8) +\n    tema_livro()\n\nPSR_plot <- ggplot(metricas_riqueza, aes(precipitacao, PSR)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\")  +\n    geom_smooth(method = lm, se = FALSE, color = \"black\") + \n    labs(x = \"Precipitação (mm)\", \n         y = \"Phylogenetic Species Richness\\n (PSR)\", \n         size = 8) +\n    tema_livro()\n\nPE_plot <- ggplot(metricas_riqueza, aes(precipitacao, PE)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\")  +\n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    labs(x = \"Precipitação (mm)\", \n         y = \"Phylogenetic Endemism\\n (PE)\", \n         size = 8) + \n    tema_livro() \n\nggarrange(Riqueza_plot, PD_plot, PSR_plot, PE_plot, ncol = 2, nrow = 2)"},{"path":"cap13.html","id":"métricas-de-diversidade-beta-filogenética","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.4 Métricas de diversidade beta filogenética","text":"Métricas de diversidade beta filogenética utilizam dados de presença e ausência ou abundância das espécies para determinar um valor que representa diferença entre comunidades em relação história evolutiva das linhagens (para detalhes de diversidade beta, consulte o Capítulo 12).","code":""},{"path":"cap13.html","id":"divergência-da-diversidade-beta-filogenética","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.4.1 Divergência da diversidade beta filogenética","text":"Community Mean Pairwise Distance (COMDIST)Esta métrica é uma extensão MPD. COMDIST calcula média da distância filogenética entre espécies de duas comunidades (Webb, Ackerly, Kembel 2008). COMDIST pode ser calculada usando dados de incidência (presença e ausência) ou abundância das espécies. Esta extensão MPD também é conhecida na literatura como Dpw (Swenson 2011, 2014).Abaixo, demonstramos os códigos R para o cálculo COMDIST utilizando os dados das comunidades de aves.comunidades 8 e 10 apresentaram menor média na distância filogenética (130,06 m.. - espécies de linhagens mais próximas) entre espécies presente em cada comunidade, enquanto comunidades 2 e 4 apresentaram maior média na distância filogenética (153.29 m.. - espécies de linhagens mais distintas).Vamos refazer análise COMDIST, mas desta vez, considerando abundância das espécies de aves nas comunidades.Como caso MPD, pensar abundância das espécies altera o padrão de distribuição dos valores de COMDIST. Neste caso, ao invés das comunidades 2 e 4, comunidades 2 e 10 apresentam maior média na distância filogenética (155,85 m..).Community Mean Nearest Taxon Distance (COMDISTNT)Esta métrica é uma extensão MNTD. COMDISTNT calcula média da distância filogenética entre o táxon mais próximo das espécies de duas comunidades (Webb, Ackerly, Kembel 2008). COMDISTNT pode ser calculada usando dados de incidência ou abundância das espécies. Esta extensão MNTD também é conhecida na literatura como Dnn (Swenson 2011).Abaixo, demonstramos os códigos R para o cálculo COMDISTNT utilizando os dados das comunidades de aves.comunidades 8 e 9 apresentaram menor média na distância vizinho mais próximo (10,69 m.. - espécies mesmo gênero ou gêneros irmãos) entre espécies presente em cada comunidade, enquanto comunidades 7 e 10 apresentaram maior média na distância entre vizinhos (60,24 m.. - espécies de linhagens distintas).Vamos refazer análise COMDISTNT, mas desta vez, considerando abundância das espécies de aves nas comunidades.comunidades 8 e 10 apresentaram menor média na distância vizinho mais próximo (5,64 m..) entre espécies presente em cada comunidade, enquanto comunidades 6 e 10 apresentaram maior média na distância entre vizinhos (82,62 m..).","code":"\n## Community Mean Pairwise Distance (COMDIST)\n# Análise com dados de presença e ausência das espécies nas comunidades.\nresultados_Comdist_PA <- comdist(composicao_especies_P, \n                                 cophenetic(filogenia_aves), \n                                 abundance.weighted = FALSE)\nresultados_Comdist_PA\n#>           Com_1    Com_2    Com_3    Com_4    Com_5    Com_6    Com_7    Com_8    Com_9\n#> Com_2  150.5242                                                                        \n#> Com_3  144.3300 148.7436                                                               \n#> Com_4  149.4782 153.2992 147.5838                                                      \n#> Com_5  141.8435 146.9829 139.9606 145.8746                                             \n#> Com_6  142.6019 148.3160 140.0376 143.1340 137.9527                                    \n#> Com_7  147.7189 150.2248 147.1703 150.3418 144.1481 145.2975                           \n#> Com_8  141.3083 145.6684 138.5875 145.3897 137.0034 137.7062 144.4818                  \n#> Com_9  141.6018 146.4697 138.3515 144.3480 136.9036 136.3453 144.3257 130.7628         \n#> Com_10 140.8810 145.9333 136.9978 145.1400 136.5394 137.2350 144.3660 130.0691 130.2321\n## Community Mean Pairwise Distance (COMDIST)\n# Análise com dados de abundância das espécies nas comunidades.\nresultados_Comdist_AB <- comdist(composicao_especies_P, \n                                 cophenetic(filogenia_aves), \n                                 abundance.weighted = TRUE)\n## Community Mean Nearest Taxon Distance (COMDISTNT)\n# Análise com dados de presença e ausência das espécies nas comunidades.\nresultados_Comdistnt_PA <- comdistnt(composicao_especies_P, \n                                     cophenetic(filogenia_aves), \n                                     abundance.weighted = FALSE)\nresultados_Comdistnt_PA\n#>           Com_1    Com_2    Com_3    Com_4    Com_5    Com_6    Com_7    Com_8    Com_9\n#> Com_2  24.65946                                                                        \n#> Com_3  18.56953 22.22310                                                               \n#> Com_4  26.85806 24.04505 17.87073                                                      \n#> Com_5  13.15074 34.14223 18.82684 25.54000                                             \n#> Com_6  35.43332 49.33726 33.82511 24.59980 28.50505                                    \n#> Com_7  30.37968 29.22411 39.89649 34.06858 29.27588 47.69536                           \n#> Com_8  38.16628 48.17376 38.46830 51.02029 32.52846 50.37593 54.37647                  \n#> Com_9  42.58727 54.09441 41.83792 46.29635 36.35245 46.18264 49.85495 10.69759         \n#> Com_10 45.71452 56.82713 40.39642 60.17935 40.74857 59.63770 60.24556 10.87162 16.12367\n## Community Mean Nearest Taxon Distance (COMDISTNT)\n# Análise com dados de abundância das espécies nas comunidades.\nresultados_Comdistnt_AB <- comdistnt(composicao_especies_P, \n                                     cophenetic(filogenia_aves), \n                                     abundance.weighted = TRUE)"},{"path":"cap13.html","id":"correlação-entre-as-métricas-de-diversidade-beta-filogenética","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.4.2 Correlação entre as métricas de diversidade beta filogenética","text":"Vamos avaliar correlação entre os valores das métricas da diversidade beta filogenética para dimensão divergência (Figura 13.10).\nFigura 13.10: Correlação entre métricas de divergência da diversidade beta filogenética.\nOs valores das métricas de divergência filogenética beta apresentam correlações mais baixas que métricas da dimensão riqueza. Lembrem-se que COMDIST e COMDISTNT dão pesos diferentes para relações de parentesco. COMDIST pesa relações mais basais e internas da filogenia, enquanto COMDISTNT pesa relações nas partes terminais da filogenia. Portanto, elas podem trazer informações complementares.","code":"\n## Dados\n# Vamos criar um data frame com os resultados das métricas da dimensão divergência.\nmetricas_divergencia_beta <- data.frame(\n    COMDIST_PA = as.numeric(resultados_Comdist_PA),\n    COMDIST_AB = as.numeric(resultados_Comdist_AB),\n    COMDISTNT_PA = as.numeric(resultados_Comdistnt_PA),\n    COMDISTNT_AB = as.numeric(resultados_Comdistnt_AB))\n\n## Gráfico\nggpairs(metricas_divergencia_beta,\n        upper = list(continuous = wrap(\"cor\", size = 4))) +\n    tema_livro()"},{"path":"cap13.html","id":"associação-entre-a-divergência-da-diversidade-beta-filogenética-e-o-ambiente","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.4.3 Associação entre a divergência da diversidade beta filogenética e o ambiente","text":"Vamos avaliar e plotar relação entre os valores de algumas métricas de divergência da diversidade beta filogenética (variável resposta) e os valores de precipitação (variável preditora) (Figura 13.11).\nFigura 13.11: Relação de algumas métrica de divergência diversidade beta filogenética e valores de precipitação.\nO COMDIST que avalia relações de parentesco mais internas da filogenia (.e., relações entre espécies de famílias diferentes) apresentou associação com o gradiente de precipitação quando avaliado pesado pela abundância das espécies. Por outro lado, o COMDISTNT que avalia relações mais terminais da filogenia (.e., espécies dentro mesmo gênero) apresentou uma relação negativa com o gradiente de precipitação quando avaliado usando incidência das espécies.","code":"\n## Dados\n# Precisamos calcular a dissimilaridade par a par da precipitação entre as comunidades.\ndis_prec <- vegdist(precipitacao, \"euclidian\")\n\n# Vamos inserir estes dados na planilha metrica_divergencia_beta.\nmetricas_divergencia_beta$dis_prec <- as.numeric(dis_prec)\n\n# Gráficos.\nCOMDIST_PA_plot <- ggplot(metricas_divergencia_beta, \n                          aes(dis_prec, COMDIST_PA)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") +\n    labs(x = \"Diferença na precipitação (mm)\", \n         y = \"COMDIST\\n (Presença e Ausência)\") + \n    tema_livro() \n\nCOMDIST_AB_plot <- ggplot(metricas_divergencia_beta, \n                          aes(dis_prec, COMDIST_AB)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") +\n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    labs(x = \"Diferença na precipitação (mm)\", \n         y = \"COMDIST\\n (Abundância)\", size = 8) +\n    tema_livro()\n\nCOMDISTNT_PA_plot <- ggplot(metricas_divergencia_beta, \n                            aes(dis_prec, COMDISTNT_PA)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") +\n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    labs(x = \"Diferença na precipitação (mm)\", \n         y = \"COMDISTNT\\n (Ausência e Presença)\", \n         size = 8) + \n    tema_livro()\n\nCOMDISTNT_AB_plot <- ggplot(metricas_divergencia_beta, \n                            aes(dis_prec, COMDISTNT_AB)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") +\n    labs(x = \"Diferença na precipitação (mm)\", \n         y = \" COMDISTNT\\n (Abundância)\", \n         size = 8) +\n    tema_livro() \n\nggarrange(COMDIST_PA_plot, COMDIST_AB_plot, COMDISTNT_PA_plot, \n          COMDISTNT_AB_plot, ncol = 2, nrow = 2)"},{"path":"cap13.html","id":"riqueza-da-diversidade-beta-filogenética","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.4.4 Riqueza da diversidade beta filogenética","text":"Phylogenetic index beta diversity (Phylosor)Phylosor é uma métrica de similaridade e determina o comprimento total dos ramos da filogenia que é compartilhado entre pares de comunidades (Bryant et al. 2008).Abaixo, demonstramos os códigos R para o cálculo Phylosor utilizando os dados das comunidades de aves.espécies presentes nas comunidades 6 e 10 compartilham menor porção comprimento dos ramos da filogenia (53% - menor similaridade entre os pares de comunidades), enquanto espécies presentes nas comunidades 8 e 10 compartilham grande parte dos comprimentos dos ramos da filogenia (91% - maior similaridade entre os pares de comunidades).Unique Fraction metric (UniFrac)UniFrac é uma métrica de dissimilaridade e determina fração única da filogenia contida em cada uma das duas comunidades (Lozupone Knight 2005).Abaixo, demonstramos os códigos R para o cálculo da UniFrac utilizando os dados das comunidades de aves.espécies presentes nas comunidades 6 e 10 apresentam menor dissimilaridade (16,4 % - maior fração única da filogenia em cada comunidade), enquanto espécies presentes nas comunidades 8 e 10 apresentam maior dissimilaridade (63,36 % - maior compartilhamento de ramos da filogenia entre os pares de comunidades).","code":"\n## Phylogenetic index of beta diversity (Phylosor)\n# Análise com dados de presença e ausência das espécies nas comunidades.\nresultados_Phylosor <- phylosor(composicao_especies_P, filogenia_aves)\n\n# Mostra uma matriz triangular com a similaridade entre a fração dos ramos \n# compartilahdos entre duas comunidades\nresultados_Phylosor \n#>            Com_1     Com_2     Com_3     Com_4     Com_5     Com_6     Com_7     Com_8     Com_9\n#> Com_2  0.7856828                                                                                \n#> Com_3  0.8052839 0.7794964                                                                      \n#> Com_4  0.7831520 0.8066793 0.8595462                                                            \n#> Com_5  0.8586780 0.6919478 0.8083230 0.7930266                                                  \n#> Com_6  0.6717414 0.5827551 0.6977734 0.7494945 0.7383098                                        \n#> Com_7  0.7414284 0.7325727 0.6836231 0.7289449 0.7384561 0.6425717                              \n#> Com_8  0.6826177 0.6283918 0.6443193 0.6404373 0.7123197 0.5928097 0.6146854                    \n#> Com_9  0.6789983 0.6074676 0.6405220 0.6727656 0.7082867 0.6169977 0.6506880 0.9016007          \n#> Com_10 0.6264594 0.5709671 0.6422800 0.5823317 0.6493935 0.5362575 0.6015983 0.9106658 0.8607104\n## Unique Fraction metric (UniFrac)\n# Análise com dados de presença e ausência das espécies nas comunidades.\nresultados_UniFrac <- unifrac(composicao_especies_P, filogenia_aves)"},{"path":"cap13.html","id":"correlação-entre-phylosor-e-unifrac","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.4.5 Correlação entre Phylosor e Unifrac","text":"Vamos calcula correlação entre duas métricas de Riqueza da diversidade beta filogenética: Phylosor e Unifrac (Figura 13.12).\nFigura 13.12: Correlação entre métricas de riqueza da diversidade beta filogenética.\n 📝 Importante Os valores de Phylosor e UniFrac apresenta 99% de correlação entre eles. Portanto, essas duas métricas identificam padrões idênticos e não devem ser utilizadas simultaneamente.","code":"\n## Dados\n# Vamos criar um data.frame com os resultados das métricas separados \n# para as dimensões de riqueza e divergência.\nmetricas_riqueza_beta <- data.frame(Phylosor = as.numeric(resultados_Phylosor),\n                                    UniFrac = as.numeric(resultados_UniFrac))\n\n## Gráfico\nggpairs(metricas_riqueza_beta, upper=list(continuous = wrap(\"cor\", size = 4))) +\n    tema_livro()"},{"path":"cap13.html","id":"associação-entre-a-riqueza-da-diversidade-beta-filogenética-e-o-ambiente","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.4.6 Associação entre a riqueza da diversidade beta filogenética e o ambiente","text":"Vamos avaliar e plotar relação entre os valores de algumas métricas de riqueza da diversidade beta filogenética (variável resposta) e os valores de precipitação (variável preditora) (Figura 13.13).\nFigura 13.13: Relação de algumas métrica de riqeuza diversidade beta filogenética e valores de precipitação.\nPhylosor (similaridade) e UniFrac (dissimilaridade) foram relacionadas com o gradiente de precipitação. Comunidades com quantidade de precipitação parecidas abrigaram linhagens similares enquanto comunidades que recebem quantidade de precipitação diferentes abrigam linhagens mais distintas.","code":"\n## Dados\n# Vamos inserir os dados de precipitação na planilha metrica_riqueza_beta.\nmetricas_riqueza_beta$dis_prec <- as.numeric(dis_prec)\n\n## Gráficos\n# Phylosor.\nplot_phylosor <- ggplot(metricas_riqueza_beta, aes(dis_prec, Phylosor)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") + \n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    scale_y_continuous(limits = c(0, 1.0)) +\n    labs(x = \"Diferença na precipitação (mm)\", \n         y = \"Phylosor\", size = 8) +\n    tema_livro()\n\n# Unifrag.\nplot_unifrac <- ggplot(metricas_riqueza_beta, aes(dis_prec, UniFrac)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") + \n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    scale_y_continuous(limits = c(0, 1.0)) +\n    labs(x = \"Diferença na precipitação (mm)\", \n         y = \"UniFrac\", size = 8) +\n    tema_livro()\n\nggarrange(plot_phylosor, plot_unifrac, ncol = 2)"},{"path":"cap13.html","id":"partição-da-diversidade-beta-filogenética","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.4.7 Partição da diversidade beta filogenética","text":"métricas, Phylosor e UniFrac, podem ser particionadas em dois componentes (Baselga 2010; Leprieur et al. 2012): ) substituição (inglês turnover) de espécies entre comunidades; e ii) componente de aninhamento (inglês nestedness) que representa perda ou ganho de espécies entre comunidades atribuídos diferença na riqueza de espécies. partição da diversidade beta nestes componentes permite avaliar diferentes hipóteses sobre os processos e mecanismos atuando na montagem de comunidades.Abaixo, demonstramos os códigos R para o cálculo da partição da diversidade beta filogenética utilizando os dados das comunidades de aves.Vamos refazer análise para UniFrac.Gráfico com os resultados dos componentes substituição e aninhamento da diversidade beta filogenética - Phylosor (Figura 13.14).\nFigura 13.14: Relação dos componentes substituição e aninhamento da diversidade beta filogenética - Phylosor e valores de precipitação.\nPercebam que o componente substituição é maior entre comunidades que apresentam diferenças altas na quantidade de precipitação, enquanto o componente aninhamento é maior entre comunidades que apresentam quantidade similar de precipitação.","code":"\n## Partição\n# Temos que transformar os dados para presença e ausência das espécies nas comunidades.\ndados_PA <- decostand(composicao_especies_P, \"pa\")\n\n# Partição dos componentes do Phylosor.\nresultados_Phylosor_particao <- phylo.beta.pair(dados_PA,\n                                                filogenia_aves, \n                                                index.family = \"sorensen\")\n# Partição dos componentes do UniFrac.\nresultados_UniFrac_particao <- phylo.beta.pair(dados_PA,\n                                               filogenia_aves, \n                                               index.family = \"jaccard\")\n# Resultado tem três matrizes: \n# i) dissimilaridade total (phylo.beta.jac); \n# ii) componente substituição de espécies (phylo.beta.jtu); e \n# iii) componente aninhamento (phylo.beta.jne).\n# resultados_UniFrac_particao (para ver os resultados corra este comando)\n## Dados\n# Vamos preparar os dados para o gráfico.\nparticao_phylosor <- data.frame(\n    substituicao = as.numeric(resultados_Phylosor_particao$phylo.beta.sim),\n    aninhamento = as.numeric(resultados_Phylosor_particao$phylo.beta.sne),\n    sorensen = as.numeric(resultados_Phylosor_particao$phylo.beta.sor),\n    dis_prec = as.numeric(dis_prec))\n\n## Gráficos\nsorensen_plot <- ggplot(particao_phylosor, \n                        aes(dis_prec, sorensen)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") + \n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    labs(x = \"\", y = \"Sorensen\") +\n    tema_livro()\n\nsubst_plot <- ggplot(particao_phylosor, \n                     aes(dis_prec, substituicao)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") + \n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    labs(x = \"Diferença na precipitação\\n (mm)\", \n         y = \"Componente Substituição\", size = 8) +\n    tema_livro()\n\naninha_plot <- ggplot(particao_phylosor, \n                      aes(dis_prec, aninhamento)) +\n    geom_point(size = 4, shape = 19, col = \"darkorange\") + \n    geom_smooth(method = lm, se = FALSE, color = \"black\") +\n    labs(x = \"\", y = \"Componente aninhamento\", size = 8) +\n    tema_livro()\n\nggarrange(sorensen_plot, subst_plot, aninha_plot, \n          ncol = 3, nrow = 1)"},{"path":"cap13.html","id":"modelos-nulos","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.5 Modelos Nulos","text":"Em muitos casos, os valores de diversidade filogenética são correlacionados com riqueza de espécies nas comunidades. Por exemplo, se um pesquisador relata que duas comunidades apresentam diferentes valores de PD, é impossível saber se esta diferença é simplesmente porque elas têm diferentes valores de riqueza de espécies ou se há algum fator fundamental sobre informação filogenética que é importante. Outra questão abordada nos estudos de montagem das comunidades é saber se os valores observados para métricas (e.g. MPD ou MNTD) relacionadas com estrutura filogenética das comunidades seriam diferentes se colonização das espécies pool regional fosse aleatória? Os modelos nulos respondem estas perguntas. Contudo, definição pool regional não é uma tarefa trivial (Lessard et al. 2012; Carstensen et al. 2013).Os modelos nulos são construídos considerando processos ecológicos ou evolutivos de interesse. Eles geram padrões que são baseados na aleatorização dos dados ecológicos ou amostragens aleatórias de uma distribuição conhecida ou hipotética (Gotelli Graves 1996). Neste caso, alguns elementos dos dados (como colunas ou linhas) são mantidos constantes, e outros são permitidos variar aleatoriamente para criar novos padrões. O principal motivo para construção de modelos nulos é produzir um padrão que seria esperado na ausência de um mecanismo ecológico específico (Gotelli Graves 1996). Contudo, ressaltamos que os modelos nulos podem revelar padrões não comuns, mas eles não podem determinar os mecanismos responsáveis por gerar estes padrões (Gotelli Graves 1996).Os modelos nulos empregados para contrapor os padrões observados pelas métricas de diversidade filogenética utilizam aleatorização dos dados de duas formas principais: ) aleatorizando o nome das espécies na árvore filogenética mantendo estrutura e composição da matriz de co-ocorrência das espécies e o comprimento dos ramos da árvore inalterados; e ii) aleatorizando linhas e/ou colunas da matriz de co-ocorrência das espécies (Gotelli 2000; Ulrich Gotelli 2010). De forma geral, nas análises de diversidade filogenética aleatorizações são repetidas 999 vezes (pode ser mais ou menos, critério pesquisador) e calcula-se média e o desvio padrão dos valores gerados pelos modelos. Com estes dados, calcula-se o tamanho efeito padronizado (inglês Standardized Effect Size - SES) utilizando seguinte fórmula:SES = (valor observado - média dos valores gerados na aleatorização)/ desvio padrão dos valores gerados na aleatorizaçãoOs valores de SES são utilizados para rejeitar ou não hipótese nula de que o padrão observado difere esperado ao acaso. Contudo, tenha em mente que definição esquema de aleatorização dos modelos nulos não é meramente uma questão técnica (Götzenberger et al. 2012). definição esquema de aleatorização irá determinar quais os mecanismos ecológicos são permitidos ou excluídos modelo nulo (Götzenberger et al. 2012). Consequentemente, ele estará avaliando diferentes hipóteses nulas.Abaixo, demonstramos os códigos R para calcular os modelos nulos para métricas de diversidade filogenética.Nearest Relative Index (NRI) ou Standardized Effect Size MPDEsta métrica calcula o tamanho efeito padronizado para métrica MPD. Contudo, NRI é calculado multiplicando os resultados SES por -1. Valores positivos de NRI indicam agrupamento filogenético e valores negativos de NRI indicam dispersão filogenética (Webb, Ackerly, Kembel 2008).Veja ajuda desta função usando ?ses.mpd() para ver todas possibilidades de modelos nulos disponíveis.Somente comunidades 5 e 6 apresentaram valores de p < 0.05 indicando que os resultados observados de MPD são menores que o esperado ao acaso (.e., valores simulados). Neste caso, composição de espécies presentes nessas duas comunidades apresenta agrupamento filogenético. Por outro lado, os valores de MPD observados para outras comunidades são similares aos valores obtidos para comunidades simuladas com redistribuição dos nomes das espécies na filogenia.Nearest Taxon Index (NTI) ou Standardized Effect Size MNTDEsta métrica calcula o tamanho efeito padronizado para métrica MNTD. Contudo, NTI é calculado multiplicando os resultados SES por -1. Valores positivos de NTI indicam agrupamento filogenético e valores negativos de NTI indicam dispersão filogenética (Webb, Ackerly, Kembel 2008).Somente comunidade 9 apresentou valor de p < 0.05 indicando que o resultado observado de MNTD foi menor que o esperado ao acaso (.e., valores simulados). Neste caso, composição de espécies presente nessa comunidade apresenta agrupamento filogenético. Por outro lado, os valores de MNTD observados para outras comunidades são similares aos valores obtidos para comunidades simuladas com redistribuição dos nomes das espécies na filogenia.Standardized Effect Size PDEsta métrica calcula o tamanho efeito padronizado para métrica PD (Webb, Ackerly, Kembel 2008).Nenhuma comunidade apresentou valor de p < 0.05. Neste caso, os valores observados de PD são similares aos valores obtidos para comunidades simuladas com redistribuição dos nomes das espécies na filogenia.Standardized effect size PhylosorNão há pacotes que calculam o SES para métrica Phylosor. Assim, iremos usar função phylosor.rnd() para criar modelos nulos para o Physolor, e em seguida, iremos usar uma função criada por Pedro Braga & Katherine Hébert (disponível em https://pedrohbraga.github.io/CommunityPhylogenetics-Workshop/CommunityPhylogenetics-Workshop.html) para calcular os valores de SES e os valores de P.Nenhum valor de similaridade entre pares comunidade apresentou valor de p < 0.05. Neste caso, os valores de Phylosor observados são similares aos valores obtidos para comunidades simuladas com redistribuição dos nomes das espécies na filogenia.","code":"\n## NRI ou SES_MPD\nresultados_SES_MPD <- ses.mpd(composicao_especies_P, cophenetic(filogenia_aves),\n                              null.model = \"taxa.labels\", \n                              abundance.weighted = FALSE,\n                              runs = 999) \n\n# Mostra a riqueza de espéices, MPD observado, média e desvio padrão dos \n# valores de MPD das aleatorizações, SES e o valor de p.\nhead(resultados_SES_MPD)\n#>       ntaxa  mpd.obs mpd.rand.mean mpd.rand.sd mpd.obs.rank  mpd.obs.z mpd.obs.p runs\n#> Com_1    27 150.7914      153.9953    3.891718          215 -0.8232597     0.215  999\n#> Com_2    26 157.3158      153.5833    4.367399          770  0.8546215     0.770  999\n#> Com_3    25 146.1622      153.9931    4.476428           52 -1.7493791     0.052  999\n#> Com_4    25 154.5005      153.5113    4.720348          551  0.2095591     0.551  999\n#> Com_5    22 143.0727      153.7490    5.294432           24 -2.0165115     0.024  999\n#> Com_6    18 141.1926      153.9709    6.951990           34 -1.8380846     0.034  999\n## NTI ou SES_MNTD\nresultados_SES_MNTD <- ses.mntd(composicao_especies_P, cophenetic(filogenia_aves),\n                                null.model = \"taxa.labels\", \n                                abundance.weighted = FALSE,\n                                runs = 999) \n\n# Mostra a riqueza de espéices,MNTD observado, média e desvio padrão dos \n# valores de MNTD das aleatorizações, SES e o valor de p.\nhead(resultados_SES_MNTD)\n#>       ntaxa mntd.obs mntd.rand.mean mntd.rand.sd mntd.obs.rank  mntd.obs.z mntd.obs.p runs\n#> Com_1    27 63.89727       63.30504     6.864478           518  0.08627467      0.518  999\n#> Com_2    26 66.15828       64.81499     7.217753           575  0.18610860      0.575  999\n#> Com_3    25 72.96912       65.76333     7.754651           811  0.92922217      0.811  999\n#> Com_4    25 67.67170       65.70886     7.752673           600  0.25318258      0.600  999\n#> Com_5    22 64.93477       69.46545     9.133114           305 -0.49607138      0.305  999\n#> Com_6    18 63.72337       76.12099    11.819687           162 -1.04889562      0.162  999\n## SES_PD\nresultados_SES_PD <- ses.pd(composicao_especies_P, filogenia_aves,\n                            null.model = \"independentswap\", \n                            runs = 999) \n\n# Mostra a riqueza de espéices,MNTD observado, média e desvio padrão dos \n# valores de PD das aleatorizações, SES e o valor de p.\nhead(resultados_SES_PD) \n#>       ntaxa   pd.obs pd.rand.mean pd.rand.sd pd.obs.rank   pd.obs.z pd.obs.p runs\n#> Com_1    27 1259.315     1272.480   66.74674         429 -0.1972372    0.429  999\n#> Com_2    26 1293.152     1238.690   68.04238         799  0.8004172    0.799  999\n#> Com_3    25 1222.310     1201.471   65.51971         624  0.3180526    0.624  999\n#> Com_4    25 1254.541     1205.879   68.34858         750  0.7119674    0.750  999\n#> Com_5    22 1021.967     1096.024   67.73466         129 -1.0933409    0.129  999\n#> Com_6    18  856.781      950.239   64.54864          76 -1.4478696    0.076  999\n## Standardized effect size do Phylosor\n# Modelo nulo que rearranja o nome das espécies na filogenia. \nmodelos_nulo <- phylosor.rnd(composicao_especies_P, filogenia_aves, \n                             null.model = \"taxa.labels\", runs = 9)\n\n# Função para calcular o SES eo valor de P.\nses.physo <- function(obs, nulo_phylosor){\n    nulo_phylosor <- t(as.data.frame(lapply\n                                     (nulo_phylosor, as.vector)))\n    physo.obs <- as.numeric(obs)\n    physo.mean <- apply(nulo_phylosor, MARGIN = 2, \n                        FUN = mean, na.rm = TRUE)\n    physo.sd <- apply(nulo_phylosor, MARGIN = 2, \n                      FUN = sd, na.rm = TRUE)\n    physo.ses <- (physo.obs - physo.mean)/physo.sd\n    physo.obs.rank <- apply(X = rbind(physo.obs, \n                                      nulo_phylosor), MARGIN = 2, \n                            FUN = rank)[1, ]\n    physo.obs.rank <- ifelse(is.na(physo.mean), NA, \n                             physo.obs.rank)\n    data.frame(physo.obs, physo.mean, physo.sd, \n               physo.obs.rank, physo.ses, \n               physo.obs.p = physo.obs.rank/\n                   (dim(nulo_phylosor)[1] + 1))\n}\n\n## Resultados\nresultados <- ses.physo (resultados_Phylosor, modelos_nulo)\nhead(resultados)\n#>   physo.obs physo.mean   physo.sd physo.obs.rank  physo.ses physo.obs.p\n#> 1 0.7856828  0.7915561 0.04011508              6 -0.1464099         0.6\n#> 2 0.8052839  0.8813422 0.03356047              1 -2.2663056         0.1\n#> 3 0.7831520  0.8174306 0.04545519              2 -0.7541175         0.2\n#> 4 0.8586780  0.8687616 0.03659588              5 -0.2755416         0.5\n#> 5 0.6717414  0.7460991 0.02806215              1 -2.6497526         0.1\n#> 6 0.7414284  0.6904007 0.04842748              9  1.0536941         0.9"},{"path":"cap13.html","id":"para-se-aprofundar-9","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.6 Para se aprofundar","text":"","code":""},{"path":"cap13.html","id":"livros-8","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.6.1 Livros","text":"Recomendamos aos interessados() os livros: ) Swenson (2014) Functional Phylogenetic Ecology R; ii) Paradis (2012) Analysis Phylogenetics Evolution R; iii) Cadotte & Davies (2016) Phylogenies Ecology, iv) Gotelli & Graves (1996) Null Models Ecology; e v) Magurran & McGill (2011) Biological Diversity Frontiers Measurement Assessment.","code":""},{"path":"cap13.html","id":"links-8","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.6.2 Links","text":"O blog Ferramentas filogenéticas para biologia comparada pesquisador Liam Revell é uma ferramenta excelente para obter informações e aplicações das análises filogenéticas diretamente R.","code":""},{"path":"cap13.html","id":"exercícios-9","chapter":"Capítulo 13 Diversidade Filogenética","heading":"13.7 Exercícios","text":"13.1\nCarregue os dados - anuros_composicao (.e., 211 espécies de anuros coletados em 44 localidades na Mata Atlântica), anuros_ambientais (.e., variáveis climáticas, topográficas e coordenadas geográficas) e filogenia_anuros (filogenia das 211 espécies) - que estão pacote ecodados. Use função varpart() pacote vegan para testar importância relativa dos efeitos da precipitação anual, range altitudinal e temperatura anual na distribuição espacial da diversidade filogenética (PD) e Endemismo filogenético (PE). Calcule o SES para verificar se os resultados da diversidade filogenética (PD) diferem esperado ao acaso devido ao número de espécies em cada comunidade. Qual sua interpretação sobre os resultados?13.2\nCarregue os dados - anuros_composicao (.e., 211 espécies de anuros coletados em 44 localidades na Mata Atlântica), anuros_ambientais (.e., variáveis climáticas, topográficas e coordenadas geográficas) e filogenia_anuros (filogenia das 211 espécies) - que estão pacote ecodados. Use função varpart() pacote vegan para testar importância relativa dos efeitos da precipitação anual, range altitudinal e temperatura anual na distribuição espacial NRI e NTI. Qual sua interpretação sobre os resultados?13.3\nCarregue os dados - anuros_composicao (.e., 211 espécies de anuros coletados em 44 localidades na Mata Atlântica), anuros_ambientais (.e., variáveis climáticas, topográficas e coordenadas geográficas) e filogenia_anuros (filogenia das 211 espécies) - que estão pacote ecodados. Use função varpart() pacote vegan para testar importância relativa dos efeitos da precipitação anual, range altitudinal e distância geográfica na distribuição espacial dos diferentes componentes da diversidade beta filogenética (Phylosor). Qual sua interpretação sobre os resultados?Soluções dos exercícios.","code":""},{"path":"cap14.html","id":"cap14","chapter":"Capítulo 14 Diversidade Funcional","heading":"Capítulo 14 Diversidade Funcional","text":"","code":""},{"path":"cap14.html","id":"pré-requisitos-do-capítulo-10","chapter":"Capítulo 14 Diversidade Funcional","heading":"Pré-requisitos do capítulo","text":"Pacotes, dados e funções que serão utilizados neste capítulo.","code":"\n## Pacotes \nlibrary(FD)\nlibrary(ade4)\nlibrary(ecodados)\nlibrary(gridExtra)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(tidyverse)\nlibrary(picante)\nlibrary(vegan)\nlibrary(SYNCSA)\nlibrary(GGally)\nlibrary(FD)\nlibrary(betapart)\nlibrary(nlme)\nlibrary(ape)\nlibrary(TPD)\nlibrary(cati)\nlibrary(kableExtra)\n\n## Dados e funções\ncomun_fren_dat <- ecodados::fundiv_frenette2012a_comu\nambie_fren_dat <- ecodados::fundiv_frenette2012a_amb\ntrait_fren_dat <- ecodados::fundiv_frenette2012a_trait\ntrait_dat      <- ecodados::fundiv_barbaro2009a_trait\ncomun_dat      <- ecodados::fundiv_barbaro2009a_comu\nambie_dat      <- ecodados::fundiv_barbaro2009a_amb\ntrait_baselga  <- ecodados::trait_baselga\ncomm_baselga   <- ecodados::comm_baselga\nanuros_comm    <- ecodados::anuros_comm\ntraits         <- ecodados::traits\nenv            <- ecodados::env\n# ecodados::wITV # funtion: wITV"},{"path":"cap14.html","id":"aspectos-teóricos-5","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.1 Aspectos teóricos","text":"Até década de 1990, teoria ecológica investigava basicamente quais processos determinavam abundância e riqueza de espécies espaço e tempo. décadas de 1980 e 1990 foram marcadas por intensos debates sobre regras de montagem de comunidades e como interações e filtros ambientais determinavam coexistência de espécies (Strong et al. 1984). Porém, década de 2000 foi marcada pelo uso mais explícito da características das espécies como uma variável fundamental tanto para explicar como distribuição dos organismos seria afetada pelo ambiente, quanto para entender como tais espécies afetariam o ecossistema (Dı́az Cabido 2001; McGill et al. 2006). O primeiro estudo que utilizou o termo Diversidade Funcional foi publicado por Williams (1967), que comparou espécies de naúplios filogeneticamente relacionadas e demonstrou que elas possuem alta plasticidade funcional que favorecem ampla variação de comportamentos e, desse modo, permitem que sejam espécies generalistas em ambientes em constante mudança. unidade básica desses estudos, o atributo funcional (inglês “functional trait”), é definido como uma propriedade mensurável dos organismos (geralmente em nível individual) que represente características morfológicas, fisiológicas ou fenológicas que afetam aptidão alterando aspectos crescimento, reprodução e sobrevivência (Violle et al. 2007). Mais especificamente, o atributo funcional pode ser divido em atributo efeito (.e., atributos organismo que afetam condições ambientais ou propriedades ecossistema) e atributo resposta (.e., atributos organismo que variam em resposta condições ambientais) (Violle et al. 2007).Dessa forma, medidas de diversidade passaram ser representadas não somente por diferenças número e na quantidade de espécies, mas pelas diferenças e/ou semelhanças dos atributos funcionais das espécies dentro e entre localidades. Assim, variação grau de expressão de diferentes atributos funcionais entre diferentes populações, comunidades ou ecossistemas é definida como Diversidade Funcional (sensu Garnier, Navas, Grigulis 2015). Porém, diversidade funcional não deve ser usada como medida única, uma vez que tais diferenças entre os atributos funcionais podem ser medida partir da abundância relativa, riqueza e variação dos atributos funcionais. Desse modo, podemos dividir diversidade funcional em três diferentes medidas: ) riqueza funcional, ii) divergência funcional, e iii) regularidade funcional (Villéger, Mason, Mouillot 2008). Existem dezenas de métricas que calculam cada uma dessas dimensões da diversidade funcional, mas se destacam aquelas baseadas em dendrograma (e.g., FD: Petchey Gaston 2002) ou em medidas de distância (e.g., Villéger, Mason, Mouillot 2008). Assim como diversidade taxonômica (Capítulo 12), diversidade funcional pode ser medida em componentes alfa e beta. seguir, apresentamos diferentes maneiras de calcular distância entre localidades tendo como base os atributos funcionais das espécies e, além disso, demonstramos como calcular algumas das métricas de diversidade (alfa e beta) funcional mais usadas em Ecologia. parte final deste capítulo apresenta dois exemplos de como podemos testar hipóteses ecológicas comparando diversidade funcional alfa e beta.","code":""},{"path":"cap14.html","id":"definindo-a-dissimilaridade-entre-espécies","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.2 Definindo a dis(similaridade) entre espécies","text":"Definir o quão diferente ou semelhante são duas espécies que ocorrem em uma determinada localidade é base para calcular diversidade alfa e beta funcional. Para isso, é fundamental ter em mente que os atributos funcionais podem ser de vários tipos como, por exemplo, contínuos (e.g., tamanho corporal em centímetro), categóricos (e.g., guilda: frugívoro, detritívoro, etc.), ordinais (e.g., 1 para organismo até 5 cm, 2 para organismos entre 5 e 30 cm, e 3 para organismos maiores que 30 cm), binários (e.g., presença ou ausência de espinho), entre outros [veja Figura 2.1 Capítulo 2]. Por este motivo, decisão método de distância só será possível após o reconhecimento dos tipos de atributos funcionais escolhidos. Em linhas gerais, para variáveis contínuas distância euclidiana é melhor opção, enquanto para os outros tipos de variáveis ou para conjuntos de atributos com mais de um tipo de variável, distância de Gower geralmente deve ser melhor opção (Pavoine et al. 2009).Exemplo práticoExemplo1: variáveis contínuasVamos utilizar um conjunto de dados com atributos contínuos (e.g., área foliar específica e massa foliar seca) de 34 espécies de plantas em um gradiente de aridez (Frenette-Dussault et al. 2012). Diversas análises funcionais podem ser afetadas por valores extremos ou pela diferença de unidade/escala entre variáveis utilizadas. Por este motivo, é importante padronizar matriz de atributos com média 0 e desvio padrão 1. Esta padronização é necessária tanto para fazer uma PCA como para PCoA (veja Capítulo 9).PerguntaQuais são espécies de plantas mais semelhantes? (Neste caso, sem predição, pois representa uma avaliação exploratória com características funcionais das espécies).VariáveisDependentes: atributos funcionais (matriz de atributos contínuos por espécie: trait_fren_dat)AnálisesAqui realizamos os passos para uma ordenação usando os atributos funcionais contínuos (Figura 14.1).\nFigura 14.1: Ordenação usando os atributos funcionais contínuos.\nExemplo 2: variáveis categóricasNo próximo exemplo, utilizamos atributos funcionais de besouros distribuídos na Europa (Barbaro Van Halder 2009). Esses dados são categóricos e incluem atributos como período de atividade (noturno, diurno, dioturno), tendência da população na europa (estável, aumentando, diminuindo) entre outros. 📝 Importante \nAo contrário dos dados contínuos, para dados categóricos não é possível utilizar PCA.PerguntaQuais são espécies de besouros mais semelhantes? (Neste caso, sem predição, pois representa uma avaliação exploratória com características funcionais das espécies).VariáveisDependentes: atributos funcionais (matriz de atributos categóricos por espécie: trait_dat)AnálisesAqui realizamos os passos para uma ordenação usando os atributos funcionais categóricos (Figura 14.2).\nFigura 14.2: Ordenação usando os atributos funcionais categóricos.\nExemplo 3: variáveis mistasEm casos mais complexos, pesquisa inclui diversos atributos funcionais com naturezas diferentes, como atributos contínuos, categóricos, ordinais, circulares, entre outros. Desse modo, é possível utilizar medidas como Gower (gowdis()). Porém, existe uma alternativa mais apropriada que generalizou o coeficiente de Gower para tratar cada conjunto de variáveis de acordo com sua natureza (Pavoine et al. 2009). Vamos usar o mesmo conjunto de dados que foram considerados exemplo anterior. Porém, ao invés de utilizar somente variáveis categóricas, usaremos todas elas. O primeiro passo é identificar para o programa classes apropriadas para cada tipo de variável e, além disso, preparar os dados para função dist.ktab().PerguntaQuais são espécies de besouros mais semelhantes? (Neste caso, sem predição, pois representa uma avaliação exploratória com características funcionais das espécies)VariáveisDependentes: atributos funcionais (matriz de atributos contínuos e categóricos por espécie: trait_dat)AnálisesAqui realizamos os passos para uma ordenação usando os atributos funcionais de diversas naturezas (Figura 14.3).\nFigura 14.3: Ordenação usando os atributos funcionais de dados mistos.\nPodemos combinar os dois gráficos (baseado em variáveis categóricas e em variáveis mistas) para comparar duas medidas de distância, uma somente com dados categóricos (gower()) e uma com dados categóricos e ordinais (dist.ktab()) (Figura 14.4).\nFigura 14.4: Ordenações usando os atributos funcionais categóricos e de dados mistos.\n","code":"\n## PCoA dos atributos contínuos\n\n# 1. Padronização dos dados\ntrait_pad <- decostand(trait_fren_dat, \"standardize\")\neuclid_dis <- vegdist(trait_pad, \"euclidean\")\n\n# 2. PCoA\n# Resultados são idênticos aos resultados de uma PCA.\npcoa_traits_cont <- pcoa(euclid_dis, correction = \"cailliez\") \n\n# 3. Exportandos dados para gráfico\n# Ao usar '[,1:2]' você irá selecionar os dois primeiros eixos.\neixos_cont <- as.data.frame(pcoa_traits_cont$vectors[, 1:2]) \n\n# 4. Gráfico de ordenação \nplot_trait_cont <- ggplot(eixos_cont, aes(x = Axis.1, y = Axis.2)) + \n    geom_point(pch = 21, size = 4, color = \"black\", alpha = 0.7, fill = \"red2\") + \n    geom_text_repel(aes(Axis.1, Axis.2, label = rownames(eixos_cont))) +\n    geom_hline(yintercept = 0, linetype = 2) + \n    geom_vline(xintercept = 0, linetype = 2) +\n    labs(x = \"PCO 1\", y = \"PCO 2\", title = \"Dados contínuos\") + \n    tema_livro()\nplot_trait_cont\n## PCoA dos atributos categóricos\n\n# 1. Selecionar somente os atributos categóricos\ntrait_cat <- trait_dat %>% \n    dplyr::select_if(is.character)\n\n# 2. Calcular a distância de Gower\ndist_categ <- gowdis(trait_cat)\n\n# 3. PCoA da matriz de distância funcional (Gower)\npcoa_traits_cat <- pcoa(dist_categ, correction = \"cailliez\")\n\n# 4. Exportar dados (escores) para ggplot\neixos_cat <- as.data.frame(pcoa_traits_cat$vectors[,1:2]) # Selecionar os dois primeiros eixos\n\n# 5. Gráfico de ordenação\nplot_trait_cat <- ggplot(eixos_cat, aes(x = Axis.1, y = Axis.2)) + \n    geom_point(pch = 21, size = 4, alpha = 0.7, color = \"black\", fill = \"cyan4\") + \n    geom_text_repel(aes(Axis.1, Axis.2, label = rownames(eixos_cat))) +\n    geom_hline(yintercept = 0, linetype = 2) + \n    geom_vline(xintercept = 0, linetype = 2) +\n    labs(x = \"PCO 1\", y = \"PCO 2\", title = \"Dados categóricos\") + \n    tema_livro()\nplot_trait_cat\n## PCoA dos atributos mistos\n\n## 1. Verifique a classe de todos os traits e veja se estão de acordo com sua expectativa\ntrait_dat %>% \n    dplyr::summarise_all(class) %>% \n    tidyr::gather(variable, class)\n#>    variable     class\n#> 1     trend character\n#> 2   redlist character\n#> 3     regio   integer\n#> 4      biog character\n#> 5     activ character\n#> 6      diet character\n#> 7    winter character\n#> 8     color character\n#> 9     breed character\n#> 10     body   integer\n#> 11     wing character\n#> 12   period character\n\n## 2. Neste exemplo, algumas variáveis que são ordinais (regio e body) \n# foram reconhecidas como numéricas ou categóricas. \ntrait_dat$regio <- as.ordered(trait_dat$regio)\ntrait_dat$body <- as.ordered(trait_dat$body)\n\n## 3. Combinar cada conjunto de atributos de acordo com sua natureza em um \n# data.frame separado.\n# 3.1. Categóricos.\ntrait_categ <- cbind.data.frame(\n    trend = trait_dat$trend, \n    redlist = trait_dat$redlist, \n    biog = trait_dat$biog, \n    activ = trait_dat$activ,  \n    diet = trait_dat$diet, \n    winter = trait_dat$winter,\n    color = trait_dat$color, \n    breed = trait_dat$breed,\n    wing = trait_dat$wing, \n    period = trait_dat$period)\n\n# 3.2 Ordinais.\ntrait_ord <- cbind.data.frame(regio = trait_dat$regio, \n                              body = trait_dat$body)\nrownames(trait_categ) <- rownames(trait_dat)\nrownames(trait_ord) <- rownames(trait_dat)\n\n# Agora, combinar os dois data.frames em uma lista chamada \"ktab\".\nktab_list <- ktab.list.df(list(trait_categ, trait_ord))\n\n# Por fim, calcular a distância funcional entre as espécies.\n# Em \"type\", a letra \"N\" indica variável categórica (ou nominal), \n# enquanto a letra \"O\" indica variável ordinal.\ndist_mist <- dist.ktab(ktab_list, type = c(\"N\", \"O\"))\n\n## Visualize os dados com uma PCoA\npcoa_traits_mist <- pcoa(dist_mist, correction = \"cailliez\")\neixos_mist <- as.data.frame(pcoa_traits_mist$vectors[,1:2]) \n\nplot_trait_mist <- ggplot(eixos_mist, aes(x = Axis.1, y = Axis.2)) + \n    geom_point(pch = 21, size = 4, alpha = 0.7, \n               color = \"black\", fill = \"darkorange\") + \n    geom_text_repel(aes(Axis.1, Axis.2, label = rownames(eixos_mist)))+\n    geom_hline(yintercept = 0, linetype = 2) + \n    geom_vline(xintercept = 0, linetype = 2) + \n    labs(x = \"PCO 1\", y = \"PCO 2\", title = \"Dados mistos\") + \n    tema_livro()\nplot_trait_mist\n## Gráficos\ngrid.arrange(plot_trait_cat, plot_trait_mist, ncol = 2)"},{"path":"cap14.html","id":"métricas-de-diversidade-funcional-alfa","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.3 Métricas de diversidade funcional (alfa)","text":"","code":""},{"path":"cap14.html","id":"riqueza-funcional","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.3.1 Riqueza funcional","text":"riqueza funcional mede quantidade de espaço funcional preenchido pelas espécies de uma comunidade (Mason Mouillot 2013). estimativa desse espaço pode ser calculada usando dendrogramas (Petchey Gaston 2002) ou através método Convex Hull (Cornwell, Schwilk, Ackerly 2006) que dão origem, respectivamente, às duas métricas mais usadas: ) Diversidade Funcional (FD) e ii) Riqueza Funcional (FRic). Os índices de riqueza funcional geralmente são usados como indicadores espaço de nicho que é potencialmente usado ou não (Schleuter et al. 2010).Exemplo práticoExplicação dos dadosOs dados utilizados neste exemplo são os mesmos exemplo com dados mistos, .e., categóricos e contínuos (objeto dist_mist).PerguntaQual relação entre riqueza de espécies e diversidade funcional? Todos os índices são correlacionados com riqueza?VariáveisDependentes: atributos funcionais e composição de espécies para cálculo da diversidade funcional e riqueza em cada parcelaAnálisesVamos começar olhando os dados de dissimilaridade das espécies (dist_mist) e os dados de composição (comun_dat)Agora vamos calcular riqueza de espécies por comunidadeVamos calcular Riqueza (FRic) e Diversidade funcional usando o pacote dbFD. 📝 Importante \nO índice Functional Richness só funciona para comunidades com 3 ou mais espécies. Caso você tenha comunidades com 1 ou 2 espécies, o valor será NAOs valores da Riqueza Funcional (FRic) variam entre 0 e +∞, sendo o valor máximo limitado pelo número de espécies em uma comunidade. Desse modo, os valores de FRic basicamente são uma representação da riqueza de espécies (e seus atributos funcionais) de uma comunidade (Villéger, Mason, Mouillot 2008). Ou seja, quanto maior o número de espécies, maior será o espaço funcional ocupado por essas espécies. pacote ‘dbFD’ o valor de FRic é dividido pelo valor de FRic global (ou seja, pela riqueza funcional de todas comunidades combinadas). Como resultado, ele limita variação entre 0 e 1, onde valores próximos 1 indicam que uma determinada comunidade tem riqueza funcional tão alta quanto riqueza funcional de todas comunidades juntas. Neste exemplo, localidades 21 e 89 possuem, respectivamente, maior e menor Riqueza Funcional.","code":"\n## Estrutura dos dados\n# matriz de distância: distância entre as seis primeiras espécies\nas.matrix(dist_mist)[1:6, 1:6]\n#>           sp1       sp2       sp3       sp4       sp5       sp6\n#> sp1 0.0000000 0.5000000 0.7107801 0.7771900 0.6107116 0.5041691\n#> sp2 0.5000000 0.0000000 0.6808389 0.8538292 0.7345988 0.6487320\n#> sp3 0.7107801 0.6808389 0.0000000 0.7179711 0.7381353 0.6527339\n#> sp4 0.7771900 0.8538292 0.7179711 0.0000000 0.5106682 0.6522593\n#> sp5 0.6107116 0.7345988 0.7381353 0.5106682 0.0000000 0.5177440\n#> sp6 0.5041691 0.6487320 0.6527339 0.6522593 0.5177440 0.0000000\n\n# composição de espécies: seis primeiras espécies nas seis primeiras localidades\nhead(comun_dat)[1:6, 1:6]\n#>    sp1 sp2 sp3 sp4 sp5 sp6\n#> 3    0  19   2   0   0   0\n#> 4    0   4   0   0   0   0\n#> 6    1  58   2   0   0   0\n#> 7    1   0   0   0   0   0\n#> 9    3   0   0   0   0   0\n#> 10   3  15   0   0   0   0\n## Riqueza de espécies\nrichness <- dbFD(dist_mist, comun_dat)$nbsp \n#> FRic: Dimensionality reduction was required. The last 17 PCoA axes (out of 19 in total) were removed. \n#> FRic: Quality of the reduced-space representation = 0.3243851 \n#> CWM: When 'x' is a distance matrix, CWM cannot be calculated.\nhead(richness)\n#>  3  4  6  7  9 10 \n#> 12  3  7  7  4  7\n# Functional Richness\nfric <- dbFD(dist_mist, comun_dat)$FRic\n#> FRic: Dimensionality reduction was required. The last 17 PCoA axes (out of 19 in total) were removed. \n#> FRic: Quality of the reduced-space representation = 0.3243851 \n#> CWM: When 'x' is a distance matrix, CWM cannot be calculated.\nhead(fric)\n#>           3           4           6           7           9          10 \n#> 0.226236923 0.009033539 0.158760885 0.158529234 0.014290140 0.200075112\n\n## Functional Diversity \n# Passo 1: análise de agrupamento para criar o dendrograma. \ndend <- hclust(dist_mist, \"average\")\n\n# Passo 2: transformar o dengrograma em um arquivo da classe phylo.\ntree_dend <- as.phylo(dend)\n\n# Passo 3: calcular o valor da diversidade funcional. \nFD <- pd(comun_dat, tree_dend)$PD\nhead(FD)\n#> [1] 3.590053 1.115574 2.255337 2.356478 1.472314 2.430329"},{"path":"cap14.html","id":"divergência-funcional","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.3.2 Divergência funcional","text":"divergência funcional é uma medida que descreve irregularidade na distribuição dos valores dos atributos volume espaço funcional ocupado por todas espécies de uma certa comunidade (Garnier, Navas, Grigulis 2015). Para obter os valores de divergência, o espaço funcional é calculado através método Convex Hull (Functional Divergence) ou espaço multidimensional calculado com um PCoA (Functional Dispersion). Nos dois casos, o valor da métrica representa distância média das espécies para o centro de gravidade ou centroide espaço funcional, ponderado pela abundância relativa das espécies (Villéger, Mason, Mouillot 2008; Laliberté Legendre 2010). Desse modo, divergência funcional é uma medida que calcula o grau de diferenciação em que distribuição da abundância maximiza divergência entre os atributos funcionais (Mason Mouillot 2013). Em geral, estudos que usam esses índices buscam entender o grau de diferenciação de recursos de espécies que coexistem em uma comunidade (Garnier, Navas, Grigulis 2015).Vamos calcular divergência funcional (FDiv) e dispersão funcional (FDis) usando o pacote dbFD. Para isso, usaremos matriz de distância obtida dos dados trait_dat (variáveis categóricas e ordinais) e nomeada como dist_mist 📝 Importante \n- O índice Functional Divergence só funciona para comunidades com 3 ou mais espécies. Caso você tenha comunidades com 1 ou 2 espécies, o valor será NAO índice Functional Dispersion só funciona para comunidades com 3 ou mais espécies. Caso você tenha comunidades com 1 ou 2 espécies, o valor será zeroOs valores da Divergência Funcional (FDiv) variam entre 0 e 1. Valores que se aproximam de zero indicam que espécie mais abundante está muito próxima valor atributo médio da comunidade, ao passo que valores próximos 1 indicam que espécie mais abundante está muito distante (ou seja, é muito diferente) valor médio da comunidade (Villéger, Mason, Mouillot 2008). Neste exemplo, localidades 159 e 6 possuem, respectivamente, maior e menor Divergência Funcional.","code":"\n## \"Functional Divergence\" s\nfdiv <- dbFD(dist_mist, comun_dat)$FDiv\n#> FRic: Dimensionality reduction was required. The last 17 PCoA axes (out of 19 in total) were removed. \n#> FRic: Quality of the reduced-space representation = 0.3243851 \n#> CWM: When 'x' is a distance matrix, CWM cannot be calculated.\nhead(fdiv)\n#>         3         4         6         7         9        10 \n#> 0.9692023 0.8838557 0.4082808 0.9147644 0.9010790 0.6982640\n\n# \"Functional Dispersion\" \nfdis <- dbFD(dist_mist, comun_dat)$FDis\n#> FRic: Dimensionality reduction was required. The last 17 PCoA axes (out of 19 in total) were removed. \n#> FRic: Quality of the reduced-space representation = 0.3243851 \n#> CWM: When 'x' is a distance matrix, CWM cannot be calculated.\nhead(fdis)\n#>         3         4         6         7         9        10 \n#> 0.2977975 0.3203602 0.2218237 0.3261248 0.3683898 0.3910530"},{"path":"cap14.html","id":"regularidade-funcional","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.3.3 Regularidade funcional","text":"regularidade funcional (inglês Functional Evenness - FEve) mede o quão regular é distribuição da abundância dos valores dos atributos funcionais espaço funcional. Diferente dos outros métodos, versão multidimensional deste índice utiliza um método chamado Minimum Spanning Tree (MST) para conectar todas espécies espaço funcional. distância par apar das espécies na MST é ponderada pela abundância relativa das espécies e, desse modo, o valor final da regularidade funcional (FEve) vai variar de 0 (máxima irregularidade da distribuição da abundância ou distância funcional das espécies) 1 (máxima regularidade).Vamos calcular Regularidade Funcional (FEve) usando o pacote dbFD. Para isso, usaremos matriz de distância obtida dos dados trait_dat (variáveis categóricas e ordinais) e nomeada como dist_mist 📝 Importante \nO índice Functional evenness só funciona para comunidades com 3 ou mais espécies. Caso você tenha comunidades com 1 ou 2 espécies, o valor será NAOs valores da Regularidade Funcional (FEve) variam entre 0 e 1 (máxima regularidade ou regularidade perfeita). diminuição valor de FEve em direção zero indica que uma redução da regularidade da distribuição da abundância ou distância funcional entre espécies (Villéger, Mason, Mouillot 2008). Neste exemplo, localidades 197 e 175 possuem, respectivamente, maior e menor Regularidade Funcional.","code":"\n## \"Functional evenness\" \nfeve <- dbFD(dist_mist, comun_dat)$FEve\n#> FRic: Dimensionality reduction was required. The last 17 PCoA axes (out of 19 in total) were removed. \n#> FRic: Quality of the reduced-space representation = 0.3243851 \n#> CWM: When 'x' is a distance matrix, CWM cannot be calculated.\nhead(feve)\n#>         3         4         6         7         9        10 \n#> 0.4054808 0.5587917 0.5406140 0.6974712 0.9575697 0.6297941"},{"path":"cap14.html","id":"correlação-entre-as-métricas-de-diversidade-funcional-alfa","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.3.4 Correlação entre as métricas de diversidade funcional (alfa)","text":"Aqui vamos fazer correlação entre métricas de diversidade funcional (alfa) (Figura 14.5).\nFigura 14.5: Correlação entre métricas de diversidade funcional (alfa).\nOs resultados indicam que Diversidade Funcional de Petchey & Gaston (2002) (r = 0.985) e riqueza funcional (r = 0.813) são altamente correlacionadas com riqueza de espécies. Porém, divergência funcional, regularidade funcional e dispersão funcional não estão correlacionadas com riqueza de espécies.figura obtida com o código ggpairs(metricas) representa uma matriz de correlação comparando cada par de variáveis (neste caso, os índices de diversidade). lado esquerdo da figura (abaixo da diagonal) são representados scatter plots (veja Capítulo 6), lado direito (acima da diagonal) pode-se encontrar os valores das correlações (r) entre os pares comparados. caso das correlações, quanto mais próximo de +1 ou -1, mais forte é relação entre essas variáveis par comparado. O gráfico de linhas na diagonal demonstra densidade de cada variáveis individualmente (veja Capítulo 7).","code":"\n## Gráficos\n## Você pode criar uma tabela com os resultados de todas as métricas\nmetricas <- data.frame(richness = richness,\n                       FD_gp = FD,\n                       fric = fric,\n                       fdiv = fdiv,\n                       fdis = fdis,\n                       feve = feve)\nhead(metricas)\n#>    richness    FD_gp        fric      fdiv      fdis      feve\n#> 3        12 3.590053 0.226236923 0.9692023 0.2977975 0.4054808\n#> 4         3 1.115574 0.009033539 0.8838557 0.3203602 0.5587917\n#> 6         7 2.255337 0.158760885 0.4082808 0.2218237 0.5406140\n#> 7         7 2.356478 0.158529234 0.9147644 0.3261248 0.6974712\n#> 9         4 1.472314 0.014290140 0.9010790 0.3683898 0.9575697\n#> 10        7 2.430329 0.200075112 0.6982640 0.3910530 0.6297941\n\n## Gráfico para comparar o comportamento das métricas\nggpairs(metricas) + tema_livro()"},{"path":"cap14.html","id":"métricas-de-diversidade-funcional-beta","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.4 Métricas de diversidade funcional (beta)","text":"Assim como na diversidade beta taxonômica (Capítulo 12) e na diversidade beta filogenética (Capítulo 13), diversidade beta funcional é uma medida que compara composição (e variação na composição) de atributos funcionais das espécies entre duas ou mais localidades. Porém, assim como na medida tradicional taxonômica (como Jaccard ou Sørensen), diferenças na diversidade beta podem ser geradas pela mudança na identidade das espécies (ou atributo) ou na riqueza de espécies (ou de atributos) entre duas localidades (Figura 14.6). Desse modo, é possível particionar diversidade beta funcional em aninhamento (inglês nestedness) e substituição (inglês turnover) (veja Capítulo 12). Além disso, os cálculos da diversidade beta funcional podem ser realizados par par (functional.beta.pair()) ou para comparações de múltiplas localidades (funcional.beta.multi()).\nFigura 14.6: Partição da diversidade beta taxonômica () e funcional (B). Os três cenários apresentados tanto para diversidade beta taxonômica como funcional representam, respectivamente, diversidade beta explicada somente por substituição, aninhamento e uma combinação dos dois.\nExemplo 4Os dados exemplo seguir utilizam somente informação de presença (1) ou ausência (0) das espécies nas localidades. Neste exemplo hipotético criado por Baselga et al. (2021), foram amostradas 11 espécies (sp1-sp11) em quatro localidades (-D). Para cada espécie, criamos dois atributos contínuos hipotéticos (trait1 e trait2).PerguntaQual contribuição relativa aninhamento e substituição para diversidade beta?VariáveisDependentes: atributos funcionais e composição de espécies.AnálisesVamos fazer análise da partição da diversidade beta funcional e comparar seus componentes (Figura 14.7).\nFigura 14.7: Relação entre os componentes partição da diversidade funcional (beta).\nOs resultados da análise de partição (fun_beta_multi()) indicam que 82,5% (0,710/0,861) da variação na diversidade beta é explicada pelo componente substituição, enquanto 17,5% (0,151/0,861) pelo componente aninhamento. matrizes de distância obtidas na análise par par podem ser utilizadas para testar, posteriori, relação entre gradientes ambientais e diversidade beta funcional (mais detalhes abaixo).","code":"\nknitr::include_graphics(path = \"img/cap14_fig01.svg\")\n## Partição da Diversidade beta (Método Baselga)\nfun_beta_multi <- functional.beta.multi(x = comm_baselga,\n                                        trait = trait_baselga, index = \"jaccard\") \nfun_beta_multi\n#> $funct.beta.JTU\n#> [1] 0.7101449\n#> \n#> $funct.beta.JNE\n#> [1] 0.1509662\n#> \n#> $funct.beta.JAC\n#> [1] 0.8611111\n\n## Partição da Diversidade beta (Método Baselga)\nfun_beta <- functional.beta.pair(x = comm_baselga, \n                                 trait = trait_baselga, index = \"jaccard\") \n\n# Os códigos abaixo permitem extrair a matriz de distância (par a par) com a partição em substituição e nestedness\nfun_turnover <- fun_beta$funct.beta.jne\nfun_nestedness <- fun_beta$funct.beta.jtu\nfun_jaccard <- fun_beta$funct.beta.jac\n\n## Gráfico de comparação do substituição e aninhamento\ndat_betapart <- data.frame(turnover = as.numeric(fun_turnover), \n                           nested = as.numeric(fun_nestedness))\n\n## Gráfico\nplot_betapart <- ggplot(dat_betapart, aes(x = turnover, y = nested)) + \n    geom_point(pch = 21, size = 4, alpha = 0.7, color = \"black\", fill = \"#525252\") + \n    labs(x = \"Beta Diveristy (Substituição)\", y = \"Beta Diveristy (Aninhamento)\") +\n    tema_livro()\nplot_betapart"},{"path":"cap14.html","id":"composição-funcional-community-wegihed-means---cwm","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.5 Composição Funcional (Community Wegihed Means - CWM)","text":"medidas de diversidade beta funcional apresentadas acima fornecem matrizes de distância com comparações par par de localidades em termos da composição de atributos funcionais. Porém, muitas vezes o pesquisador quer medir o “atributo médio” da comunidade para investigar, por exemplo, se um determinado gradiente ambiental afeta expressão (em termos de abundância ou densidade) de dado atributo funcional. Em geral, medida utilizada é o CWM (inglês Community Wegihed Means). O CWM é basicamente uma média ponderada de um determinado atributo (coluna m na matriz T) em relação abundância de todas espécies que ocorrem na localidade (linha n na matriz X). O cálculo R é feito pela função functcomp() e usa somente duas matrizes (T e X). Os leitores que pretendem usar essas métricas devem ler críticas em Peres-Neto et al. (2017).matriz resultante cwm_es é formada pelas localidades (linhas) e os atributos “médios” (colunas) nestas localidades. Essa matriz pode ser utilizada em diversas análises como dbRDA, RDA ou RDA parcial (veja Capítulo 9). Na sequência, vamos utilizar testes de hipóteses para entender como podemos calcular diversidades funcional alfa e beta com outros testes estatísticos apresentados neste livro.Exemplo 5Neste exemplo, usaremos novamente os dados de 34 espécies de plantas (Frenette-Dussault et al. 2012), mas agora vamos testar o efeito de um gradiente de aridez sobre diversidade alfa funcional.PerguntaO gradiente de aridez influencia divergência e regularidade funcional de plantas?PrediçõesPredição 1: locais mais áridos possuem menor divergência funcional de plantas (métrica escolhida: FDis)Predição 1: locais mais áridos possuem menor divergência funcional de plantas (métrica escolhida: FDis)Predição 2: locais mais úmidos possuem menor regularidade funcional de plantas (métrica escolhida: FEve)Predição 2: locais mais úmidos possuem menor regularidade funcional de plantas (métrica escolhida: FEve)VariáveisPreditora: gradiente de aridez (matriz de variáveis ambientais por localidade: ambie_fren_dat)Preditora: gradiente de aridez (matriz de variáveis ambientais por localidade: ambie_fren_dat)Dependentes: composição de espécies (matriz de espécies por localidade: comun_fren_dat) e atributos funcionais (matriz de atributos contínuos por espécie: trait_fren_dat)Dependentes: composição de espécies (matriz de espécies por localidade: comun_fren_dat) e atributos funcionais (matriz de atributos contínuos por espécie: trait_fren_dat)AnálisesVamos calcular primeiramente matrizes de Divergência funcional (FDis) e Regularidade Funcional (FEve), depois ajustar modelos lineares, fazer o diagnóstico dos mesmo e por fim plotar os resultados (Figura 14.8).\nFigura 14.8: Relação entre composição funcional e o gradiente de aridez, ajustado por modelos lineares com seus diagnósticos.\nOs resultados dos modelos anova(mod1) e anova(mod2) indicam que o gradiente de aridez não afeta dispersão e regularidade funcional. Os detalhes para conferir os pressupostos das análises foram descritos Capítulo 7.Exemplo 6Agora, vamos utilizar novamente os dados de 34 espécies de plantas (Frenette-Dussault et al. 2012), mas agora para testar o efeito pastejo sobre diversidade beta funcional.PerguntaO pastejo determina ocorrência de espécies de plantas com diferentes atributos funcionais?PrediçãoA composição funcional de plantas é diferente entre áreas com e sem pastejo?VariáveisPreditora: áreas com e sem pastejo de gado (variável categórica com dois níveis: grazed e ungrazed: ambie_fren_dat)Preditora: áreas com e sem pastejo de gado (variável categórica com dois níveis: grazed e ungrazed: ambie_fren_dat)Dependentes: composição de espécies (matriz de espécies por localidade: comun_fren_dat) e atributos funcionais (matriz de atributos contínuos por espécie: trait_fren_dat)Dependentes: composição de espécies (matriz de espécies por localidade: comun_fren_dat) e atributos funcionais (matriz de atributos contínuos por espécie: trait_fren_dat)AnálisesNesse exemplo vamos calcular CWM e depois usar uma PERMANOVA (Capítulo 9) para comparar o efeito das áreas com e sem pastejo sobre diversidade funcional (Figura 14.9).\nFigura 14.9: Relação pastejo sobre composição funcional.\nNeste exemplo, os resultados perman_fren demonstram que composição funcional de plantas não é afetada pelo pastejo (P > 0,05) e que dispersão da composição (uma medida potencial de diversidade beta: M. J. Anderson, Ellingsen, McArdle 2006) de espécies também não muda entre áreas com ou sem pastejo (permutest(betad_fren)). função betadisper() deve ser sempre utilizada em conjunto com PERMANOVA (adonis()) para poder interpretar quais fontes de variação na composição de espécies. Sendo assim, esta análise representa um método fundamental para comparar se o potencial efeito (quando houver) é fruto de diferença na composição de espécies (.e., diferença na posição dos centroides entre dois ou mais grupos) ou na variação da composição de espécies entre os grupos (.e., diferença na dispersão dos dados em relação aos centroides, ver mais Capítulo 9). Esta última informação, dispersão, é geralmente interpretada como uma analogia homogeneidade de variâncias de uma ANOVA (.e., Teste de Levene). hipótese nula betadisper() é que dispersão dos grupos é homogênea (ou seja, o valor de probabilidade nos casos que existem dispersões homogêneas será maior que 0,05). Porém, se esse valor de p menor que 0,05, você deve rejeitar hipótese nula e concluir que dispersões são heterogêneas.Os gráficos de PCoA são uma ferramenta poderosa para interpretar os resultados da PERMANOVA + Betadisper. Quanto mais diferente composição de espécies entre dois ou mais grupos, mais distante devem ser os centroides desses grupos. Além disso, se áreas dos polígonos que conectam todas réplicas de cada grupo forem diferentes em tamanho (hipótese que será testada com o Betadisper), é possível também visualizar esta diferença. Em conclusão, para testar se diferenças de composição funcional existem entre dois ou mais grupos, será fundamental: ) comparar variação da posição dos centróides (função adonis()) e ii) variação da dispersão da composição funcional entre os grupos (função betadisper()).","code":"\n## Matriz T\nhead(trait_baselga)\n#>     Trait.1 Trait.2\n#> sp1       1       1\n#> sp2       1       2\n#> sp3       1       4\n#> sp4       2       1\n#> sp5       2       2\n#> sp6       3       3\n\n## Matriz X\nhead(comm_baselga)\n#>   sp1 sp2 sp3 sp4 sp5 sp6 sp7 sp8 sp9 sp10 sp11\n#> A   1   1   0   1   1   0   0   0   0    0    0\n#> B   1   0   1   0   0   0   0   1   1    0    0\n#> C   0   0   0   0   0   1   1   0   0    1    1\n#> D   0   1   0   1   0   0   1   0   1    0    0\n\n## Função functcomp calcula o cwm para combinar as matrizes T e X\ncwm_ex <- functcomp(trait_baselga, as.matrix(comm_baselga))\ncwm_ex\n#>   Trait.1 Trait.2\n#> A     1.5     1.5\n#> B     2.5     2.5\n#> C     4.0     4.0\n#> D     2.5     3.0\n## Passo 1: calcular a distância funcional\ntrait_pad <- decostand(trait_fren_dat, \"standardize\")\neuclid_dis <- vegdist(trait_pad, \"euclidean\")\n\n## Passo 2: calcular a Divergência funcional (FDis) e Regularidade Funcional (FEve)\nfdis <- dbFD(euclid_dis, comun_fren_dat)$FDis # Fdis=0 em locais com somente uma espécie\n#> FRic: No dimensionality reduction was required. All 5 PCoA axes were kept as 'traits'. \n#> CWM: When 'x' is a distance matrix, CWM cannot be calculated.\nfeve <- dbFD(euclid_dis, comun_fren_dat)$FEve\n#> FRic: No dimensionality reduction was required. All 5 PCoA axes were kept as 'traits'. \n#> CWM: When 'x' is a distance matrix, CWM cannot be calculated.\n\n## Passo 3: Utilizar um modelo linear para comparar o efeito da aridez sobre FDis (predição 1) e FEve (predição 2)\n# Combinar dados em um data.frame.\nlm_dat <- data.frame(aridez = ambie_fren_dat$Aridity, fdis = fdis, feve = feve)\n\n# Modelo 1\nmod1 <- lm(fdis ~ aridez, data = lm_dat)\n\n# Conclusão: a aridez não tem efeito sobre a divergência funcional\nanova(mod1) \n#> Analysis of Variance Table\n#> \n#> Response: fdis\n#>           Df Sum Sq Mean Sq F value Pr(>F)\n#> aridez     1 0.2083 0.20834  0.9945 0.3241\n#> Residuals 44 9.2179 0.20950\n\n# Modelo 2\nmod2 <- lm(feve ~ aridez, data = lm_dat)\n\n# Conclusão: a aridez não tem efeito sobre a regularidade funcional\nanova(mod2) \n#> Analysis of Variance Table\n#> \n#> Response: feve\n#>           Df  Sum Sq  Mean Sq F value Pr(>F)\n#> aridez     1 0.02098 0.020979  1.0447 0.3123\n#> Residuals 44 0.88353 0.020080\n\n## Passo 4: gráfico para visualizar os dois resultados  \n# Gráfico modelo 1.\nplot_pred1 <- ggplot(lm_dat, aes(x = aridez, y = fdis)) + \n    geom_point(pch = 21, size = 4, alpha = 0.7, color = \"black\", fill=\"darkorange\") +\n    labs(x = \"Aridez\", y = \"Divergência Funcional (FDis)\") +\n    tema_livro()\n\n# Gráfico modelo 2.\nplot_pred2 <- ggplot(lm_dat, aes(x = aridez, y = feve)) + \n    geom_point(pch=21, size=4, alpha = 0.7, color = \"black\", fill=\"cyan4\") + \n    labs(x = \"Aridez\", y = \"Regularidade Funcional (FEve)\") + \n    tema_livro()\n\n## Visualização dos dois gráficos em um única janela\ngrid.arrange(plot_pred1, plot_pred2, ncol = 2)\n## Passo 1: CWM\ncwm_fren <- functcomp(trait_pad, as.matrix(comun_fren_dat))\nhead(cwm_fren)\n#>           LA        SLA        LDMC       LN15        LCC\n#> 1 -0.2411700 -0.3485515  0.19745200  0.1874003 -0.5367368\n#> 2 -0.3977371  0.2326622 -0.09093270 -0.2859777  0.1643190\n#> 3 -0.1857134  0.2010756 -0.39877265 -0.1250643 -0.4304617\n#> 4 -0.2284064  0.1604101  0.80496307 -0.3704253  0.7193853\n#> 5 -0.1664790  0.3486956  0.02232213 -0.2041931  0.2051391\n#> 6 -0.3258821  0.3664583  0.04996829 -0.3352572  0.4713089\n\n## Passo 2: calcular a distância funcional\ncwm_dis <- vegdist(cwm_fren, \"euclidean\")\n\n## Passo 3: testar se a composição funcional varia entre as áreas com uma PERMANOVA\nperman_fren <- adonis(cwm_fren ~ Grazing, data = ambie_fren_dat)\n\n## Passo 4: comparar a variação dentro de cada grupo com Betadisper \nbetad_fren <- betadisper(cwm_dis, ambie_fren_dat$Grazing)\npermutest(betad_fren)\n#> \n#> Permutation test for homogeneity of multivariate dispersions\n#> Permutation: free\n#> Number of permutations: 999\n#> \n#> Response: Distances\n#>           Df  Sum Sq  Mean Sq      F N.Perm Pr(>F)\n#> Groups     1  0.0539 0.053858 0.1946    999  0.669\n#> Residuals 44 12.1763 0.276735\n\n## Passo 5: PCoA\n\ncwm_pcoa <- pcoa(D = cwm_dis, correction = \"cailliez\")\npcoa_eixos <- cwm_pcoa$vectors[, 1:2]\npcoa_dat <- data.frame(pastagem = ambie_fren_dat$Grazing, pcoa_eixos)\n\n## Passo 6: definir os grupos (\"HULL\") para serem categorizados no gráfico \n\ngrp.Grazed <- pcoa_dat[pcoa_dat$pastagem == \"Grazed\", ][chull(pcoa_dat[pcoa_dat$pastagem == \"Grazed\", c(\"Axis.1\", \"Axis.2\")]), ]\ngrp.Ungrazed <- pcoa_dat[pcoa_dat$pastagem == \"Ungrazed\", ][chull(pcoa_dat[pcoa_dat$pastagem == \"Ungrazed\", c(\"Axis.1\", \"Axis.2\")]), ]\nhull_cwm <- rbind(grp.Grazed, grp.Ungrazed) \n\n## Passo 7: Gráfico biplot\n\n100 * (cwm_pcoa$values[, 1]/cwm_pcoa$trace)[1] # % de explicação do eixo 1\n#> [1] 53.5471\n100 * (cwm_pcoa$values[, 1]/cwm_pcoa$trace)[2] # % de explicação do eixo 2\n#> [1] 24.56026\n\nggplot(pcoa_dat, aes(x = Axis.1, y = Axis.2, color = pastagem, shape = pastagem)) + \n  geom_point(size = 4, alpha = 0.7) + \n  geom_polygon(data = hull_cwm, aes(fill = pastagem, group = pastagem), alpha = 0.3) + \n  scale_color_manual(values = c(\"darkorange\", \"cyan4\")) +\n  scale_fill_manual(values = c(\"darkorange\", \"cyan4\")) +\n  labs(x = \"PCO 1 (53.6%)\", y = \"PCO 2 (24.6%)\") + \n  tema_livro()"},{"path":"cap14.html","id":"variação-intraespecífica","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.6 Variação Intraespecífica","text":"Os métodos discutidos anteriormente utilizam valores médios dos atributos das espécies para descrever estrutura funcional da comunidade e interpretar relações entre determinadas variáveis preditoras (como clima, por exemplo) com diversidade funcional. Porém, ao utilizar atributos médios estamos desconsiderando que variação deste atributo dentro da espécie seja determinante para resposta da espécie ao ambiente ou seu efeito sobre o ecossistema (Bolnick et al. 2011 ; Violle et al. 2012). Os estudos que usam dados médios para testar hipóteses em ecologia funcional argumentam que variação dentro das espécie é menor que variação entre espécies e, desse modo, o ruído causado ao desconsiderar variância atributo dentro da espécies é desprezível (Siefert et al. 2015). Porém, diversos estudos têm mostrado que esse argumento é frágil e que inclusão da variação intraespecífica melhora nossa capacidade preditiva em ecologia funcional (Violle et al. 2012; Siefert et al. 2015). Uma abordagem geralmente utilizada é decomposição da variância atributo em diferentes níveis de organização: ) variação dentro da população da mesma espécie em uma mesma unidade amostral, ii) variação dentro das populações (independente da espécie) de uma comunidade em uma mesma unidade amostral, e iii) variação entre populações. Conhecida como estatística T, esta abordagem permite entender fontes (intra ou interespecífica) de variação nos atributos em diferentes escalas (Violle et al. 2012). Outro método que quantifica variância explicada pela variabilidade intraespecífica, interespecífica e covariância entre elas foi proposto por Leps et al. (2011). Este método permite calcular contribuição da variação intraespecífica dentro e entre comunidades. Agora, vamos entender contribuição da variação de um atributo dentro da espécie comparada à variação entre espécies.Exemplo 7Vamos utilizar os dados de 11 espécies de anuros associados com 26 poças Parque Nacional Lagoa Peixe (Dalmolin, Tozetti, Pereira 2020). Atributos morfológicos foram coletados em todos os indivíduos coletados em cada poça. Desse modo, é possível comparar variação morfológica entre indivíduos da mesma espécie e entre espécies diferentes. Além disso, é possível quantificar contribuição da variação dentro e entre diferentes poças. exemplo abaixo, criamos cinco atributos com nomes diferentes daqueles usados artigo de Dalmolin et al. (2020). Em cada poça, os autores coletaram os seguintes dados das poças: ) profundidade, ii) area, iii) distância entre poças, e iv) distância da poça para floresta mais próxima.Pergunta 1Qual contribuição da variação intraespecífica para variação total dos atributos morfológicos de anuros?PrediçãoA alta plasticidade fenotípica de anuros indica alta contribuição da variação intraespecífica comparada interespecíficaVariáveisPreditora: espécies (categórica)Dependente: variação dos atributos morfológicosAnálisesAqui vamos realizar o passo passo das análises para comparar contribuição da variação intraespecífica para variação total dos atributos morfológicos.Pergunta 2Qual contribuição da variação entre poças para variação total dos atributos morfológicos de anuros?PrediçãoA variação morfológica de anuros é afetada por mudanças dentro das espécies e entre diferentes espécies de poças distintasVariáveisPreditoras: poças e espécies (ambas categóricas)Dependente: variação dos atributos morfológicosAnálisesAqui vamos realizar o teste para comparar contribuição da variação entre poças para variação total dos atributos morfológicos (Figura 14.10).\nFigura 14.10: Gráfico de barras mostrando contribuição da variação entre poças para variação total dos atributos morfológicos.\nPergunta 3Características ambientais das poças afetam variação intraespecífica?PrediçãoA profundidade e área da poça aumentam contribuição da variação intraespecífica em relação variação interespecífica.VariáveisPreditora: características das poçasDependentes: variação dos atributos morfológicos e contribuição da variação intraespecíficaAnálisesPara calcular contribuição relativa da variação intraespecífica em relação à variação interespecífica dentro de uma comunidade, por exemplo, Siefert et al. (2015) sugeriram uma métrica chamada de wITV (within-community Intraspecific Trait Variation). wITV representa razão da variação intraespecífica em relação variação total dentro de uma comunidade (e.g., parcela, poça) que inclui: ) abundância relativa de cada espécie j ocorrendo na comunidade , ii) o valor médio atributo da espécie j na comunidade , e iii) o valor atributo k de cada indivíduo da espécie j que ocorre na comunidade . Como esta medida é feita por unidade amostral (ou seja, sua comunidade de interesse), é possível testar hipóteses ecológicas que tentem explicar processos que aumentem ou diminuam variação de um determinado atributo dentro ou entre espécies diferentes. função wITV foi adaptada para linguagem R por de Bello et al. (2021). Para facilitar o cálculo wITV para cada comunidade, de Bello et al. (2021) executaram os códigos com função () que repete iterativamente análise para gerar os valores de todas comunidades em uma forma dinâmica. Após executar análises com o (), função salva os resultados dentro objeto wITVResults. Após obter esses resultados, é possível utilizar modelos lineares para testar quais variáveis preditoras (em nosso exemplo, características das poças) afetam o aumento ou diminuição da contribuição relativa da variação intraespecífica (Figura 14.11).\nFigura 14.11: Gráficos diagnósticos modelo linear ajustado para iWTV em função de características das poças.\nCombinando os resultados das três análises é possível compreender que existem diferenças morfológicas entre espécies de poças diferentes (componente substituição). Porém, é evidente que variação dentro da espécie é bastante relevante para compreender diversidade funcional de anuros. Na primeira análise, os resultados dessas quatro análises indicaram que variação intraespecífica explica de 17% 45% da variação morfológica nas metacomunidades de anuros. segunda, por sua vez, demonstra que variação morfológica entre espécies de poças diferentes representa o principal componente de variação, mas que variação intraespecífica não pode ser ignorada. Por fim, ao combinar métrica wITV com modelos lineares, percebe-se que características das poças não determinam contribuição da variação intraespecífica. Além disso, existe uma variação muito grande entre poças. Ao passo que em algumas poças variação intraespecífica não contribui para variação total (wITV = 0), em outras, este componente representou 100% da variação (wITV = 1). Os resultados obtidos nas análises das perguntas 1 3 indicam que utilizar somente média dos atributos morfológicos pode refletir em interpretações incorretas em estudos que compararam diversidade funcional espaço/tempo (veja discussão em Dalmolin, Tozetti, Pereira 2020).","code":"\n## Dados necessários\n# Matriz de traits.\nhead(traits)\n#>   pond Species body_size biomass eye_size leg_size flatness\n#> 1  DN1     Sp2     2.405   2.291    3.104    0.450    0.794\n#> 2  DN1     Sp3     1.882   2.039    2.926    0.345    1.063\n#> 3  DN1     Sp4     0.699   0.342    0.782    0.104    3.055\n#> 4  DN1     Sp4     0.725   0.598    1.120    0.136    2.759\n#> 5  DN1     Sp4     0.448   0.385    0.844    0.107    3.557\n#> 6  DN1     Sp4     0.640   0.470    0.861    0.093    3.420\n\n## Partição da variação intra e interespecífica\n## Passo 1: Tamanho corporal\nmod_body_size <- aov(body_size~Species, data = traits)\nsummary(mod_body_size)\n#>              Df Sum Sq Mean Sq F value Pr(>F)    \n#> Species      10  95.91   9.591    25.5 <2e-16 ***\n#> Residuals   195  73.35   0.376                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Contribuição da variação intra-específica para o tamanho corporal.\nitv_BS <- 100 * (73.35/(95.92 + 73.35))\nitv_BS\n#> [1] 43.33314\n\n## Passo 2: Biomassa\nmod_biomass <- aov(biomass~Species, data = traits)\nsummary(mod_biomass)\n#>              Df Sum Sq Mean Sq F value Pr(>F)    \n#> Species      10 118.17  11.817   23.95 <2e-16 ***\n#> Residuals   195  96.22   0.493                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Contribuição da variação intra-específica para a biomassa.\nitv_biomass <- 100 * (96.22/(118.17 + 96.22))\nitv_biomass\n#> [1] 44.88082\n\n## Passo 3: Tamanho do olho\nmod_eye_size <- aov(eye_size ~ Species, data = traits)\nsummary(mod_eye_size)\n#>              Df Sum Sq Mean Sq F value Pr(>F)    \n#> Species      10  203.1  20.309   50.51 <2e-16 ***\n#> Residuals   195   78.4   0.402                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Contribuição da variação intra-específica para o tamanho do olho.\nitv_eye_size <- 100 * (78.39/(203.09 + 78.39))\nitv_eye_size\n#> [1] 27.84923\n\n## Passo 4: Achatamento dorso-ventral\nmod_flatness <- aov(flatness~Species, data = traits)\nsummary(mod_flatness)\n#>              Df Sum Sq Mean Sq F value Pr(>F)    \n#> Species      10 104.47  10.447   92.07 <2e-16 ***\n#> Residuals   195  22.13   0.113                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Contribuição da variação intra-específica para o achatamento dorso-ventral.\nitv_flatness <- 100 * (22.13/(104.48 + 22.13))\nitv_flatness\n#> [1] 17.47887\n\n## Passo 5: Combinar os valores de cada trait em um vetor\nvalores <- c(itv_BS, itv_biomass, itv_eye_size, itv_flatness)\n\n# Passo 6: Combinar valores e traits em um data.frame.\nitv_results <- data.frame(trait = c(\"body_size\", \"biomass\", \"eye_size\", \"flatness\"),\n                          itv_explic = valores)\n\n## Tabela com resultados da explicação atribuida para a variação intraespecífica\nitv_results %>%\n    mutate(\"explained intraspecific variance\" = round(itv_explic, 2)) %>% \n    dplyr::select(trait, \"explained intraspecific variance\")\n#>       trait explained intraspecific variance\n#> 1 body_size                            43.33\n#> 2   biomass                            44.88\n#> 3  eye_size                            27.85\n#> 4  flatness                            17.48\n## Dados necessários\n# Matriz de traits sem nomes de espécies ou localidades\ntrait_m <- traits[, c(\"body_size\", \"biomass\", \"eye_size\", \"leg_size\", \"flatness\")]\nhead(trait_m)\n#>   body_size biomass eye_size leg_size flatness\n#> 1     2.405   2.291    3.104    0.450    0.794\n#> 2     1.882   2.039    2.926    0.345    1.063\n#> 3     0.699   0.342    0.782    0.104    3.055\n#> 4     0.725   0.598    1.120    0.136    2.759\n#> 5     0.448   0.385    0.844    0.107    3.557\n#> 6     0.640   0.470    0.861    0.093    3.420\ntrait_decomp <- decompCTRE(traits = trait_m, sp = traits$Species, \n                           ind.plot = traits$pond, print = FALSE)\nbarplot.decompCTRE(trait_decomp)\n## Dados necessários\n# Matriz de traits.\nhead(traits)\n#>   pond Species body_size biomass eye_size leg_size flatness\n#> 1  DN1     Sp2     2.405   2.291    3.104    0.450    0.794\n#> 2  DN1     Sp3     1.882   2.039    2.926    0.345    1.063\n#> 3  DN1     Sp4     0.699   0.342    0.782    0.104    3.055\n#> 4  DN1     Sp4     0.725   0.598    1.120    0.136    2.759\n#> 5  DN1     Sp4     0.448   0.385    0.844    0.107    3.557\n#> 6  DN1     Sp4     0.640   0.470    0.861    0.093    3.420\n\n# Matriz de comunidades e padronização para abundância relativa\nhead(anuros_comm)\n#>      Sp10 Sp11 Sp2 Sp3 Sp4 Sp6 Sp7 Sp8 Sp1 Sp9 Sp5\n#> DN1     1    8   1   1   6   8   4   0   0   0   0\n#> DN2     0    0   0   0   1   1   4   2   0   0   0\n#> DN3     1    0   0   0   0   0   0   0   0   0   0\n#> DN4     0    0   2   2   1   0   0   0   6   0   0\n#> DN5     0    0   3   0   1   0   4   1   0   0   0\n#> FIG1    0    0   1   0   0   0   1   0   0   0   0\nanuros_comm_rel <- decostand(anuros_comm, \"total\")\n\n# Variáveis ambientais.\nhead(env)\n#>      depth  area dits_bt_pond dist_for\n#> DN1   0.50  3800          115     2650\n#> DN2   0.60 54600          250     2500\n#> DN3   0.80 29110          150     1800\n#> DN4   1.00  1386          410      195\n#> DN5   1.00   590          770      100\n#> FIG1  0.15    30           25      135\n\n## Prearação da matriz para receber os resultados do `for`\nwITVResults <- data.frame(ITV = matrix(ncol = 1, nrow = length(unique(traits$pond))))\nrownames(wITVResults) <- unique(traits$pond)\n\nfor(i in 1:length(unique(traits$pond))){\n    commAux <- subset(traits, traits$pond == unique(traits$pond)[i])\n    commAux$Species <- droplevels(factor(commAux$Species))\n    spNames <- unique(commAux$Species)\n    relAbund <- anuros_comm_rel[i, as.character(spNames)] \n    traitsVector <- commAux$body_size\n    spVector <- commAux$Species\n    wITVResults[i, 1] <-  wITV(spIDs = spVector, traitVals = traitsVector, relAbund = relAbund)\n}\n\nwITVResults$ITV\n#>  [1] 0.82517670 0.23326457        NaN 0.15341806 0.10298952 0.00000000 0.02338235 0.68170997 0.79275763 0.66446945 0.81726278 1.00000000\n#> [13] 0.00000000 1.00000000 1.00000000 0.99220999 1.00000000 0.55519098 0.58945126 0.55148974 0.80178255 1.00000000 1.00000000        NaN\n#> [25] 0.19621528 0.14467854\nenv$wITV <- wITVResults$ITV # NaN = locais com uma única espécie\n\n## Remover NAs para executar o modelo linear\nenv2 <- na.omit(env)\nhead(env2)  \n#>      depth  area dits_bt_pond dist_for       wITV\n#> DN1   0.50  3800          115     2650 0.82517670\n#> DN2   0.60 54600          250     2500 0.23326457\n#> DN4   1.00  1386          410      195 0.15341806\n#> DN5   1.00   590          770      100 0.10298952\n#> FIG1  0.15    30           25      135 0.00000000\n#> FIG2  0.15    30           25      135 0.02338235\n\n## Modelo linear \nmod_itv <- lm(wITV ~ depth + area + dits_bt_pond + dist_for, data = env)\n\n## Testar pressuposto da análise\npar(mfrow = c(2, 2))\nplot(mod_itv)\n\n## Resultado\nsummary(mod_itv)\n#> \n#> Call:\n#> lm(formula = wITV ~ depth + area + dits_bt_pond + dist_for, data = env)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.6298 -0.3937  0.0448  0.3387  0.5266 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)   4.555e-01  2.113e-01   2.156   0.0449 *\n#> depth         2.767e-01  3.789e-01   0.730   0.4746  \n#> area         -3.116e-06  9.276e-06  -0.336   0.7409  \n#> dits_bt_pond -3.653e-04  6.193e-04  -0.590   0.5626  \n#> dist_for      5.698e-05  1.419e-04   0.401   0.6928  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.4103 on 18 degrees of freedom\n#>   (3 observations deleted due to missingness)\n#> Multiple R-squared:  0.03155,    Adjusted R-squared:  -0.1837 \n#> F-statistic: 0.1466 on 4 and 18 DF,  p-value: 0.9621"},{"path":"cap14.html","id":"para-se-aprofundar-10","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.7 Para se aprofundar","text":"","code":""},{"path":"cap14.html","id":"livros-9","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.7.1 Livros","text":"Recomendamos leitura dos livros: ) Garnier et al. (2016) Plant Functional Diversity: Organism Traits, Community Structure, Ecosystem Properties, e ii) de Bello et al. (2021) Handbook Trait-Based Ecology. Esses livros oferecem excelente oportunidade para se aprofundar campo teórico da diversidade funcional e, além disso, o livro liderado pelo pesquisador Francesco de Bello fornece diversas aplicações analíticas na linguagem R. Outro recurso excelente foi publicado por Mammola et al. (2021) Concepts Applications Functional Diversity e serve para se aprofundar nas diferentes medidas da diversidade funcional.","code":""},{"path":"cap14.html","id":"links-9","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.7.2 Links","text":"Um passo importante nos estudos de Diversidade Funcional é encontrar atributos funcionais dos organismos estudados. Abaixo indicamos uma lista de potenciais bases de dados:COMADRE - Animal Matrix DatabaseCOMPADRE - Plant Matrix DatabaseElton Traits 1.0TetraDENSITY - Density estimates terrestrial vertebratesTRY - Plant Trait DatabaseWorld Spider Trait","code":""},{"path":"cap14.html","id":"exercícios-10","chapter":"Capítulo 14 Diversidade Funcional","heading":"14.8 Exercícios","text":"14.1\nUtilize os dados aviurba pacote ade4 para testar o efeito de variáveis ambientais na dispersão (FDis) e regularidade funcional (FEve). Utilize modelos lineares (LM, veja Capítulo 7) para testar quais variáveis ambientais são mais importantes para dispersão e regularidade funcional. Além disso, faça um boxplot (veja Capítulo 6) comparando os valores de FDis e FEve entre categorias das variáveis ambientais mais relevantes.14.2\nUtilize os dados mafragh pacote ade4 para teste o efeito da das variáveis conductivity, silt e K2O na diversidade funcional (método de Petchey e Gaston). Utilize modelos lineares (regressão múltipla, veja Capítulo 7) para testar relação entre essas variáveis e discuta: () qual variável mais importante (se houver) e (b) se conclusões são coerentes tendo como base os pressupostos dos modelos lineares. Além disso, caso exista alguma relação significativa, faça um gráfico (scatterplot, veja Capítulo 6) da relação da variável mais importante e diversidade funcional.14.3\nUtilize os dados mafragh pacote ade4 para comparar composição filogenética e funcional em áreas com alta e baixa concentração de potássio. Para fazer esta comparação será necessário transformar matriz de atributos funcionais e árvore filogenética em matrizes de distância e, depois, utilizar o CWM para criar uma matriz de localidades por composição funcional ou filogenética. Depois, você poderá usar matriz CWM para testar potenciais diferenças entre concentrações com PERMANOVA e para visualizar com PCoA.Soluções dos exercícios.","code":""},{"path":"cap15.html","id":"cap15","chapter":"Capítulo 15 Dados geoespaciais","heading":"Capítulo 15 Dados geoespaciais","text":"","code":""},{"path":"cap15.html","id":"pré-requisitos-do-capítulo-11","chapter":"Capítulo 15 Dados geoespaciais","heading":"Pré-requisitos do capítulo","text":"Pacotes e dados que serão utilizados neste capítulo.","code":"\n## Pacotes\nlibrary(ecodados)\nlibrary(here)\nlibrary(tidyverse)\nlibrary(sf) \nlibrary(raster) \nlibrary(rgdal) \nlibrary(spData)\nlibrary(rnaturalearth)\nlibrary(geobr)\nlibrary(ggplot2)\nlibrary(ggspatial)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(grid)\nlibrary(mapview)\nlibrary(leaflet)\nlibrary(viridis)\nlibrary(knitr)\nlibrary(sidrar)\nlibrary(landscapetools)\n\n## Dados\n# world <- world\n# volcano <- volcano\n# geo_anfibios_locais <- ecodados::geo_anfibios_locais\n# geo_anfibios_especies <- ecodados::geo_anfibios_especies\n# geo_vetor_nascentes <- ecodados::geo_vetor_nascentes\n# geo_vetor_hidrografia <- ecodados::geo_vetor_hidrografia\n# geo_vetor_cobertura <- ecodados::geo_vetor_cobertura\n# geo_vetor_rio_claro <- ecodados::geo_vetor_rio_claro\n# geo_vetor_brasil <- ecodados::geo_vetor_brasil\n# geo_vetor_brasil_anos <- ecodados::geo_vetor_brasil_anos\n# geo_vetor_am_sul <- ecodados::geo_vetor_am_sul\n# geo_vetor_biomas <- ecodados::geo_vetor_biomas\n# geo_vetor_mata_atlantica <- ecodados::geo_vetor_mata_atlantica\n# geo_raster_srtm <- ecodados::geo_raster_srtm\n# geo_raster_bioclim <- ecodados::geo_raster_bioclim\n# geo_raster_globcover_mata_atlantica <- ecodados::geo_raster_globcover_mata_atlantica"},{"path":"cap15.html","id":"contextualização-3","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.1 Contextualização","text":"Nesta seção, vamos fazer uma breve introdução aos principais conceitos sobre manipulação e visualização de dados geoespaciais R. Iremos abordar temas de forma teórica e prática, utilizando linguagem R, focando em: ) formatos de dados vetoriais e dados raster, ii) Sistemas de Referências de Coordenadas e unidades (geográficas e projetadas), iii) fontes de dados, iv) importar e exportar dados, v) descrição de objetos geoespaciais e vi) principais operações (atributos, espaciais e geométricas). Num segundo momento, criaremos mapas com seus principais elementos como mapas principal e secundário, título, legenda, barra de escala, indicador de orientação (Norte), gride de coordenadas, descrição Sistema de Referência de Coordenadas e informações de origem dos dados. Por fim, apresentaremos exemplos de aplicações de análises geoespaciais para dados ecológicos, focadas em: ) agregar informações sobre biodiversidade, ii) preparar dados para compor variáveis preditoras, e iii) como fazer predições espaciais de distribuição de uma espécie e riqueza de espécies.Esse capítulo segue parte da estrutura organizada por Lovelace et al. (2019), principalmente os Capítulos 2 8, sendo adaptado para atender aos principais requisitos que julgamos necessários estudos ecológicos. Entretanto, não foi possível cobrir todos os assuntos sobre o uso de dados geoespaciais R, sendo um tema muito extenso que requer leitura de livros especializados na área como: ) Mas et al. (2019) Análise espacial com R, ii) Wegmann, Leutner & Dech (2016) Remote Sensing GIS Ecologists: Using Open Source Software, iii) Wegmann, Schwalb-Willmann & Dech (2020) Introduction Spatial Data Analysis Remote Sensing GIS Open Source Software, e iv) Fletcher & Fortin (2018) Spatial ecology conservation modeling: Applications R. Outros livros sobre análise geoespacial R podem ser consultados Capítulo 11 - Geospatial Big Book R.","code":""},{"path":"cap15.html","id":"vetor","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.2 Vetor","text":"Dados vetoriais são usados para mapear fenômenos ou objetos espacialmente explícitos que possuem localização ou dimensões bem definidas, representado partir de formas geométricas (como pontos, linhas e polígonos) e possuem possibilidade de ter associado eles informações tabulares. tabela de atributos é uma tabela que inclui dados geoespaciais e dados alfanuméricos. Os dados geoespaciais são representados por feições geolocalizadas espacialmente (ponto, linha ou polígono), e os dados alfanuméricos (tabela de dados). Dessa forma, tabela de atributos reúne informações sobre cada feição e pode ser utilizada para realizar filtros ou agregações dos dados de cada feição (Figura 15.1).\nFigura 15.1: Representação das geometrias de ponto, linha e polígono e atributos. Adaptado de: Olaya (2020).\n","code":""},{"path":"cap15.html","id":"sf-principal-pacote-no-r-para-dados-vetoriais","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.2.1 sf: principal pacote no R para dados vetoriais","text":"Atualmente o principal pacote para trabalhar com dados vetoriais R é o sf, que implementou o Simple Feature (E. Pebesma 2018). Entretanto, outro pacote pode ser tão versátil quanto o sf, caso o terra, com algumas mudanças na sintaxe que não abordaremos nesse livro por questões de redução de espaço.Os tipos de geometrias apresentadas são representados por diferentes classes: POINT, LINESTRING e POLYGON para apenas uma feição de cada tipo de geometria; MULTIPOINT, MULTILINESTRING e MULTIPOLYGON para várias feições de cada tipo de geometria e; GEOMETRYCOLLECTION para várias feições e tipos de geometrias e classes.Ao olharmos informações de um objeto da classe sf, podemos notar diversas informações que descrevem o mesmo, numa espécie de cabeçalho:resumo vetor: indica o número de feições (linhas) e campos (colunas)tipo da geometria: umas das sete classes (ou mais outras) listadas anteriormentedimensão: número de dimensões, geralmente duas (XY)bbox (bordas): coordenadas mínimas e máximas da longitude e latitudeinformação CRS: epsg ou proj4string indicando o CRS (Coordinate Reference System)tibble: tabela de atributos, com destaque para coluna geom ou geometry que representa cada feição ou geometriaPodemos fazer um mapa simples utilizando função plot() desse objeto. Para facilitar, escolheremos apenas primeira coluna [1] (Figura 15.2). Caso não escolhermos apenas uma coluna, um mapa para cada coluna será plotado. 📝 Importante \nFaremos mapas mais elaborados na seção de visualização de dados geoespaciais deste capítulo.\nFigura 15.2: Mapa vetorial mundo.\n","code":"\n## Dados vetoriais de polígonos do mundo\ndata(world)\nworld\n#> Simple feature collection with 177 features and 10 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\n#> Geodetic CRS:  WGS 84\n#> # A tibble: 177 × 11\n#>    iso_a2 name_long        continent     region_un subregion          type            area_km2     pop lifeExp gdpPercap                      geom\n#>  * <chr>  <chr>            <chr>         <chr>     <chr>              <chr>              <dbl>   <dbl>   <dbl>     <dbl>        <MULTIPOLYGON [°]>\n#>  1 FJ     Fiji             Oceania       Oceania   Melanesia          Sovereign coun…   1.93e4  8.86e5    70.0     8222. (((-180 -16.55522, -179.…\n#>  2 TZ     Tanzania         Africa        Africa    Eastern Africa     Sovereign coun…   9.33e5  5.22e7    64.2     2402. (((33.90371 -0.95, 31.86…\n#>  3 EH     Western Sahara   Africa        Africa    Northern Africa    Indeterminate     9.63e4 NA         NA         NA  (((-8.66559 27.65643, -8…\n#>  4 CA     Canada           North America Americas  Northern America   Sovereign coun…   1.00e7  3.55e7    82.0    43079. (((-132.71 54.04001, -13…\n#>  5 US     United States    North America Americas  Northern America   Country           9.51e6  3.19e8    78.8    51922. (((-171.7317 63.78252, -…\n#>  6 KZ     Kazakhstan       Asia          Asia      Central Asia       Sovereign coun…   2.73e6  1.73e7    71.6    23587. (((87.35997 49.21498, 86…\n#>  7 UZ     Uzbekistan       Asia          Asia      Central Asia       Sovereign coun…   4.61e5  3.08e7    71.0     5371. (((55.96819 41.30864, 57…\n#>  8 PG     Papua New Guinea Oceania       Oceania   Melanesia          Sovereign coun…   4.65e5  7.76e6    65.2     3709. (((141.0002 -2.600151, 1…\n#>  9 ID     Indonesia        Asia          Asia      South-Eastern Asia Sovereign coun…   1.82e6  2.55e8    68.9    10003. (((104.37 -1.084843, 104…\n#> 10 AR     Argentina        South America Americas  South America      Sovereign coun…   2.78e6  4.30e7    76.3    18798. (((-68.63401 -52.63637, …\n#> # … with 167 more rows\n## Plot dos polígonos do mundo\nplot(world[1], col = viridis::viridis(100), main = \"Mapa do mundo\")"},{"path":"cap15.html","id":"raster","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.3 Raster","text":"Os dados formato raster consistem em uma matriz (com linhas e colunas) em que os elementos representam células, geralmente igualmente espaçadas (pixels; Figura 15.3). células dos dados raster possuem duas informações: ) identificação das células (IDs das células) para especificar sua posição na matriz (Figura 15.3 ) e; ii) valores das células (Figura 15.3 B), que geralmente são coloridos para facilitar interpretação da variação dos valores espaço (Figura 15.3 C). Além disso, valores ausentes ou não amostrados são representados por NA, ou seja, available (Figura 15.3 B e C).\nFigura 15.3: Raster: () IDs das células, (B) valores das células, (C) células coloridas. Adaptado de: Lovelace et al. (2019).\nPodemos ainda fazer uma comparação com representações de dados vetoriais vistos na Figura 15.1, mas agora formato raster (Figura 15.4).\nFigura 15.4: Representação das geometrias de ponto, linha e polígono formato raster. Adaptado de: Olaya (2020).\n","code":""},{"path":"cap15.html","id":"raster-principal-pacote-no-r-para-dados-raster","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.3.1 raster: principal pacote no R para dados raster","text":"Atualmente, o principal pacote para trabalhar com dados raster é o raster, apesar de existir outros dois: terra e stars, com algumas mudanças na sintaxe que não abordaremos neste livro.O pacote raster fornece uma ampla gama de funções para criar, importar, exportar, manipular e processar dados raster R. O objeto raster criado à partir pacote raster pode assumir três classes: RasterLayer, RasterStack e RasterBrick.classe RasterLayer representa apenas uma camada raster. Para criar ou importar um raster R podemos utilizar função raster::raster(). Observando essa classe, podemos notar seguintes informações:class: classe raster objeto rasterdimensions: número de linhas, colunas e célulasresolution: largura e altura da célulaextent: coordenadas mínimas e máximas da longitude e latitudecrs: Sistema de Referência de Coordenadas (CRS)source: fonte dos dados (memória ou disco)names: nome das camadasvalues: valores máximos e mínimos das célulasVamos utilizar os dados volcano, que possui informações topográficas (elevação) vulcão Maunga Whau de Auckland na Nova Zelândia.Vamos transformar essa matriz de dados em um raster com função raster::raster().Um mapa simples objeto raster pode ser obtido utilizando função plot(), próprio pacote raster (Figura 15.5).\nFigura 15.5: Mapa simples de um RasterLayer.\nAlém da classe RasterLayer, há mais duas classes que trabalham com múltiplas camadas: RasterBrick e RasterStack. Elas diferem em relação ao formato dos arquivos suportados, tipo de representação interna e velocidade de processamento.classe RasterBrick geralmente corresponde à importação de um único arquivo de imagem de satélite multiespectral (multicamadas) ou um único objeto com várias camadas na memória. função raster::brick() cria um objeto RasterBrick.Ao utilizarmos função plot() pacote raster, podemos visualizar os raster contidos objeto RasterBrick (Figura 15.6).\nFigura 15.6: Mapas simples de um raster RasterBrick.\nJá classe RasterStack permite conectar vários objetos raster armazenados em arquivos diferentes ou vários objetos ambiente R. Um RasterStack é uma lista de objetos RasterLayer com mesma extensão, resolução e CRS. Uma maneira de criá-lo é com junção de vários objetos geoespaciais já existentes ambiente R ou listar vários arquivos raster em um diretório armazenado disco. função raster::stack() cria um objeto RasterStack.Outra diferença é que o tempo de processamento, para objetos RasterBrick geralmente é menor que para objetos RasterStack. decisão sobre qual classe Raster deve ser usada depende principalmente caráter dos dados de entrada.Da mesma forma, ao utilizar função plot() pacote raster, podemos visualizar os raster contidos objeto RasterStack (Figura 15.7).\nFigura 15.7: Mapas simples de um raster RasterStack.\n","code":"\n## Dados de altitude de um vulcão\nvolcano[1:5, 1:5]\n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]  100  100  101  101  101\n#> [2,]  101  101  102  102  102\n#> [3,]  102  102  103  103  103\n#> [4,]  103  103  104  104  104\n#> [5,]  104  104  105  105  105\n## Rasterlayer\nraster_layer <- raster::raster(volcano)\nraster_layer\n#> class      : RasterLayer \n#> dimensions : 87, 61, 5307  (nrow, ncol, ncell)\n#> resolution : 0.01639344, 0.01149425  (x, y)\n#> extent     : 0, 1, 0, 1  (xmin, xmax, ymin, ymax)\n#> crs        : NA \n#> source     : memory\n#> names      : layer \n#> values     : 94, 195  (min, max)\n## Plot raster layers\nplot(raster_layer, col = viridis::viridis(n = 100))\n## Raster layers\nraster_layer1 <- raster_layer\nraster_layer2 <- raster_layer * raster_layer\nraster_layer3 <- sqrt(raster_layer)\nraster_layer4 <- log10(raster_layer)\n\n## Raster brick\nraster_brick <- raster::brick(raster_layer1, raster_layer2, \n                              raster_layer3, raster_layer4)\nraster_brick\n#> class      : RasterBrick \n#> dimensions : 87, 61, 5307, 4  (nrow, ncol, ncell, nlayers)\n#> resolution : 0.01639344, 0.01149425  (x, y)\n#> extent     : 0, 1, 0, 1  (xmin, xmax, ymin, ymax)\n#> crs        : NA \n#> source     : memory\n#> names      :      layer.1,      layer.2,      layer.3,      layer.4 \n#> min values :    94.000000,  8836.000000,     9.695360,     1.973128 \n#> max values :   195.000000, 38025.000000,    13.964240,     2.290035\n## Plot raster brick\nplot(raster_brick, col = viridis::viridis(n = 25), main = \"\")\n## Raster layers\nraster_layer1 <- raster_layer\nraster_layer2 <- raster_layer * raster_layer\nraster_layer3 <- sqrt(raster_layer)\nraster_layer4 <- log10(raster_layer)\n\n## Raster stack\nraster_stack <- raster::stack(raster_layer1, raster_layer2, \n                              raster_layer3, raster_layer4)\nraster_stack\n#> class      : RasterStack \n#> dimensions : 87, 61, 5307, 4  (nrow, ncol, ncell, nlayers)\n#> resolution : 0.01639344, 0.01149425  (x, y)\n#> extent     : 0, 1, 0, 1  (xmin, xmax, ymin, ymax)\n#> crs        : NA \n#> names      :      layer.1,      layer.2,      layer.3,      layer.4 \n#> min values :    94.000000,  8836.000000,     9.695360,     1.973128 \n#> max values :   195.000000, 38025.000000,    13.964240,     2.290035\n## Plot raster stack\nplot(raster_stack, col = viridis::viridis(n = 25), main = \"\")"},{"path":"cap15.html","id":"sistema-de-referência-de-coordenadas-e-unidades","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.4 Sistema de Referência de Coordenadas e Unidades","text":"Os dados geoespaciais (vetor e raster) possuem ainda um outro componente fundamental que é o Sistema de Referência de Coordenadas, ou inglês Coordinate Reference System (CRS). Esse componente define referência espacial dos elementos geoespaciais (vetor e raster) na superfície da Terra. Ele é composto por dois principais conceitos: primeiro, que tipos de unidades estão sendo utilizadas para representação geográfica, podendo assumir dois tipos - ângulos ou metros, que definem o Sistema de Coordenadas Geográficas e o Sistema de Coordenadas Projetadas, respectivamente. O segundo componente é o datum, que é relação sistema de coordenadas (geográfica ou projetada) com superfície da Terra. Esse último componente faz parte de uma área da Cartografia denominada Geodésia que estuda forma e dimensões da Terra, campo gravitacional e localização de pontos fixos e sistemas de coordenadas. O livro de Lapaine & Usery (2017) é um excelente material para se aprofundar nesse assunto.","code":""},{"path":"cap15.html","id":"sistema-de-coordenadas-geográficas","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.4.1 Sistema de Coordenadas Geográficas","text":"O Sistema de Coordenadas Geográficas utiliza ângulos (graus) para representar feições na superfície da Terra através de dois valores: longitude e latitude. longitude representa o eixo Leste-Oeste e latitude o eixo Norte-Sul. Nesse sistema, superfície da Terra é representada geralmente por uma superfície elipsoidal, pois Terra é ligeiramente achatada nos polos devido ao movimento de rotação.","code":""},{"path":"cap15.html","id":"sistema-de-coordenadas-projetadas","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.4.2 Sistema de Coordenadas Projetadas","text":"O Sistema de Coordenadas Projetadas utiliza um Sistema Cartesiano de Coordenadas em uma superfície plana. Dessa forma, partir de uma origem traçam-se eixos X e Y e uma unidade linear é utilizada, como o metro. Todos projeções feitas de sistemas geoespaciais convertem uma superfície tridimensional em uma superfície plana bidimensional. Sendo assim, essa conversão traz consigo algum tipo de distorção em relação à porção real, podendo ser distorções em: ) formas locais, ii) áreas, iii) distâncias, iv) flexão ou curvatura, v) assimetria ou vi) lacunas de continuidade. Dessa forma, um sistema de coordenadas projetadas pode preservar somente uma ou duas dessas propriedades.Existem três grandes grupos de projeções: ) cilíndricos, ii) cônicos e iii) planares. Na projeção cilíndrica, superfície da Terra é mapeada em um cilindro, criada tocando superfície da Terra ao longo de uma ou duas linhas de tangência, sendo utilizada com mais frequência para mapear todo o globo tendo como exemplo mais conhecido Projeção Universal Transversa de Mercator (UTM). Na projeção cônica, superfície da Terra é projetada em um cone ao longo de uma linha ou duas linhas de tangência, de modo que distorções são minimizadas ao longo das linhas e aumentam com distância das mesmas sendo, portanto, mais adequada para mapear áreas de latitudes médias, tendo como exemplo mais conhecido Projeção Cônica Equivalente de Albers e Projeção Cônica Conforme de Lambert. E na projeção plana, também denominada Projeção Azimutal, o mapeamento toca o globo em um ponto ou ao longo de uma linha de tangência, sendo normalmente utilizado mapeamento de regiões polares, sendo mais comum Projeção Azimutal Equidistante, mesma utilizada na bandeira da ONU.","code":""},{"path":"cap15.html","id":"datum","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.4.3 Datum","text":"Como dito anteriormente, o datum é relação sistema de coordenadas com superfície da Terra. Ele representa o ponto de intersecção elipsoide de referência com superfície da Terra (geoide, forma verdadeira da Terra), compensando diferenças campo gravitacional da Terra. Existem dois tipos de datum: ) local e ii) geocêntrico. Em um datum local, como o SAD69 - South American Datum 1969, o elipsoide de referência é deslocado para se alinhar com superfície em um determinado local, por exemplo, na América Sul. Já em um datum geocêntrico, como WGS84 - World Geodetic System 1984, o centro elipsoide é o centro de gravidade da Terra e precisão das projeções não é otimizada para um local específico globo.Brasil, desde 2015, o Instituto Brasileiro de Geografia e Estatística (IBGE) ajudou desenvolver e reafirmou o uso datum SIRGAS2000 - Sistema de Referencia Geocéntrico para las Américas 2000 para todos os mapeamentos realizados Brasil, um esforço conjunto para adotar o mesmo datum em toda América. Mais sobre esse datum pode ser lido aqui: SIRGAS2000.","code":""},{"path":"cap15.html","id":"sistema-de-referência-de-coordenadas-crs-no-r","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.4.4 Sistema de Referência de Coordenadas (CRS) no R","text":"R, há duas formas principais de representar um Sistema de Referência de Coordenadas: ) código epsg e ii) proj4string. O código EPSG (European Petroleum Survey Group) é uma sequência de números curta, referindo-se apenas um CRS. O site epsg.io permite consultar diversas informações como procurar por um código, representação de mapas e fazer transformações de CRS.Já o proj4string permite mais flexibilidade para especificar diferentes parâmetros, como o tipo de projeção, datum e elipsoide. Dessa forma, é possível especificar muitas projeções, ou mesmo modificar projeções existentes, tornando representação proj4string mais complexa e flexível.Além disso, ainda é possível consultar uma extensa lista de CRSs site spatialreference.org, que fornece descrições em diversos formatos, baseados em GDAL e Proj.4. Essa abordagem permite consultar uma URL que pode produzir uma referência espacial em um formato que seu software SIG ou o R pode utilizar como referência.Os pacotes (geo)espaciais R suportam uma ampla variedade de CRSs e usam biblioteca PROJ. função rgdal::make_EPSG() retorna um data frame das projeções disponíveis, com informações dos códigos epsg e proj4string numa mesma tabela, facilitando busca e uso de CRSs (Tabela 15.1).\nTabela 15.1: Listagem de Sistemas de Referências de Coordenadas disponíveis R, com informações dos códigos epsg e proj4string\n","code":"\n## Listagem dos Sistemas de Referências de Coordenadas no R\ncrs_data <- rgdal::make_EPSG()\nhead(crs_data)"},{"path":"cap15.html","id":"principais-fontes-de-dados-geoespaciais","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.5 Principais fontes de dados geoespaciais","text":"Existem diversas fontes de dados geoespaciais em diferentes bases de dados disponíveis gratuitamente. Geralmente essas bases de dados são disponibilizadas separadamente em apenas dados vetoriais e dados raster. Para dados vetoriais, grande parte dos dados disponibilizados são utilizados em mapas como limites políticos, limites de biomas ou distribuição de espécies para polígonos; estradas e rios para dados lineares, ou ainda pontos de ocorrência de espécies ou comunidades, ou medidas tomadas em campo sobre condições naturais como clima ou relevo, como pontos. Entretanto, é sempre recomendado o uso de bases oficiais, principalmente em relação dados vetoriais de limites políticos. Para tanto, é fundamental buscar bases oficiais de cada país, entretanto, há bases que podem ser utilizadas globalmente, como veremos.Sobre bases de dados raster, há uma infinidade de dados para diferentes objetivos, mas grande parte deles são relativos condições ambientais, representando uma variável de interesse de forma contínua espaço, como temperatura, precipitação, elevação, etc.Há uma compilação de dados geoespaciais vetoriais e raster feita por Marcus Vinícius Alves de Carvalho e Angelica Carvalho Di Maio, chamada GeoLISTA. Entretanto, como bases de dados tendem ser muito dinâmicas, é possível que muitas bases tenham surgido e desaparecido desde listagem realizada.Além das bases de dados, há pacotes específicos R que fazem o download de dados vetoriais e rasters, facilitando aquisição e reprodutibilidade. Para conferir uma listagem completa de pacotes para diversas análises espaciais, veja CRAN Task View: Analysis Spatial Data.","code":""},{"path":"cap15.html","id":"vetor-1","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.5.1 Vetor","text":"Dentre bases vetoriais, destacamos seguintes na Tabela 15.2.\nTabela 15.2: Principais bases de dados vetoriais para o Brasil e o Mundo.\n","code":""},{"path":"cap15.html","id":"raster-1","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.5.2 Raster","text":"Dentre bases raster, destacamos seguintes na Tabela 15.3.\nTabela 15.3: Principais bases de dados raster para o Brasil e o Mundo.\n","code":""},{"path":"cap15.html","id":"pacotes-do-r","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.5.3 Pacotes do R","text":"Dentre os pacotes R para download de dados geoespaciais, destacamos os seguintes na Tabela 15.4.\nTabela 15.4: Principais pacotes R para download de dados vetoriais e raster.\n","code":""},{"path":"cap15.html","id":"importar-e-exportar-dados-geoespaciais","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.6 Importar e exportar dados geoespaciais","text":"Agora que sabemos o que são dados geoespaciais e em quais bases de dados podemos buscar e baixar esses dados, veremos seus principais formatos e como importá-los e exportá-los R.","code":""},{"path":"cap15.html","id":"principais-formatos-de-arquivos-geoespaciais","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.6.1 Principais formatos de arquivos geoespaciais","text":"Há diversos formatos de arquivos geoespaciais, alguns específicos para dados vetoriais e raster, e outros formato de banco de dados geoespaciais, como PostGIS, que podem armazenar ambos os formatos.Entretanto, todos os formatos para serem importados para o R usam o GDAL (Geospatial Data Abstraction Library), uma interface unificada para leitura e escrita de diversos formatos de arquivos geoespaciais, sendo utilizado também por uma série de softwares de GIS como QGIS, GRASS GIS e ArcGIS.Dentre esses formatos, destacamos os seguintes na Tabela 15.5.\nTabela 15.5: Principais formatos de arquivos geoespaciais. Adaptado de: Lovelace et al. (2019).\nO formato mais comum para arquivos vetoriais é o ESRI Shapefile; para arquivos raster é o GeoTIFF; e para dados climáticos em múltiplas camadas, geralmente há disponibilização de dados formato NetCDF. Entretanto, recentemente tivemos o surgimento GeoPackage, que possui diversas vantagens em relação aos formatos anteriores, podendo armazenar em apenas um arquivo, dados formato vetorial, raster e também dados não-espaciais (e.g., tabelas), além de possuir uma grande integração com diversos softwares e bancos de dados.","code":""},{"path":"cap15.html","id":"importar-dados-1","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.6.2 Importar dados","text":"principais funções para importar dados R são: ) para vetores função sf::st_read(), e ii) para raster função raster::raster() e suas variações raster::brick() e raster::stack() para múltiplas camadas. Essas funções atribuem objetos ao seu espaço de trabalho, armazenando-os na memória RAM disponível em seu hardware, sendo essa maior limitação para trabalhar com dados geoespaciais R. Por exemplo, se um arquivo raster possui mais de 8 Gb de tamanho, e seu computador possui exatamente 8 Gb de RAM, é muito provável que ele não seja importado ou mesmo criado como um objeto dentro ambiente R. Existem soluções para esses problemas, mas não abordaremos neste capítulo.VetorComo vimos, os arquivos vetoriais são disponibilizados em diversos formatos. Para sabermos se um determinado formato pode ser importado ou exportado utilizando o pacote sf, podemos utilizar função sf::st_drivers(). Uma amostra desses formatos é apresentado na Tabela 15.6.\nTabela 15.6: Alguns formatos vetoriais importados e exportados pelo pacote sf.\nImportar dados vetoriais existentesPara importar vetores existentes para o R, utilizaremos função sf::st_read(). estrutura é semelhante para todos os formatos descritos na Tabela 15.6, de modo que sempre preencheremos o argumento dsn (data source name) com o nome arquivo ser importado. Entretanto, para banco de dados, como GeoPackage, pode ser necessário especificar camada que se tem interesse com um segundo argumento chamado layer, com o nome da camada.Para quase todas operações vetoriais nesse capítulo, usaremos os dados disponíveis para o município de Rio Claro/SP. Primeiramente, baixaremos esses dados da FBDS (Fundação Brasileira para o Desenvolvimento Sustentável), através desse repositório de dados. Em 2013, FBDS deu início ao Projeto de Mapeamento em Alta Resolução dos Biomas Brasileiros, mapeando cobertura da terra, hidrografia (nascentes, rios e lagos) e Áreas de Preservação Permanente (APPs). O mapeamento foi concluído para os municípios dos Biomas Mata Atlântica e Cerrado, e mais recentemente para os outros biomas. Para fazer o download dos arquivos de interesse, utilizaremos o R, através da função download.file().Primeiramente, criaremos um diretório com função dir.create(), usando função ::() para indicar o repositório (ver o Capítulo 5).Em seguida, vamos fazer o download de pontos de nascentes, linhas de hidrografia e polígonos de cobertura da terra para o município de Rio Claro/SP.Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.Agora podemos importar esses dados para o R. Primeiro vamos importar nascentes (Figura 15.8).\nFigura 15.8: Mapa de nascentes de Rio Claro/SP.\nAgora vamos importar hidrografia (Figura 15.9).\nFigura 15.9: Mapa da hidrografia de Rio Claro/SP.\nE por fim, vamos importar cobertura da terra (Figura 15.10).\nFigura 15.10: Mapa de cobertura da terra de Rio Claro/SP.\nImportar utilizando pacotesAlém de dados existentes, podemos importar dados vetoriais de pacotes, como listado anteriormente na Tabela 15.4. Para o Brasil, o pacote mais interessante trata-se geobr, Instituto de Pesquisa Econômica Aplicada (IPEA), que possui dados oficiais Instituto Brasileiro de Geografia e Estatística (IBGE).É possível listar todos os dados disponíveis pacote através da função geobr::list_geobr(). Na Tabela 15.7 é possível ver alguns desses dados.\nTabela 15.7: Alguns dados disponíveis pacote geobr.\nComo exemplo, vamos fazer o download o limite município de Rio Claro/SP, utilizando o código município (3543907) (Figura 15.11). 📝 Importante \nPara saber todos os códigos dos municípios Brasil, recomendamos verificação site IBGE.Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.\nFigura 15.11: Limite município de Rio Claro/SP.\nJá para o mundo, o pacote mais interessante trata-se rnaturalearth, que faz o download de dados Natural Earth. Vamos fazer o download limite Brasil (Figura 15.12).Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.\nFigura 15.12: Limite Brasil.\nCriar um objeto espacial de uma tabela de coordenadasÉ muito comum em coletas de campo ou bases de dados, ter coordenadas de locais de estudo ou de ocorrências de espécies organizadas em tabelas. Essas tabelas devem possuir duas colunas: longitude e latitude, ou X e Y para dados UTM, por exemplo. Ao importá-las para o R, o formato que assumem pode ser de uma das classes: matrix, data frame ou tibble, ou seja, ainda não são da classe vetorial sf. Nesta seção iremos ver como fazer essa conversão.Para tanto, vamos usar os dados de comunidades de anfíbios da Mata Atlântica (Atlantic Amphibians, Vancine et al. (2018)). Faremos o download diretamente site da fonte dos dados. Antes vamos criar um diretório.Em seguida, vamos fazer o download de um arquivo .zip e vamos extrair usando função unzip() nesse mesmo diretório.Agora podemos importar tabela de dados com função readr::read_csv().Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.Por fim, podemos facilmente criar um objeto espacial tipo MULTIPOINT utilizando função sf::st_as_sf(). Podemos ver essas coordenadas plotadas mapa simples da Figura 15.13 (Vancine et al. 2018).É necessário antes se ater ao argumento coords que deve indicar colunas de longitude e latitude, nessa ordem; e também ao argumento crs para indicar o CRS correspondente dessas coordenadas, que aqui sabemos se tratar de coordenadas geográficas e datum WGS84. Então podemos facilmente utilizar o código EPSG 4326. Entretanto, se coordenadas estiverem em metros, por exemplo, teremos de nos ater qual CRS mesmas foram coletadas, ou seja, se forem coordenadas de GPS, é preciso saber como o GPS estava configurado (projeção e datum).\nFigura 15.13: Coordenadas das comunidades Atlantic Amphibians.\nConverter dados espaciais sp para sfO pacote sf é mais recente e mais fácil de manipular objetos vetoriais R. Seu predecessor, o pacote sp possui uma classe própria e homônima. Entretanto, muitos pacotes de análises espaciais ainda utilizam essa classe em suas funções, apesar dessa migração ter ocorrido rapidamente recentemente. Dessa forma, conversão entre essas classes pode ser necessária em alguns momentos.Abaixo, veremos como podemos fazer essa conversão facilmente. Primeiramente, vamos importar dados sp.Agora, podemos converter facilmente com função sf::st_as_sf().Podemos facilmente converter esse objeto novamente para classe sp com função sf::as_Spatial().RasterPara importar dados raster R, utilizaremos função raster::raster(), raster::brick() ou raster::stack(). Para apenas uma camada raster, usaremos função raster::raster(), com o argumento x sendo o nome arquivo. Já para mais camadas, usaremos raster::brick() para um arquivo que possua múltiplas camadas, ou ainda função raster::stack() para vários arquivos em diferentes camadas também argumento x, sendo necessário listar os arquivos diretório, geralmente utilizando função dir() ou list.files(). Entretanto, para especificar uma camada, podemos utilizar o argumento band ou layer e o nome dessa camada.Raster LayerPrimeiramente, vamos criar um diretório para os dados raster que fazeremos o download.Em seguida, vamos fazer o download de dados de elevação, na verdade dados de Modelo Digital de Elevação (Digital Elevation Model - DEM), localizados também para o município de Rio Claro. Utilizaremos os dados Shuttle Radar Topography Mission - SRTM. Para saber mais sobre esses dados, recomendamos leitura artigo de Farr et al. (2007).Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.Agora podemos importar essa camada para o R, e visualizá-la em relação ao limite município de Rio Claro/SP (Figura 15.14).\nFigura 15.14: Camada raster DEM em relação ao limite município de Rio Claro/SP.\nRaster StackAlém dos dados de elevação, dados de temperatura e precipitação podem ser obtidos WorldClim. Para saber mais sobre esses dados, recomendamos leitura artigo Fick & Hijmans (2017).Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.Para importar essa série de camadas, primeiramente listaremos os arquivos e depois importaremos formato RasterStack (Figura 15.15).\nFigura 15.15: Camadas rasters WorldClim (BIO01 e BIO12) para o mundo.\n","code":"\n## Formatos vetoriais importados e exportados pelo pacote sf\nhead(sf::st_drivers())\n## Criar diretório\ndir.create(here::here(\"dados\"))\ndir.create(here::here(\"dados\", \"vetor\"))\n## Aumentar o tempo de download\noptions(timeout = 1e3)\n\n## Download\nfor(i in c(\".dbf\", \".prj\", \".shp\", \".shx\")){\n    \n    # Pontos de nascentes\n    download.file(\n        url = paste0(\"http://geo.fbds.org.br/SP/RIO_CLARO/HIDROGRAFIA/SP_3543907_NASCENTES\", i),\n        destfile = here::here(\"dados\", \"vetor\", paste0(\"SP_3543907_NASCENTES\", i)), mode = \"wb\")\n    \n    # Linhas de hidrografia\n    download.file(\n        url = paste0(\"http://geo.fbds.org.br/SP/RIO_CLARO/HIDROGRAFIA/SP_3543907_RIOS_SIMPLES\", i),\n        destfile = here::here(\"dados\", \"vetor\", paste0(\"SP_3543907_RIOS_SIMPLES\", i)), mode = \"wb\")\n    \n    # Polígonos de cobertura da terra\n    download.file(\n        url = paste0(\"http://geo.fbds.org.br/SP/RIO_CLARO/USO/SP_3543907_USO\", i),\n        destfile = here::here(\"dados\", \"vetor\", paste0(\"SP_3543907_USO\", i)), mode = \"wb\")\n}\n## Importar os dados pelo pacote ecodados\necodados::geo_vetor_nascentes\necodados::geo_vetor_hidrografia\necodados::geo_vetor_cobertura\n## Importar nascentes\ngeo_vetor_nascentes <- sf::st_read(\n    here::here(\"dados\", \"vetor\", \"SP_3543907_NASCENTES.shp\"), quiet = TRUE)\n## Plot\nplot(geo_vetor_nascentes[1], pch = 20, col = \"blue\", main = NA, \n     axes = TRUE, graticule = TRUE)\n## Importar hidrografia\ngeo_vetor_hidrografia <- sf::st_read(\n    here::here(\"dados\", \"vetor\", \"SP_3543907_RIOS_SIMPLES.shp\"), quiet = TRUE)\n## Plot\nplot(geo_vetor_hidrografia[1], col = \"steelblue\", main = NA, axes = TRUE, graticule = TRUE)\n## Importar cobertura da terra\ngeo_vetor_cobertura <- sf::st_read(\n    here::here(\"dados\", \"vetor\", \"SP_3543907_USO.shp\"), quiet = TRUE)\n## Plot\nplot(geo_vetor_cobertura[5], \n     col = c(\"blue\", \"orange\", \"gray30\", \"forestgreen\", \"green\"), \n     main = NA, axes = TRUE, graticule = TRUE)\nlegend(x = .1, y = .3, pch = 15, cex = .7, pt.cex = 2.5, \n       legend = (geo_vetor_cobertura$CLASSE_USO), \n       col = c(\"blue\", \"orange\", \"gray30\", \"forestgreen\", \"green\"))\n## Listar todos os dados do geobr\ngeobr::list_geobr()\n## Polígono do limite do município de Rio Claro\ngeo_vetor_rio_claro <- geobr::read_municipality(code_muni = 3543907, \n                                                year = 2020, showProgress = FALSE)\n## Importar os dados pelo pacote ecodados\necodados::geo_vetor_rio_claro\n## Plot\nplot(geo_vetor_rio_claro[1], col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\n## Polígono do limite do Brasil\ngeo_vetor_brasil <- rnaturalearth::ne_countries(scale = \"large\", \n                                                country = \"Brazil\", returnclass = \"sf\")\n## Importar os dados pelo pacote ecodados\necodados::geo_vetor_brasil\n## Plot\nplot(geo_vetor_brasil[1], col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\n## Criar diretório\ndir.create(here::here(\"dados\", \"tabelas\"))\n## Download\ndownload.file(url = \"https://esajournals.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1002%2Fecy.2392&file=ecy2392-sup-0001-DataS1.zip\",\n              destfile = here::here(\"dados\", \"tabelas\", \"atlantic_amphibians.zip\"), mode = \"wb\")\n\n## Unzip\nunzip(zipfile = here::here(\"dados\", \"tabelas\", \"atlantic_amphibians.zip\"),\n      exdir = here::here(\"dados\", \"tabelas\"))\n## Importar tabela de locais\ngeo_anfibios_locais <- readr::read_csv(\n    here::here(\"dados\", \"tabelas\", \"ATLANTIC_AMPHIBIANS_sites.csv\"),\n    locale = readr::locale(encoding = \"latin1\")\n)\ngeo_anfibios_locais\n#> # A tibble: 1,163 × 25\n#>    id      reference_number species_number record sampled_habitat active_methods passive_methods complementary_meth… period month_start year_start\n#>    <chr>              <dbl>          <dbl> <chr>  <chr>           <chr>          <chr>           <chr>               <chr>        <dbl>      <dbl>\n#>  1 amp1001             1001             19 ab     fo,ll           as             pt              <NA>                mo,da…           9       2000\n#>  2 amp1002             1002             16 co     fo,la,ll        as             pt              <NA>                mo,da…          12       2007\n#>  3 amp1003             1002             14 co     fo,la,ll        as             pt              <NA>                mo,da…          12       2007\n#>  4 amp1004             1002             13 co     fo,la,ll        as             pt              <NA>                mo,da…          12       2007\n#>  5 amp1005             1003             30 co     fo,ll,br        as             <NA>            <NA>                mo,da…           7       1988\n#>  6 amp1006             1004             42 co     tp,pp,la,ll,is  <NA>           <NA>            <NA>                <NA>            NA         NA\n#>  7 amp1007             1005             23 co     sp              as             <NA>            <NA>                <NA>             4       2007\n#>  8 amp1008             1005             19 co     sp,la,sw        as,sb,tr       <NA>            <NA>                tw,ni            4       2007\n#>  9 amp1009             1005             13 ab     fo              <NA>           pt              <NA>                mo,da…           4       2007\n#> 10 amp1010             1006              1 ab     fo              <NA>           pt              <NA>                mo,da…           5       2011\n#> # … with 1,153 more rows, and 14 more variables: month_finish <dbl>, year_finish <dbl>, effort_months <dbl>, country <chr>, state <chr>,\n#> #   state_abbreviation <chr>, municipality <chr>, site <chr>, latitude <dbl>, longitude <dbl>, coordinate_precision <chr>, altitude <dbl>,\n#> #   temperature <dbl>, precipitation <dbl>\n## Importar os dados pelo pacote ecodados\necodados::geo_anfibios_locais\n## Converter dados tabulares para sf\ngeo_anfibios_locais_vetor <- geo_anfibios_locais %>% \n    sf::st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\ngeo_anfibios_locais_vetor\n#> Simple feature collection with 1163 features and 23 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: -56.74194 ymin: -33.51083 xmax: -34.79667 ymax: -3.51525\n#> Geodetic CRS:  WGS 84\n#> # A tibble: 1,163 × 24\n#>    id      reference_number species_number record sampled_habitat active_methods passive_methods complementary_meth… period month_start year_start\n#>  * <chr>              <dbl>          <dbl> <chr>  <chr>           <chr>          <chr>           <chr>               <chr>        <dbl>      <dbl>\n#>  1 amp1001             1001             19 ab     fo,ll           as             pt              <NA>                mo,da…           9       2000\n#>  2 amp1002             1002             16 co     fo,la,ll        as             pt              <NA>                mo,da…          12       2007\n#>  3 amp1003             1002             14 co     fo,la,ll        as             pt              <NA>                mo,da…          12       2007\n#>  4 amp1004             1002             13 co     fo,la,ll        as             pt              <NA>                mo,da…          12       2007\n#>  5 amp1005             1003             30 co     fo,ll,br        as             <NA>            <NA>                mo,da…           7       1988\n#>  6 amp1006             1004             42 co     tp,pp,la,ll,is  <NA>           <NA>            <NA>                <NA>            NA         NA\n#>  7 amp1007             1005             23 co     sp              as             <NA>            <NA>                <NA>             4       2007\n#>  8 amp1008             1005             19 co     sp,la,sw        as,sb,tr       <NA>            <NA>                tw,ni            4       2007\n#>  9 amp1009             1005             13 ab     fo              <NA>           pt              <NA>                mo,da…           4       2007\n#> 10 amp1010             1006              1 ab     fo              <NA>           pt              <NA>                mo,da…           5       2011\n#> # … with 1,153 more rows, and 13 more variables: month_finish <dbl>, year_finish <dbl>, effort_months <dbl>, country <chr>, state <chr>,\n#> #   state_abbreviation <chr>, municipality <chr>, site <chr>, coordinate_precision <chr>, altitude <dbl>, temperature <dbl>, precipitation <dbl>,\n#> #   geometry <POINT [°]>\n## Plot\nplot(geo_anfibios_locais_vetor[1], pch = 20, col = \"black\", \n     main = NA, axes = TRUE, graticule = TRUE)\n## Polígonos países sp\nco110_sp <- rnaturalearth::countries110\nclass(co110_sp)\n#> [1] \"SpatialPolygonsDataFrame\"\n#> attr(,\"package\")\n#> [1] \"sp\"\n## Polígonos países sf\nco110_sf <- sf::st_as_sf(co110_sp)\nclass(co110_sf)\n#> [1] \"sf\"         \"data.frame\"\n## Polígonos países sp\nco110_sp <- sf::as_Spatial(co110_sf)\nclass(co110_sp)\n#> [1] \"SpatialPolygonsDataFrame\"\n#> attr(,\"package\")\n#> [1] \"sp\"\n## Criar diretório\ndir.create(here::here(\"dados\", \"raster\"))\n## Aumentar o tempo de download\noptions(timeout = 1e3)\n\n## Download\ndownload.file(url = \"https://srtm.csi.cgiar.org/wp-content/uploads/files/srtm_5x5/TIFF/srtm_27_17.zip\",\n              destfile = here::here(\"dados\", \"raster\", \"srtm_27_17.zip\"), mode = \"wb\")\n\n## Unzip\nunzip(zipfile = here::here(\"dados\", \"raster\", \"srtm_27_17.zip\"),\n      exdir = here::here(\"dados\", \"raster\"))\n## Importar os dados pelo pacote ecodados\necodados::geo_raster_srtm\n## Importar raster de altitude\ngeo_raster_srtm <- raster::raster(here::here(\"dados\", \"raster\", \"srtm_27_17.tif\"))\ngeo_raster_srtm\n#> class      : RasterLayer \n#> dimensions : 6000, 6000, 3.6e+07  (nrow, ncol, ncell)\n#> resolution : 0.0008333333, 0.0008333333  (x, y)\n#> extent     : -50, -45, -25, -20  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : srtm_27_17.tif \n#> names      : srtm_27_17 \n#> values     : -32768, 32767  (min, max)\n## Plot\nplot(geo_raster_srtm, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Aumentar o tempo de download\noptions(timeout = 1e3)\n\n## Download\ndownload.file(url = \"https://biogeo.ucdavis.edu/data/worldclim/v2.1/base/wc2.1_10m_bio.zip\",\n              destfile = here::here(\"dados\", \"raster\", \"wc2.0_10m_bio.zip\"), mode = \"wb\")\n\n## Unzip\nunzip(zipfile = here::here(\"dados\", \"raster\", \"wc2.0_10m_bio.zip\"),\n      exdir = here::here(\"dados\", \"raster\"))\n## Importar os dados pelo pacote ecodados\necodados::geo_raster_bioclim\n## Listar arquivos\narquivos_raster <- dir(path = here::here(\"dados\", \"raster\"), pattern = \"wc\") %>% \n    grep(\".tif\", ., value = TRUE)\narquivos_raster\n#>  [1] \"wc2.1_10m_bio_1.tif\"  \"wc2.1_10m_bio_10.tif\" \"wc2.1_10m_bio_11.tif\" \"wc2.1_10m_bio_12.tif\" \"wc2.1_10m_bio_13.tif\" \"wc2.1_10m_bio_14.tif\"\n#>  [7] \"wc2.1_10m_bio_15.tif\" \"wc2.1_10m_bio_16.tif\" \"wc2.1_10m_bio_17.tif\" \"wc2.1_10m_bio_18.tif\" \"wc2.1_10m_bio_19.tif\" \"wc2.1_10m_bio_2.tif\" \n#> [13] \"wc2.1_10m_bio_3.tif\"  \"wc2.1_10m_bio_4.tif\"  \"wc2.1_10m_bio_5.tif\"  \"wc2.1_10m_bio_6.tif\"  \"wc2.1_10m_bio_7.tif\"  \"wc2.1_10m_bio_8.tif\" \n#> [19] \"wc2.1_10m_bio_9.tif\"\n\n## Importar vários rasters como stack\ngeo_raster_bioclim <- raster::stack(here::here(\"dados\", \"raster\", arquivos_raster))\ngeo_raster_bioclim\n#> class      : RasterStack \n#> dimensions : 1080, 2160, 2332800, 19  (nrow, ncol, ncell, nlayers)\n#> resolution : 0.1666667, 0.1666667  (x, y)\n#> extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> names      : wc2.1_10m_bio_1, wc2.1_10m_bio_10, wc2.1_10m_bio_11, wc2.1_10m_bio_12, wc2.1_10m_bio_13, wc2.1_10m_bio_14, wc2.1_10m_bio_15, wc2.1_10m_bio_16, wc2.1_10m_bio_17, wc2.1_10m_bio_18, wc2.1_10m_bio_19, wc2.1_10m_bio_2, wc2.1_10m_bio_3, wc2.1_10m_bio_4, wc2.1_10m_bio_5, ... \n#> min values :      -54.724354,       -37.781418,       -66.311249,         0.000000,         0.000000,         0.000000,         0.000000,         0.000000,         0.000000,         0.000000,         0.000000,        1.000000,        9.131122,        0.000000,      -29.686001, ... \n#> max values :        30.98764,         38.21617,         29.15299,      11191.00000,       2381.00000,        484.00000,        229.00169,       5284.00000,       1507.00000,       5282.00000,       4467.00000,        21.14754,       100.00000,      2363.84595,        48.08275, ...\n## Plot\nplot(geo_raster_bioclim[[c(1, 4)]], col = viridis::viridis(10))"},{"path":"cap15.html","id":"exportar-dados-1","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.6.3 Exportar dados","text":"Saber melhor forma de exportar dados geoespaciais de objetos recém-criados R é fundamental, principalmente porque essa ação dependerá tipo de dado (vetor ou raster), classe objeto (por exemplo, MULTIPOINT ou RasterLayer) e tipo e quantidade de informações armazenadas (por exemplo, tamanho objeto, intervalo de valores, etc.).VetorPara dados vetoriais, principal função utilizada é sf::st_write(). Essa função permite gravar objetos sf em vários formatos de arquivos vetoriais, como .shp, .gpkg ou .geojson. O formato ser exportado vai influenciar na velocidade processo de gravação.Os argumentos dessa função será o obj que é o objeto sf criado ambiente R, e o dsn (data source name), ou seja, o nome que o arquivo terá ao ser exportado R, de modo que o complemento .shp nome de saída, por exemplo, definirá que o arquivo terá extensão ESRI Shapefile. Entretanto, essa extensão pode ser definida também utilizando o argumento driver, com possibilidades listadas nesse site.Ou podemos ainda exportar o objeto vetorial na extensão GeoPackage. Entretanto, aqui é interessante acrescentar um argumento chamado layer para definir o nome das camadas serem exportadas mesmo arquivo GeoPackage, por exemplo.Ainda sobre o formato GeoPackage, há algo muito interessante que podemos fazer: podemos acrescentar outros arquivos vetoriais ao mesmo arquivo já criado. Como exemplo, exportaremos o limite Brasil para o mesmo arquivo.RasterPara exportar dados raster utilizamos geralmente função raster::writeRaster(). Exportar dados raster é um pouco mais complexo que exportar dados vetoriais. Teremos de definir se exportaremos arquivos em uma ou várias camadas, quantidade de informações por pixel, e ainda diferentes extensões de saída. 📝 Importante \nArquivos raster escritos em discos geralmente ocupam bastante espaço, e dessa forma, há parâmetros específicos para certos tipos de dados, que detalharemos seguir para contornar esse problema e comprimir os arquivos.Na função raster::writeRaster(), o argumento x diz respeito ao objeto raster ambiente R. O argumento filename é nome arquivo que será exportado R, podendo ou não possuir extensão que se pretende que o arquivo tenha. O argumento format é o formato arquivo, sendo principais possibilidades resumidas na Tabela 15.8, e para saber das possibilidades suportadas, use função raster::writeFormats(). O argumento bylayer diz se múltiplas camadas serão exportadas em arquivos diferentes ou em apenas um arquivo.\nTabela 15.8: Principais formatos de arquivos raster exportados R.\nDentre os argumentos adicionais, temos ainda o datatype, que faz referência um dos nove tipos de formato de dados detalhados na Tabela 15.9, sendo que o tipo de dado determina representação em bits (quantidade de informação) na célula objeto raster exportado e depende da faixa de valores objeto raster em cada pixel. Quanto mais valores um tipo de dado puder representar, maior será o arquivo exportado disco. Dessa forma, é interessante utilizar um tipo de dado que diminua o tamanho arquivo ser exportado, dependendo tipo de dado em cada pixel. Para função raster::writeRaster(), o default é FLT4S, o que pode ocupar mais espaço em disco que o necessário.\nTabela 15.9: Tipos de dados suportados pelo pacote raster.\nOutros argumentos de suporte são: overwrite para sobrescrever um arquivo que já exista, progress para mostrar uma barra de progresso da exportação como “text” ou “window”, e options que permite opções GDAL. Para esse último, quando exportar especificamente na extensão GeoTIFF, podemos utilizar options = c(\"COMPRESS=DEFLATE\", \"TFW=YES\") para que haja compressão arquivo, diminuindo consideravelmente seu tamanho (cerca de um terço), aliado à criação de um arquivo auxiliar .tfw, para ser carregado em softwares específicos de SIG, como o ArcGIS.Para exportar apenas uma camada RasterLayer, podemos utilizar função raster::writeRaster() em um formato mais simples.Para mais de uma camada RasterBrick ou RasterStack, podemos utilizar função raster::writeRaster() com o bylayer = TRUE.","code":"\n## Exportar o polígono de Rio Claro na extensão ESRI Shapefile\nsf::st_write(obj = geo_vetor_rio_claro, \n             dsn = here::here(\"dados\", \"vetor\", \"geo_vetor_rio_claro.shp\"))\n## Exportar o polígono de Rio Claro na extensão Geopackage\nsf::st_write(obj = geo_vetor_rio_claro, \n             dsn = here::here(\"dados\", \"vetor\", \"vetores.gpkg\"), \n             layer = \"rio_claro\")\n## Exportar o polígono do Brasil na extensão Geopackage\nsf::st_write(obj = geo_vetor_brasil, \n             dsn = here::here(\"dados\", \"vetor\", \"vetores.gpkg\"), \n             layer = \"brasil\")\n## Criar diretório\ndir.create(here::here(\"dados\", \"raster\", \"exportados\"))\n\n## Exportar raster layer\nraster::writeRaster(geo_raster_srtm, \n                    filename = here::here(\"dados\", \"raster\", \"exportados\", \"elevation\"),\n                    format = \"GTiff\",\n                    datatype = \"INT2S\",\n                    options = c(\"COMPRESS=DEFLATE\", \"TFW=YES\"),\n                    progress = \"text\",\n                    overwrite = TRUE)\n## Exportar raster stack\nraster::writeRaster(x = geo_raster_bioclim, \n                    filename = here::here(\"dados\", \"raster\", \"exportados\", names(geo_raster_bioclim)),\n                    bylayer = TRUE, \n                    format = \"GTiff\",\n                    datatype = \"INT2S\",\n                    options = c(\"COMPRESS=DEFLATE\", \"TFW=YES\"),\n                    progress = \"text\",\n                    overwrite = TRUE)"},{"path":"cap15.html","id":"descrição-de-objetos-geoespaciais","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.7 Descrição de objetos geoespaciais","text":"Muitas vezes precisaremos verificar informações dos objetos geoespaciais importados para o R. Apesar de chamar o objeto trazer grande parte das informações que precisamos consultar, existem funções específicas que nos auxiliam nesse processo de descrição dos objetos.","code":""},{"path":"cap15.html","id":"vetor-2","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.7.1 Vetor","text":"Podemos acessar informações geoespaciais e tabela de atributos de um objeto importado como vetor simplesmente chamando o nome objeto R.Mas também podemos acessar informações geoespaciais com funções específicas, como tipo de geometria, limites geoespaciais vetor (extensão), sistema de referência de coordenadas (CRS), e tabela de atributos.","code":"\n## Município de Rio Claro\ngeo_vetor_rio_claro\n#> Simple feature collection with 1 feature and 7 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -47.76521 ymin: -22.55203 xmax: -47.46188 ymax: -22.24368\n#> Geodetic CRS:  SIRGAS 2000\n#>     code_muni name_muni code_state abbrev_state name_state code_region name_region                           geom\n#> 493   3543907 Rio Claro         35           SP  São Paulo           3     Sudeste MULTIPOLYGON (((-47.46875 -...\n## Tipo de geometria\nsf::st_geometry_type(geo_vetor_rio_claro)\n#> [1] MULTIPOLYGON\n#> 18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT MULTILINESTRING MULTIPOLYGON GEOMETRYCOLLECTION CIRCULARSTRING ... TRIANGLE\n\n## Extensão\nsf::st_bbox(geo_vetor_rio_claro)\n#>      xmin      ymin      xmax      ymax \n#> -47.76521 -22.55203 -47.46188 -22.24368\n\n## CRS\nsf::st_crs(geo_vetor_rio_claro)\n#> Coordinate Reference System:\n#>   User input: SIRGAS 2000 \n#>   wkt:\n#> GEOGCRS[\"SIRGAS 2000\",\n#>     DATUM[\"Sistema de Referencia Geocentrico para las AmericaS 2000\",\n#>         ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Horizontal component of 3D system.\"],\n#>         AREA[\"Latin America - Central America and South America - onshore and offshore. Brazil - onshore and offshore.\"],\n#>         BBOX[-59.87,-122.19,32.72,-25.28]],\n#>     ID[\"EPSG\",4674]]\n\n## Acessar a tabela de atributos\ngeo_vetor_rio_claro_tab <- sf::st_drop_geometry(geo_vetor_rio_claro)\ngeo_vetor_rio_claro_tab\n#>     code_muni name_muni code_state abbrev_state name_state code_region name_region\n#> 493   3543907 Rio Claro         35           SP  São Paulo           3     Sudeste"},{"path":"cap15.html","id":"raster-2","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.7.2 Raster","text":"Da mesma forma, podemos acessar informações objetos raster chamando o nome objeto.Além disso, podemos selecionar informações desse objeto com funções específicas, tanto para RasterLayer, quanto para RasterBrick ou RasterStack como: classe, dimensões (número de linhas, colunas e camadas), número de camadas, número de linhas, número de colunas, número de células, resolução (largura e altura tamanho pixel), extensão (limites geoespaciais), sistema de referência de coordenadas (CRS), nome das camadas e extrair os valores de todos os pixels.","code":"\n## Raster layer\ngeo_raster_srtm\n#> class      : RasterLayer \n#> dimensions : 6000, 6000, 3.6e+07  (nrow, ncol, ncell)\n#> resolution : 0.0008333333, 0.0008333333  (x, y)\n#> extent     : -50, -45, -25, -20  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : srtm_27_17.tif \n#> names      : srtm_27_17 \n#> values     : -32768, 32767  (min, max)\n## Classe\nclass(geo_raster_srtm)\n#> [1] \"RasterLayer\"\n#> attr(,\"package\")\n#> [1] \"raster\"\n\n## Dimensões\ndim(geo_raster_srtm)\n#> [1] 6000 6000    1\n\n## Número de camadas\nnlayers(geo_raster_srtm)\n#> [1] 1\n\n## Número de linhas\nnrow(geo_raster_srtm)\n#> [1] 6000\n\n## Número de colunas\nncol(geo_raster_srtm)\n#> [1] 6000\n\n## Número de células\nncell(geo_raster_srtm)\n#> [1] 3.6e+07\n\n## Resolução\nres(geo_raster_srtm)\n#> [1] 0.0008333333 0.0008333333\n\n## Extensão\nextent(geo_raster_srtm)\n#> class      : Extent \n#> xmin       : -50 \n#> xmax       : -45 \n#> ymin       : -25 \n#> ymax       : -20\n\n## Projeção ou CRS\nprojection(geo_raster_srtm)\n#> [1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n## Nomes\nnames(geo_raster_srtm)\n#> [1] \"srtm_27_17\"\n\n## Valores\ngetValues(geo_raster_srtm) %>% head\n#> [1] 382 379 379 379 379 383\nvalues(geo_raster_srtm) %>% head\n#> [1] 382 379 379 379 379 383\ngeo_raster_srtm[] %>% head\n#> [1] 382 379 379 379 379 383"},{"path":"cap15.html","id":"reprojeção-de-dados-geoespaciais","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.8 Reprojeção de dados geoespaciais","text":"Em algumas situações é necessário alterar o CRS de um objeto espacial para um novo CRS. reprojeção é justamente transformação de coordenadas de um CRS para outro: geoespaciais (‘lon/lat’, com unidades em graus de longitude e latitude) e projetados (normalmente com unidades de metros partir de um datum).Geralmente precisaremos fazer essa operação para transformar camadas vetoriais ou rasters para o mesmo CRS, de modo que possam ser exibidas conjuntamente, ou ainda que camadas possuem CRS projetado para realizar alguma operação espacial entre camadas, ou quando precisamos calcular áreas, formatos ou distâncias, como métricas de paisagem, por exemplo. Existe uma infinidade de projeções e um excelente material de consulta é o livro de Lapaine & Usery (2017).Podemos verificar o CRS de uma camada através da função sf::st_crs() ou raster::projection() e raster::crs(), ou ainda, saber se mesma possui um CRS geográfico ou não, com função sf::st_is_longlat().Já para reprojetar um objeto sf usamos função sf::st_transform() e para um objeto raster usamos função raster::projectRaster().funções sf::st_transform() e raster::projectRaster() possuem dois argumentos importantes: x que é o objeto ser reprojetado e o crs que é o CRS alvo. O argumento crs pode ser especificado de quatro maneiras: ) código EPSG (por exemplo, 4326), ii) string PROJ4 (por exemplo, + proj = longlat + datum = WGS84 + no_defs), iii) string WKT, ou iv) objeto crs de outra camada, conforme retornado por sf::st_crs() ou raster::crs(). Essas informações de EPSG, PROJ4 e WKT podem ser acessadas nas bases: epsg.io e spatialreference.org.Dentre os possíveis CRSs serem utilizados, alguns são mais comuns para CRSs geoespaciais e projetados. Para CRSs geoespaciais, o mais comum para o mundo é o World Geodetic System 1984 (WGS84), ou seja, geográfico com datum WGS84. Para o Brasil, o CRS adotado é o Sistema de Referencia Geocéntrico para las Américas 2000 (SIRGAS 2000), ou seja, geográfico com datum SIRGAS2000.Para CRSs projetados, essa escolha vai depender da extensão e localização da área de interesse globo terrestre. Aqui destacaremos os principais, para três escalas: global, regional e local. Para escala global, geralmente usa-se umas dessas projeções, dependendo objetivo: ) Projeção de Mollweide, ii) Projeção de Winkel Tripel, iii) Projeção de Eckert IV, iv) Projeção Azimutal de Lambert. Para escala regional, como um hemisfério, geralmente usa-se Projeção Cônica de Albers. Por fim, para escala local, usa-se geralmente Projeção Universal Transverse Mercator (UTM), um conjunto de CRSs que divide Terra em 60 cunhas longitudinais e 20 segmentos latitudinais, como pode ser visto neste link.Os principais CRSs são descritos na Tabela 15.10.\nTabela 15.10: Principais CRSs utilizados.\n","code":"\n## Projeção de vetores\nsf::st_crs(geo_vetor_rio_claro)\n#> Coordinate Reference System:\n#>   User input: SIRGAS 2000 \n#>   wkt:\n#> GEOGCRS[\"SIRGAS 2000\",\n#>     DATUM[\"Sistema de Referencia Geocentrico para las AmericaS 2000\",\n#>         ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Horizontal component of 3D system.\"],\n#>         AREA[\"Latin America - Central America and South America - onshore and offshore. Brazil - onshore and offshore.\"],\n#>         BBOX[-59.87,-122.19,32.72,-25.28]],\n#>     ID[\"EPSG\",4674]]\n\n## Projeção de raster\nraster::projection(geo_raster_srtm)\n#> [1] \"+proj=longlat +datum=WGS84 +no_defs\"\nraster::crs(geo_raster_srtm)\n#> Coordinate Reference System:\n#> Deprecated Proj.4 representation: +proj=longlat +datum=WGS84 +no_defs \n#> WKT2 2019 representation:\n#> GEOGCRS[\"WGS 84 (with axis order normalized for visualization)\",\n#>     DATUM[\"World Geodetic System 1984\",\n#>         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433,\n#>                 ID[\"EPSG\",9122]]],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433,\n#>                 ID[\"EPSG\",9122]]],\n#>     REMARK[\"Axis order reversed compared to EPSG:4326\"]]\n\n## Verificar se o CRS é geográfico\nsf::st_is_longlat(geo_vetor_rio_claro)\n#> [1] TRUE"},{"path":"cap15.html","id":"vetor-3","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.8.1 Vetor","text":"Como dissemos, para reprojetar um vetor, utilizamos função sf::st_transform(), observando os argumentos x que é camada ser reprojetada, e o crs que é o CRS alvo.Vamos reprojetar o limite município de Rio Claro/SP CRS SIRGAS2000/geográfico para o CRS projetado SIRGAS2000/UTM23S, com os efeitos da transformação podendo ser notados na Figura 15.16.\nFigura 15.16: Limites município de Rio Claro/SP com CRS SIRGAS2000/geográfico e com CRS SIRGAS2000/UTM23S.\nPodemos ainda utilizar o formato proj4string argumento crs para fazer transformação. Vamos primeiramente plotar o mundo em WGS84/Geográfico (Figura 15.17).\nFigura 15.17: Limite dos países mundo com CRS geográfico e datum WGS84.\nAgora, reprojetaremos utilizando Projeção de Mollweide (Figura 15.18).\nFigura 15.18: Limite dos países mundo com CRS Projeção de Mollweide.\nOu ainda podemos utilizar Projeção Azimutal de Lambert com alguns parâmetros ajustados para centralizar projeção Brasil (Figura 15.19).\nFigura 15.19: Limite dos países mundo com CRS Projeção Azimutal de Lambert centrado Brasil.\n","code":"\n## Converter CRS\ngeo_vetor_rio_claro_sirgas2000_utm23s <- sf::st_transform(x = geo_vetor_rio_claro, \n                                                          crs = 31983)\n## Plot\nplot(co110_sf[1], col = \"gray\",  main = \"WGS84/Geográfio\", graticule = TRUE)\n## Projeção de Mollweide \nco110_sf_moll <- sf::st_transform(x = co110_sf, crs = \"+proj=moll\")\n## Plot\nplot(co110_sf_moll[1], col = \"gray\", main = \"Projeção de Mollweide\", graticule = TRUE)\n## Projeção Azimutal de Lambert\nco110_sf_laea <- sf::st_transform(x = co110_sf, \n                                  crs = \"+proj=laea +x_0=0 +y_0=0 +lon_0=-50 +lat_0=0\")\n## Plot\nplot(co110_sf_laea[1], col = \"gray\", main = \"Projeção Azimutal de Lambert\", graticule = TRUE)"},{"path":"cap15.html","id":"raster-3","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.8.2 Raster","text":"reprojeção de objetos raster não é uma tarefa tão simples quanto reprojeção de vetores. Em vetores, reprojeção altera coordenadas de cada vértice. Entretanto, como rasters são compostos de células retangulares mesmo tamanho,reprojeção raster envolve criação de um novo objeto raster, com duas operações espaciais separadas: ) reprojeção vetorial dos centroides celulares para outro CRS (.e., muda posição e tamanho pixel) e, ii) cálculo de novos valores pixel por meio de reamostragem (.e., muda o valor pixel).função raster::projectRaster() possui alguns parâmetros que necessitam de algumas especificações. O argumento é o objeto raster de entrada que sofre reprojeção. O argumento é um objeto raster qual todas propriedades dos CRSs, como extensão e resolução serão associadas ao objeto raster indicado argumento . O argumento res permite ajustar resolução pixel de saída objeto raster reprojetado.O argumento crs aceita apenas definições de proj4string extensas de um CRS em vez de códigos EPSG concisos. Contudo, é possível usar um código EPSG em uma definição de proj4string com +init=epsg:EPSG. Por exemplo, pode-se usar definição +init=epsg:4326 para definir CRS para WGS84 (código EPSG de 4326). biblioteca PROJ adiciona automaticamente o resto dos parâmetros e os converte em +init=epsg:4326 +proj=longlat +datum=WGS84 + no_defs + ellps=WGS84 + towgs84=0,0,0.O argumento method permite escolher entre os métodos ngb (vizinho mais próximo) ou biliniar (interpolação bilinear), sendo o primeiro mais indicado para reprojeção de rasters categóricos, pois os valores estimados devem ser iguais aos raster original. O método ngb define cada novo valor de célula para o valor da célula mais próxima (centro) raster de entrada. Já o método biliniar é indicado para raster contínuos e calcula o valor da célula de saída com base nas quatro células mais próximas raster original, sendo média ponderada da distância dos valores dessas quatro células. Existem outras formas de interpolação, mas não abordaremos aqui.Aqui, vamos reprojetar os dados de elevação para Rio Claro/SP. Para que esse processo seja mais rápido, iremos antes ajustar extensão raster para o limite município usando função raster::crop() (Figura 15.20). Essa função é melhor explicada na seção de cortes e máscaras, mais adiante.\nFigura 15.20: Ajuste da extensão raster de elevação para o município de Rio Claro/SP.\nPrimeiramente, vamos reprojetar indicando uma projeção e sem especificar o tamanho da célula. Note que o tamanho da célula vai se ajustar para valores diferentes, sendo portanto, pixels retangulares e não quadrados.Agora vamos reprojetar especificando o tamanho da célula (Figura 15.21). Dessa forma, todas células terão o mesmo, .e., quadrados de 90 metros.\nFigura 15.21: Reprojeção raster de elevação para SIRGAS2000/UTM23S especificado por um objeto e informando o tamanho da célula.\nVamos também reprojetar uma camada mundial da média de temperatura anual (BIO01), indicando o tamanho da célula para 25.000 m (Figura 15.22).\nFigura 15.22: Reprojeção raster de média de temperatura anual (BIO01) para Projeção de Mollweide informando o tamanho da célula.\n","code":"\n## Ajuste do limite\ngeo_raster_srtm_rio_claro <- raster::crop(x = geo_raster_srtm, \n                                          y = geo_vetor_rio_claro)\ngeo_raster_srtm_rio_claro\n#> class      : RasterLayer \n#> dimensions : 370, 364, 134680  (nrow, ncol, ncell)\n#> resolution : 0.0008333333, 0.0008333333  (x, y)\n#> extent     : -47.765, -47.46167, -22.55167, -22.24333  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : memory\n#> names      : srtm_27_17 \n#> values     : 491, 985  (min, max)\n## Plot\nplot(geo_raster_srtm_rio_claro, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Reprojeção\ngeo_raster_srtm_rio_claro_sirgas2000_utm23s <- raster::projectRaster(\n    from = geo_raster_srtm_rio_claro, \n    crs = \"+init=epsg:31983\", \n    method = \"bilinear\")\ngeo_raster_srtm_rio_claro_sirgas2000_utm23s\n#> class      : RasterLayer \n#> dimensions : 386, 381, 147066  (nrow, ncol, ncell)\n#> resolution : 85.8, 92.3  (x, y)\n#> extent     : 214575.4, 247265.2, 7503009, 7538637  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=utm +zone=23 +south +ellps=GRS80 +units=m +no_defs \n#> source     : memory\n#> names      : srtm_27_17 \n#> values     : 491.6033, 980.4151  (min, max)\n## Reprojeção\ngeo_raster_srtm_rio_claro_sirgas2000_utm23s <- raster::projectRaster(\n    from = geo_raster_srtm_rio_claro, \n    crs = \"+init=epsg:31983\", \n    method = \"bilinear\", \n    res = 90)\ngeo_raster_srtm_rio_claro_sirgas2000_utm23s\n#> class      : RasterLayer \n#> dimensions : 396, 364, 144144  (nrow, ncol, ncell)\n#> resolution : 90, 90  (x, y)\n#> extent     : 214554.4, 247314.4, 7502985, 7538625  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=utm +zone=23 +south +ellps=GRS80 +units=m +no_defs \n#> source     : memory\n#> names      : srtm_27_17 \n#> values     : 493.2395, 986.686  (min, max)\n## Plot\nplot(geo_raster_srtm_rio_claro_sirgas2000_utm23s, \n     col = viridis::viridis(10))\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom,\n     col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Reprojeção\ngeo_raster_bioclim_moll <- raster::projectRaster(\n    from = geo_raster_bioclim[[1]], \n    crs = \"+proj=moll\",\n    res = 25000, \n    method = \"bilinear\")\ngeo_raster_bioclim_moll\n#> class      : RasterLayer \n#> dimensions : 732, 1453, 1063596  (nrow, ncol, ncell)\n#> resolution : 25000, 25000  (x, y)\n#> extent     : -18159905, 18165095, -9154952, 9145048  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=moll +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs \n#> source     : memory\n#> names      : wc2.1_10m_bio_1 \n#> values     : -54.66752, 30.71805  (min, max)\n## Plot\nplot(geo_raster_bioclim_moll, col = viridis::viridis(10))\nplot(co110_sf_moll[1], col = NA, add = TRUE)"},{"path":"cap15.html","id":"principais-operações-com-dados-geoespaciais","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.9 Principais operações com dados geoespaciais","text":"Nesta seção veremos principais funções para realizar operações com dados geoespaciais. Essas operações são separadas conforme Lovelace et al. (2019) em: Operações de atributos, Operações espaciais, e Operações geométricas.","code":""},{"path":"cap15.html","id":"operações-de-atributos","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.9.1 Operações de atributos","text":"São modificação de objetos geoespaciais baseado em informações não espaciais, como tabela de atributos ou valores das células e nome das camadas dos rasters.VetorAs principais operações de atributos vetoriais são com respeito à tabela de atributos, sendo principais: ) filtro, ii) junção, iii) agregação e iv) manipulação da tabela de atributos. lista de possíveis operações é longa, dessa forma, apresentaremos algumas operações utilizando principais funções e listamos demais funções e suas operações, que dependerão de objetivos específicos.Quase todas operações serão mesmas realizadas pelo pacote dplyr em uma tabela de dados (ver o Capítulo 5), sendo algumas operações específicas para alterar apenas campos da tabela de atributos e outras que refletem operações nas feições, ou seja, alterarão através da tabela de atributos características das feições. Essas funções e suas operações são descritas com detalhes na Tabela 15.11.\nTabela 15.11: Principais funções para realizar operações de atributos e suas descrições.\nPara exemplificar operações de atributos, vamos utilizar os dados de nascentes, hidrologia e cobertura da terra para o município de Rio Claro/SP.FiltroVamos iniciar operações fazendo o filtro de feições pela tabela de atributos, que permite selecionar feições pelos seus valores atribuídos, utilizando função dplyr::filter(). Aqui vamos selecionar feições de floresta mapa de cobertura da terra para Rio Claro/SP (Figura 15.23).\nFigura 15.23: Filtro da classe floresta para o mapeamento de cobertura da terra para o município de Rio Claro/SP.\nJunçãoUma das funções mais úteis de operações de atributos é junção, referida em inglês como join, realizada através das funções dplyr::*_join() (ver detalhes Capítulo 5). Nela, usamos uma coluna identificadora para atribuir dados de outra tabela de dados. Como exemplo, vamos criar uma tabela de dados com novos nomes das classes de cobertura da terra e atribuir esses novos nomes à tabela de atributos objeto vetorial. É fundamental destacar que para que essa função funcione, precisamos de uma coluna identificadora dos valores para que junção seja possível.AgregaçãoOutra função bastante útil é agregação de atributos. Apesar de existir uma função que realiza união de feições que veremos na próxima seção, o uso conjunto das funções dplyr::group_by() e dplyr::summarise() realizam uma tarefa semelhante. Aqui vamos agregar nascentes para Rio Claro/SP, .e., juntar cada ponto que estava numa linha da tabela de atributos de modo que todos fiquem numa mesma linha, com o valor da quantidade de nascentes (Figura 15.24).\nFigura 15.24: Agregação e contagem das nascentes para o município de Rio Claro/SP.\nManipulação da tabela de atributosPor fim, é muito comum em análises de softwares SIG criação ou atualização dos valores de colunas na tabela de atributos. Aqui, podemos utilizar função dplyr::mutate() para criar essas novas colunas, assim como atualizar os valores de colunas existentes. Em nosso exemplo, faremos uma composição das colunas CLASSE_USO e AREA_HA numa nova coluna chamada classe_area.Duas funções são bastante interessantes de serem integradas junto à manipulação de tabelas de atributos. Elas calculam propriedades geométricas numéricas dos vetores de linhas (comprimento) e polígonos (área): sf::st_length() e sf::st_area(). Essas funções calculam essas propriedades em metros para comprimento e em metros quadrados para área, independentemente CRS. Para tanto, vamos utilizar linhas de hidrografia e os polígonos de cobertura da terra para Rio Claro/SP, e atribuir esses valores à tabela de atributos de ambos os objetos geoespaciais, utilizando em conjunto função dplyr::mutate().RasterDevido à estrutura espacial raster ser formada por uma ou mais superfícies contínuas, manipulações como subconjunto e outras operações em objetos raster funcionam de uma maneira diferente que em objetos vetoriais. Veremos aqui três principais: ) subconjunto de células usando o operador [] ou subconjunto de camadas RasterStack ou RasterBrick utilizando os operadores [[]] e $, ii) renomear nomes das camadas, e iii) resumir informações de todos os pixels.SubconjuntoPodemos fazer um subconjunto de células utilizando dentro dos operadores [] valores para indicar posição da linha e da coluna de um raster, ou ainda posição de uma célula utilizando apenas um número. Essas operações resultarão em valores diferentes para RasterLayer e RasterBrick ou RasterStack.Para selecionar uma camada de um RasterBrick ou RasterStack, podemos utilizar funções raster::subset() ou raster::raster() com o argumento layer indicando ordem ou o nome da camada, além dos operadores [[]] e $ (Figura 15.25).\nFigura 15.25: Camada BIO01 selecionada pelas operações de subconjunto.\nRenomearPodemos ainda renomear camadas dos raster RasterLayer utilizando função names().E essa operação também funciona para RasterBrick e RasterStack.ResumirMuitas vezes queremos fazer cálculos para todos células de um raster. Podemos resumir informações de todos os pixels fazendo cálculos simples com todos os pixels de cada camada com função raster::cellStats(), sendo x o argumento objeto raster e stat o nome da função resumo, como “mean” ou “sum”.Ou ainda, podemos analisar frequência com que cada valor dos pixels ocorre, utilizando função raster::freq().","code":"\n## Filtro\ngeo_vetor_cobertura_floresta <- geo_vetor_cobertura %>% \n    dplyr::filter(CLASSE_USO == \"formação florestal\")\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_cobertura_floresta$geometry, col = \"forestgreen\", add = TRUE)\n## Dados\ndados_classes <- tibble::tibble(\n    CLASSE_USO = geo_vetor_cobertura$CLASSE_USO, \n    classe = c(\"agua\", \"antropico\", \"edificado\", \"floresta\", \"silvicultura\"))\ndados_classes\n#> # A tibble: 5 × 2\n#>   CLASSE_USO         classe      \n#>   <chr>              <chr>       \n#> 1 água               agua        \n#> 2 área antropizada   antropico   \n#> 3 área edificada     edificado   \n#> 4 formação florestal floresta    \n#> 5 silvicultura       silvicultura\n## Junção\ngeo_vetor_cobertura_classes <- dplyr::left_join(\n    x = geo_vetor_cobertura, \n    y = dados_classes, \n    by = \"CLASSE_USO\") %>% \n    sf::st_drop_geometry()\ngeo_vetor_cobertura_classes\n#>   GEOCODIGO MUNICIPIO UF CD_UF         CLASSE_USO   AREA_HA       classe\n#> 1   3543907 RIO CLARO SP    35               água   357.027         agua\n#> 2   3543907 RIO CLARO SP    35   área antropizada 37297.800    antropico\n#> 3   3543907 RIO CLARO SP    35     área edificada  5078.330    edificado\n#> 4   3543907 RIO CLARO SP    35 formação florestal  7017.990     floresta\n#> 5   3543907 RIO CLARO SP    35       silvicultura   138.173 silvicultura\n## Agregar\ngeo_vetor_nascentes_n <- geo_vetor_nascentes %>% \n    dplyr::group_by(MUNICIPIO, HIDRO) %>% \n    dplyr::summarise(n = n())\ngeo_vetor_nascentes_n\n#> Simple feature collection with 1 feature and 3 fields\n#> Geometry type: MULTIPOINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 217622.9 ymin: 7504132 xmax: 246367.4 ymax: 7537855\n#> Projected CRS: SIRGAS 2000 / UTM zone 23S\n#> # A tibble: 1 × 4\n#> # Groups:   MUNICIPIO [1]\n#>   MUNICIPIO HIDRO        n                                                                                  geometry\n#>   <chr>     <chr>    <int>                                                                          <MULTIPOINT [m]>\n#> 1 RIO CLARO nascente  1220 ((217622.9 7528315), (217836.5 7528103), (217988.9 7528203), (218288.9 7528237), (2183...\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, \n     col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_nascentes_n$geometry, pch = 20, \n     col = \"blue\", add = TRUE)\n## Criar coluna\ngeo_vetor_cobertura_cob_col_area <- geo_vetor_cobertura %>% \n    dplyr::mutate(classe_area = paste0(CLASSE_USO, \" (\", AREA_HA, \" ha)\")) %>% \n    sf::st_drop_geometry()\ngeo_vetor_cobertura_cob_col_area\n#>   GEOCODIGO MUNICIPIO UF CD_UF         CLASSE_USO   AREA_HA                     classe_area\n#> 1   3543907 RIO CLARO SP    35               água   357.027               água (357.027 ha)\n#> 2   3543907 RIO CLARO SP    35   área antropizada 37297.800   área antropizada (37297.8 ha)\n#> 3   3543907 RIO CLARO SP    35     área edificada  5078.330     área edificada (5078.33 ha)\n#> 4   3543907 RIO CLARO SP    35 formação florestal  7017.990 formação florestal (7017.99 ha)\n#> 5   3543907 RIO CLARO SP    35       silvicultura   138.173       silvicultura (138.173 ha)\n## Comprimento das linhas\ngeo_vetor_hidrografia_comp <- geo_vetor_hidrografia %>% \n    dplyr::mutate(comprimento = sf::st_length(.))\ngeo_vetor_hidrografia_comp\n#> Simple feature collection with 1 feature and 7 fields\n#> Geometry type: MULTILINESTRING\n#> Dimension:     XY\n#> Bounding box:  xmin: 215155.3 ymin: 7504132 xmax: 246367.4 ymax: 7537978\n#> Projected CRS: SIRGAS 2000 / UTM zone 23S\n#>   GEOCODIGO MUNICIPIO UF CD_UF                  HIDRO COMP_KM                       geometry comprimento\n#> 1   3543907 RIO CLARO SP    35 curso d'água (0 - 10m) 1142.98 MULTILINESTRING ((231815.7 ... 1142981 [m]\n## Área dos polígonos\ngeo_vetor_cobertura_area <- geo_vetor_cobertura %>% \n    dplyr::mutate(area_m2 = sf::st_area(.))\ngeo_vetor_cobertura_area\n#> Simple feature collection with 5 features and 7 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: 215151.7 ymin: 7503723 xmax: 246582.4 ymax: 7537978\n#> Projected CRS: SIRGAS 2000 / UTM zone 23S\n#>   GEOCODIGO MUNICIPIO UF CD_UF         CLASSE_USO   AREA_HA                       geometry         area_m2\n#> 1   3543907 RIO CLARO SP    35               água   357.027 MULTIPOLYGON (((235487.6 75...   3570267 [m^2]\n#> 2   3543907 RIO CLARO SP    35   área antropizada 37297.800 MULTIPOLYGON (((232275 7504... 372978415 [m^2]\n#> 3   3543907 RIO CLARO SP    35     área edificada  5078.330 MULTIPOLYGON (((233123.6 75...  50783283 [m^2]\n#> 4   3543907 RIO CLARO SP    35 formação florestal  7017.990 MULTIPOLYGON (((232355 7504...  70179895 [m^2]\n#> 5   3543907 RIO CLARO SP    35       silvicultura   138.173 MULTIPOLYGON (((243052.1 75...   1381726 [m^2]\n## Raster - linha 1 e columna 1\ngeo_raster_srtm[1, 1]\n#>     \n#> 382\n\n## Raster - célula 1\ngeo_raster_srtm[1]\n#>     \n#> 382\n\n## Stack - linha 1 e columna 1\ngeo_raster_bioclim[1, 1]\n#>      wc2.1_10m_bio_1 wc2.1_10m_bio_10 wc2.1_10m_bio_11 wc2.1_10m_bio_12 wc2.1_10m_bio_13 wc2.1_10m_bio_14 wc2.1_10m_bio_15 wc2.1_10m_bio_16\n#> [1,]              NA               NA               NA               NA               NA               NA               NA               NA\n#>      wc2.1_10m_bio_17 wc2.1_10m_bio_18 wc2.1_10m_bio_19 wc2.1_10m_bio_2 wc2.1_10m_bio_3 wc2.1_10m_bio_4 wc2.1_10m_bio_5 wc2.1_10m_bio_6\n#> [1,]               NA               NA               NA              NA              NA              NA              NA              NA\n#>      wc2.1_10m_bio_7 wc2.1_10m_bio_8 wc2.1_10m_bio_9\n#> [1,]              NA              NA              NA\n\n## Stack - célula 1\ngeo_raster_bioclim[1]\n#>      wc2.1_10m_bio_1 wc2.1_10m_bio_10 wc2.1_10m_bio_11 wc2.1_10m_bio_12 wc2.1_10m_bio_13 wc2.1_10m_bio_14 wc2.1_10m_bio_15 wc2.1_10m_bio_16\n#> [1,]              NA               NA               NA               NA               NA               NA               NA               NA\n#>      wc2.1_10m_bio_17 wc2.1_10m_bio_18 wc2.1_10m_bio_19 wc2.1_10m_bio_2 wc2.1_10m_bio_3 wc2.1_10m_bio_4 wc2.1_10m_bio_5 wc2.1_10m_bio_6\n#> [1,]               NA               NA               NA              NA              NA              NA              NA              NA\n#>      wc2.1_10m_bio_7 wc2.1_10m_bio_8 wc2.1_10m_bio_9\n#> [1,]              NA              NA              NA\n## Seleção de camada num objeto stack utilizando a função subset\ngeo_raster_bioclim_bio01 <- raster::subset(geo_raster_bioclim, \"wc2.1_10m_bio_1\")\ngeo_raster_bioclim_bio01\n#> class      : RasterLayer \n#> dimensions : 1080, 2160, 2332800  (nrow, ncol, ncell)\n#> resolution : 0.1666667, 0.1666667  (x, y)\n#> extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : wc2.1_10m_bio_1.tif \n#> names      : wc2.1_10m_bio_1 \n#> values     : -54.72435, 30.98764  (min, max)\n\n## Seleção de camada num objeto stack utilizando a função raster\ngeo_raster_bioclim_bio01 <- raster::raster(geo_raster_bioclim, layer = 1)\ngeo_raster_bioclim_bio01\n#> class      : RasterLayer \n#> dimensions : 1080, 2160, 2332800  (nrow, ncol, ncell)\n#> resolution : 0.1666667, 0.1666667  (x, y)\n#> extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : wc2.1_10m_bio_1.tif \n#> names      : wc2.1_10m_bio_1 \n#> values     : -54.72435, 30.98764  (min, max)\n\n## Seleção de camada num objeto stack utilizando os operadores [[]] e o nome\ngeo_raster_bioclim_bio01 <- geo_raster_bioclim[[\"wc2.1_10m_bio_1\"]]\ngeo_raster_bioclim_bio01\n#> class      : RasterLayer \n#> dimensions : 1080, 2160, 2332800  (nrow, ncol, ncell)\n#> resolution : 0.1666667, 0.1666667  (x, y)\n#> extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : wc2.1_10m_bio_1.tif \n#> names      : wc2.1_10m_bio_1 \n#> values     : -54.72435, 30.98764  (min, max)\n\n## Seleção de camada num objeto stack utilizando os operadores [[]] e a posicao\ngeo_raster_bioclim_bio01 <- geo_raster_bioclim[[1]]\ngeo_raster_bioclim_bio01\n#> class      : RasterLayer \n#> dimensions : 1080, 2160, 2332800  (nrow, ncol, ncell)\n#> resolution : 0.1666667, 0.1666667  (x, y)\n#> extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : wc2.1_10m_bio_1.tif \n#> names      : wc2.1_10m_bio_1 \n#> values     : -54.72435, 30.98764  (min, max)\n\n## Seleção de camada num objeto stack utilizando o operador $\ngeo_raster_bioclim_bio01 <- geo_raster_bioclim$wc2.1_10m_bio_1\ngeo_raster_bioclim_bio01\n#> class      : RasterLayer \n#> dimensions : 1080, 2160, 2332800  (nrow, ncol, ncell)\n#> resolution : 0.1666667, 0.1666667  (x, y)\n#> extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : wc2.1_10m_bio_1.tif \n#> names      : wc2.1_10m_bio_1 \n#> values     : -54.72435, 30.98764  (min, max)\n# Plot\nplot(geo_raster_bioclim_bio01, col = viridis::viridis(10))\n## Raster - nomes\nnames(geo_raster_srtm_rio_claro)\n#> [1] \"srtm_27_17\"\n\n## Raster - renomear\nnames(geo_raster_srtm_rio_claro) <- \"elevacao\"\n\n## Raster - nomes\nnames(geo_raster_srtm_rio_claro)\n#> [1] \"elevacao\"\n## Stack - nomes\nnames(geo_raster_bioclim)\n#>  [1] \"wc2.1_10m_bio_1\"  \"wc2.1_10m_bio_10\" \"wc2.1_10m_bio_11\" \"wc2.1_10m_bio_12\" \"wc2.1_10m_bio_13\" \"wc2.1_10m_bio_14\" \"wc2.1_10m_bio_15\"\n#>  [8] \"wc2.1_10m_bio_16\" \"wc2.1_10m_bio_17\" \"wc2.1_10m_bio_18\" \"wc2.1_10m_bio_19\" \"wc2.1_10m_bio_2\"  \"wc2.1_10m_bio_3\"  \"wc2.1_10m_bio_4\" \n#> [15] \"wc2.1_10m_bio_5\"  \"wc2.1_10m_bio_6\"  \"wc2.1_10m_bio_7\"  \"wc2.1_10m_bio_8\"  \"wc2.1_10m_bio_9\"\n\n## Stack - renomear\nnames(geo_raster_bioclim) <- c(\"bio01\", paste0(\"bio\", 10:19), paste0(\"bio0\", 2:9))\n\n## Stack - nomes\nnames(geo_raster_bioclim)\n#>  [1] \"bio01\" \"bio10\" \"bio11\" \"bio12\" \"bio13\" \"bio14\" \"bio15\" \"bio16\" \"bio17\" \"bio18\" \"bio19\" \"bio02\" \"bio03\" \"bio04\" \"bio05\" \"bio06\" \"bio07\"\n#> [18] \"bio08\" \"bio09\"\n## Raster - média de todas as células de altitude\nraster::cellStats(x = geo_raster_srtm_rio_claro, stat = mean)\n#> [1] 625.8273\n\n## Stack - média de todas as células de cada camada bioclimática\nraster::cellStats(x = geo_raster_bioclim, stat = mean)\n#>       bio01       bio10       bio11       bio12       bio13       bio14       bio15       bio16       bio17       bio18       bio19       bio02 \n#>  -4.0378283   7.2035545 -13.8963286 550.0569022  93.4633916  15.3689993  74.7084151 241.6525005  55.4149542 156.4237816 108.8950626   9.9432120 \n#>       bio03       bio04       bio05       bio06       bio07       bio08       bio09 \n#>  34.5221528 880.1215546  13.9386423 -19.7938943  33.7325366  -0.9226276  -5.3774489\n## Raster - frequência das células\nraster::freq(x = geo_raster_srtm_rio_claro) %>% head()\n#>      value count\n#> [1,]   491     1\n#> [2,]   492     4\n#> [3,]   493     9\n#> [4,]   494    19\n#> [5,]   495    32\n#> [6,]   496    44\n\n## Stack - frequência das células\nraster::freq(x = geo_raster_bioclim[[1]]) %>% head()\n#>      value count\n#> [1,]   -55   319\n#> [2,]   -54  4529\n#> [3,]   -53  5778\n#> [4,]   -52  6128\n#> [5,]   -51  6090\n#> [6,]   -50  7892"},{"path":"cap15.html","id":"operações-espaciais","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.9.2 Operações espaciais","text":"operações espaciais são modificações de objetos geoespaciais baseado em informações espaciais, como localização e formato. Seria quase impossível abordar todas operações realizáveis nesse capítulo, então demonstraremos principais para dados vetoriais e raster.VetorAs principais operações espaciais para dados vetoriais são: ) filtro espacial, ii) junção espacial, iii) agregação espacial e iv) distância espacial. Apresentaremos essas operações utilizando principais funções utilizando os dados de nascentes, hidrologia e cobertura da terra para o município de Rio Claro/SP.Filtro espacialFiltros espaciais são operações que realizam seleção de feições espaciais entre dois objetos geoespaciais (x e y). Existe uma grande quantidade de funções para realizar filtros espaciais R, como podemos ver na Tabela 15.12. Essas funções verificam se cada feição em x mantém sua relação em y. Ao especificar o parâmetro sparse = FALSE, funções retornam uma matriz lógica (composta por TRUE e FALSE).\nTabela 15.12: Principais pacotes para composição de mapas R.\nEm nosso exemplo, utilizaremos função sf::intersects() para filtrar nascentes dentro de floresta para Rio Claro/SP. Essa função vai retornar resposta binária se nascentes estão (1) ou não (empty) dentro dos polígonos de floresta.Podemos usar essa mesma função em conjunto com função dplyr::filter() para filtrar nascentes dentro de florestas, mas agora com o argumento sparse = FALSE para valores lógicos funcionarem com o filtro.Ou ainda podemos utilizar o operador [] para realizar esse filtro, como podemos notar na Figura 15.26.\nFigura 15.26: Nascentes dentro de florestas município de Rio Claro/SP.\nEntretanto, muitas vezes queremos fazer o filtro de feições que estão fora de feições de outro objeto espacial. Para isso, podemos usar função sf::st_disjoint() ou ainda utilizando o operador [], mas com o argumento op, nesse caso utilizando mesma função sf::st_disjoint() como operação (Figura 15.27). Atentar o segundo argumento vazio nessa operação.\nFigura 15.27: Nascentes fora de florestas município de Rio Claro/SP.\nJunção espacialOutra operação muito usada dentro de análises espaciais é junção espacial ou inglês spatial join. ideia base é muito semelhante com junção baseada em atributos, mas aqui atribuiremos o valor da tabela de atributos das feições de um objeto espacial y às feições que fazem intersecção com um objeto espacial x, de modo que esses valores sejam armazenados na tabela de atributos primeiro objeto espacial.Para exemplificar, vamos atribuir os valores dos polígonos de cobertura da terra aos pontos de nascentes para Rio Claro/SP, fazendo um agrupamento pela tabela de atributos para permitir criar o mapa da Figura 15.28.\nFigura 15.28: Junção espacial da cobertura da terra para nascentes município de Rio Claro/SP.\nAgregação espacialMuitas vezes queremos contabilizar quantas feições ou agregar valores de feições para polígonos. Podemos realizar essa operação usando funções dplyr::group_by() e dplyr::summarise, ou utilizar função aggregate(). Nesse exemplo, vamos contabilizar quantas nascentes há por polígono de cobertura da terra para o município de Rio Claro/SP (Figura 15.29).\nFigura 15.29: Agregação espacial contabilizando o número de nascentes para cada classe de cobertura da terra município de Rio Claro/SP.\nDistância espacialA distância espacial é distância calculada em duas dimensões (2D) entre um objeto espacial x e y baseado CRS e para cada feição dos objetos geoespaciais. Para realizar esse cálculo, utilizamos função sf::st_distance(). Em nosso exemplo, vamos calcular distância das nascentes até floresta mais próxima, e adicionar essa informação para cada ponto na tabela de atributos com função dplyr::mutate(), para o município de Rio Claro/SP (Figura 15.30).\nFigura 15.30: Distância espacial das nascentes até o fragmento de floresta mais próxima município de Rio Claro/SP.\nRasterAs principais operações espaciais para dados raster podem ser classificas, segundo Lovelace et al. (2019), em: ) operações locais (por célula), ii) operações focais (por bloco de múltiplas células regulares - e.g. 3x3), iii) operações zonais (por bloco de múltiplas células irregulares) e iv) operações globais (por um ou vários rasters inteiros). Cada uma delas é aplicada para objetivos e escalas espaciais específicas. Para os exemplos desta seção, utilizaremos o dado raster de elevação para o município de Rio Claro/SP.Operações locaisAs operações locais contemplam todas operações realizadas célula célula em uma ou várias camadas de um objeto raster. álgebra de raster é uma das mais comuns, simples e poderosas operações R envolvendo rasters. Com ela podemos fazer operações simples através de operadores aritméticos (soma, subtração, multiplicação, divisão ou potenciação) entre dois ou mais objetos raster, ou utilizar funções para alterar todos os valores dos pixels como, por exemplo, funções log10() ou sqrt(), ou ainda função raster::scale() para padronizar ou centralizar os valores dos rasters (Figura 15.31).\nFigura 15.31: Rasters de soma e log10 mapa de elevação para Rio Claro/SP.\nAlém das operações aritméticas, álgebra de rasters também permite operações lógicas, como criar um raster binário (composto por 1 quando operação lógica é verdadeira, e 0 quanto é falsa). Em nosso caso, buscamos todos os pixels acima de 600 metros para o raster de elevação de Rio Claro/SP (Figura 15.32).\nFigura 15.32: Operação local lógica mostrando todos os pixels acima de 600 metros de elevação para Rio Claro/SP.\nAlém dos operadores aritméticos, também podemos usar funções raster::calc() (uma camada) e raster::overlay() (duas ou mais camadas) para realizar operações em todas células. Elas funcionam com criação de uma função específica através da função function() (Capítulo 4), para que esta seja aplicada em todas células raster. Essas funções são muito eficientes, portanto, são preferíveis para grandes conjuntos de dados raster. Exemplificaremos essa operação calculando o produto de todos os pixels por eles mesmos raster de elevação de Rio Claro/SP (Figura 15.33).\nFigura 15.33: Operação local de multiplicação de todos os pixels por eles mesmos raster de elevação para Rio Claro/SP.\npredição de objetos raster (utilizando função raster::predict()) é outra aplicação extremamente útil em operações locais, nós veremos mais à frente neste capítulo. Essa função possui basicamente dois argumentos: object que é os rasters preditores e model com o modelo ajustado para o qual os valores serão preditos com base nos valores dos rasters. partir da relação entre variáveis respostas (e.g, pontos espaço, como ocorrência ou riqueza de espécies), e variáveis preditoras (rasters contínuos de elevação, pH, precipitação, temperatura, cobertura da terra ou classe de solo), criamos modelos usando funções como lm(), glm(), gam() ou uma técnica de aprendizado de máquina, e fazemos predições espaciais aplicando os coeficientes estimados aos valores dos raster preditores (consulte od Capítulos 7 e 8).Por fim, reclassificação de rasters é outra operação muito comum quando trabalhamos com rasters. Nela é realizada classificação de intervalos de valores numéricos em grupos, e.g. agrupar um modelo digital de elevação em classes de valores. função que faz essa operação é raster::reclassify(). Ela possui dois argumentos: x que é o raster ser reclassificado, e o segundo rcl para o qual devemos construir uma matriz de reclassificação, onde primeira coluna é extremidade inferior, segunda coluna é extremidade superior, e terceira coluna representa o novo valor para os intervalos das colunas um e dois. Vamos reclassificar o raster de elevação de Rio Claro/SP para os intervalos 400–600, 600–800 e 800–1000 que são reclassificados para os valores 1, 2 e 3, respectivamente (Figura 15.34).\nFigura 15.34: Operação local de reclassificação para três classes de elevação para Rio Claro/SP.\nOperações focaisAs operações focais levam em consideração uma célula central e seus vizinhos. vizinhança (também chamada de janela móvel - moving window) tipicamente é composta de células de 3 por 3 (célula central e seus oito vizinhos), mas pode assumir outra forma. operação focal aplica uma função de agregação todas células dentro da vizinhança especificada, e usa saída correspondente como o novo valor para célula central, e segue para próxima célula central e seus vizinhos. Essa operação é realizada através da função raster::focal(). O parâmetro x especifica o raster de entrada, o parâmetro w define janela móvel por uma matriz cujos valores correspondem pesos, e por fim, o parâmetro fun especifica função que desejamos aplicar às células, como min(), max(), sum(), mean(), sd() ou var(). Existem diversas aplicações dessa operação para dados raster, como processamento de imagens de satélite (ver mais em Wegmann et al. (2016)). Outra utilidade é para o cálculo de características topográficas, como declividade, aspecto e direções de fluxo. Para calcular essas métricas específicas, podemos utilizar função raster::terrain().Para nosso exemplo, vamos realizar o cálculo desvio padrão da elevação e métrica de aspecto (orientação da vertente) para o raster de elevação em Rio Claro/SP (Figura 15.35).\nFigura 15.35: Cálculo desvio padrão da elevação para uma janela de 3x3 e aspecto para Rio Claro/SP.\nOperações zonaisAs operações zonais aplicam uma função de agregação para várias células de um raster. Geralmente usa-se um segundo raster categórico para definir zonas, de modo que células raster que definem zona não precisam ser vizinhas, como na operação focal. O resultado de uma operação zonal é uma tabela de resumo agrupada por zona, explicando porque essa operação também é conhecida como estatística zonal. Isso é um contraste com operações focais que retornam um objeto raster.operação zonal é realizada através da função raster::zonal(), que recebe de entrada x o raster contínuo, em z o raster categórico, e em fun função que resumirá células. Em nosso exemplo, vamos calcular diversas medidas resumo da elevação com função summary() para cada classe de elevação que criamos anteriormente.Operações globaisAs operações globais usam todo o conjunto de dados raster representando uma única zona. operações globais mais comuns são estatísticas descritivas para todos os pixels raster, utilizando função raster::cellStats() ou raster::freq(), já vistas. Além das estatísticas descritivas, podemos gerar rasters de distância, que calcula distância de cada célula uma ou um grupo células-alvo específica, utilizando função raster::distance().Em nosso exemplo, vamos selecionar os pixels abaixo de 500 m raster de elevação e calcular Distância Euclidiana (Figura 15.36).\nFigura 15.36: Raster de distância Euclidiana dos pixels abaixo de 500 m de elevação para Rio Claro/SP.\n","code":"\n## Filtro espacial\nsf::st_intersects(x = geo_vetor_nascentes, y = geo_vetor_cobertura_floresta)\n#> Sparse geometry binary predicate list of length 1220, where the predicate was `intersects'\n#> first 10 elements:\n#>  1: 1\n#>  2: 1\n#>  3: (empty)\n#>  4: 1\n#>  5: (empty)\n#>  6: (empty)\n#>  7: (empty)\n#>  8: (empty)\n#>  9: 1\n#>  10: (empty)\n## Filtro espacial - interno\ngeo_vetor_nascentes_floresta_int <- geo_vetor_nascentes %>% \n    dplyr::filter(sf::st_intersects(x = ., y = geo_vetor_cobertura_floresta, sparse = FALSE))\n## Filtro espacial com [] - interno\ngeo_vetor_nascentes_floresta_int <- geo_vetor_nascentes[geo_vetor_cobertura_floresta, ]\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_cobertura_floresta$geometry, col = \"forestgreen\", add = TRUE)\nplot(geo_vetor_nascentes_floresta_int$geometry, col = \"blue\", pch = 20, cex = 1, add = TRUE)\n## Filtro espacial - externo\ngeo_vetor_nascentes_floresta_ext <- geo_vetor_nascentes %>% \n    dplyr::filter(sf::st_disjoint(x = ., y = geo_vetor_cobertura_floresta, sparse = FALSE))\n\n## Filtro espacial com [] - externo\ngeo_vetor_nascentes_floresta_ext <- geo_vetor_nascentes[geo_vetor_cobertura_floresta, , op = st_disjoint]\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_cobertura_floresta$geometry, col = \"forestgreen\", add = TRUE)\nplot(geo_vetor_nascentes_floresta_ext$geometry, col = \"steelblue\", pch = 20, cex = 1, add = TRUE)\n## Junção espacial\ngeo_vetor_nascentes_cob_jun <- geo_vetor_nascentes %>% \n    sf::st_join(x = ., y = geo_vetor_cobertura) %>% \n    dplyr::group_by(CLASSE_USO) %>% \n    dplyr::summarise(n = n())\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_nascentes_cob_jun[1], col = c(\"blue\", \"orange\", \"gray30\", \"forestgreen\", \"green\"),\n     pch = 20, add = TRUE)\nlegend(x = 209000, y = 7520000, pch = 15, cex = .7, pt.cex = 2.5, \n       legend = (geo_vetor_nascentes_cob_jun$CLASSE_USO), \n       col = c(\"blue\", \"orange\", \"gray30\", \"forestgreen\", \"green\"))\n## Agregação espacial\ngeo_vetor_cobertura_nas_agre <- geo_vetor_nascentes %>% \n    aggregate(x = ., by = geo_vetor_cobertura, FUN = length)\n## Plot\nplot(geo_vetor_cobertura_nas_agre[1], axes = TRUE, graticule = TRUE, main = NA)\n## Distância espacial\ngeo_vetor_nascentes_dist_flo <- geo_vetor_nascentes %>% \n    dplyr::mutate(dist_flo = sf::st_distance(geo_vetor_nascentes, geo_vetor_cobertura_floresta))\n## Plot\nplot(geo_vetor_nascentes_dist_flo[7], pch = 20, axes = TRUE, graticule = TRUE, main = NA)\n## Soma\ngeo_raster_srtm_rio_claro2 <- geo_raster_srtm_rio_claro + geo_raster_srtm_rio_claro\n\n## Log10\ngeo_raster_srtm_rio_claro_log10 <- log10(geo_raster_srtm_rio_claro)\n## Plot\nold_par <- par(mfrow = c(1, 2))\nplot(geo_raster_srtm_rio_claro2, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n\nplot(geo_raster_srtm_rio_claro_log10, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\npar(old_par)\n## Acima de 600\ngeo_raster_srtm_rio_claro_acima_600 <- geo_raster_srtm_rio_claro > 600\n## Plot\nplot(geo_raster_srtm_rio_claro_acima_600, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Produto dos pixel - calc\ngeo_raster_srtm_rio_claro_prod <- raster::calc(x = geo_raster_srtm_rio_claro, fun = function(x){x * x})\ngeo_raster_srtm_rio_claro_prod \n#> class      : RasterLayer \n#> dimensions : 370, 364, 134680  (nrow, ncol, ncell)\n#> resolution : 0.0008333333, 0.0008333333  (x, y)\n#> extent     : -47.765, -47.46167, -22.55167, -22.24333  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : memory\n#> names      : layer \n#> values     : 241081, 970225  (min, max)\n## Plot\nplot(geo_raster_srtm_rio_claro_prod, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Matriz de reclassificação\nrcl  <- matrix(c(400,600,1, \n                 600,800,2, \n                 800,1000,3), \n               ncol = 3, byrow = TRUE)\n\n## Reclassifição\ngeo_raster_srtm_rio_claro_rcl <- raster::reclassify(x = geo_raster_srtm_rio_claro, rcl = rcl)\n## Plot\nplot(geo_raster_srtm_rio_claro_rcl, col = viridis::viridis(3))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Janela móvel - moving window\ngeo_raster_srtm_rio_claro_focal_sd <- raster::focal(\n    x = geo_raster_srtm_rio_claro, \n    w = matrix(data = 1, nrow = 3, ncol = 3), \n    fun = sd)\n\n## Declividade\ngeo_raster_srtm_rio_claro_asp <- raster::terrain(x = geo_raster_srtm_rio_claro, opt = \"aspect\")\n## Plot\nold_par <- par(mfrow = c(1, 2))\nplot(geo_raster_srtm_rio_claro_focal_sd, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n\nplot(geo_raster_srtm_rio_claro_asp, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\npar(old_par)\n## Estatística zonal\ngeo_raster_srtm_rio_claro_zonal <- data.frame(raster::zonal(geo_raster_srtm_rio_claro, geo_raster_srtm_rio_claro_rcl, fun = \"summary\"))\ncolnames(geo_raster_srtm_rio_claro_zonal) <- c(\"zona\", \"min\", \"1qt\", \"mediana\", \"media\", \"3qt\", \"max\")\ngeo_raster_srtm_rio_claro_zonal\n#>   zona min 1qt mediana    media 3qt max\n#> 1    1 491 552   574.0 567.5995 589 600\n#> 2    2 601 620   640.0 650.6829 670 800\n#> 3    3 801 817   832.5 834.2732 846 985\n## Distância euclidiana\ngeo_raster_srtm_rio_claro_abaixo_500 <- raster::calc(\n    x = geo_raster_srtm_rio_claro, \n    fun = function(x) ifelse(x < 500, 1, NA))\ngeo_raster_srtm_rio_claro_global_dist <- raster::distance(geo_raster_srtm_rio_claro_abaixo_500)\n## Plot\nplot(geo_raster_srtm_rio_claro_global_dist, col = viridis::viridis(10))\nplot(geo_raster_srtm_rio_claro_abaixo_500, add = TRUE, col = \"white\", legend = FALSE)\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)"},{"path":"cap15.html","id":"operações-geométricas","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.9.3 Operações geométricas","text":"operações geométricas realizam modificações em objetos geoespaciais baseado na geometria vetor ou raster e na interação e conversão entre vetor-raster. operações geométricas vetoriais podem ser unárias (funcionam em uma única geometria) ou binárias (modificam uma geometria com base na forma de outra geometria). Ainda podemos fazer transformações para alterar os tipos vetores, que refletirá se feições são únicas ou múltiplas, inclusive na tabela de atributos. operações geométricas em rasters envolvem mudar posição, tamanho e número dos pixels subjacentes e atribuir-lhes novos valores. Por fim, podemos ainda fazer operações de interações e conversões entre raster-vetor para ajustar rasters vetores, assim como converter um objeto espacial vetorial para raster e vice-versa.VetorComo dissemos, operações geométricas em vetores criam ou alteraram geometria de objetos da classe sf, podendo fazer alterações em única geometria (unárias): ) simplificação, ii) centroides, iii) pontos aleatórios, iv) buffers, v) polígono convexo, vi) polígonos de Voronoi, vii) quadrículas e hexágonos; ou modificar uma geometria com base na forma de outra geometria (binárias), viii) união e ix) recortes; ou ainda fazer transformações de tipo de geometrias.Para exemplificar operações geométricas com vetores, vamos utilizar os dados limite, nascentes, hidrologia e cobertura da terra para o município de Rio Claro/SP.SimplificaçãoA simplificação possui o intuito de generalizar linhas ou polígonos, diminuindo assim suas complexidades em relação ao número de vértices. É utilizada para representação em mapas menores ou mapas interativos ou ainda quando um objeto vetorial é muito grande. função utilizada é sf::st_simplify(), que usa o argumento x para uma geometria de entrada e dTolerance para controlar o nível de generalização nas unidades mapa. Em nosso exemplo, simplificaremos hidrografia de Rio Claro/SP (Figura 15.37).\nFigura 15.37: Simplificação da hidrografia para Rio Claro/SP. linhas em azul é hidrografia original e linhas em preto mostram simplificação.\nCentroidesA operação de centroides identifica o centro de objetos geoespaciais, geralmente o centro de massa das feições. É utilizado para gerar um ponto simples para representações complexas ou para estimar distância entre polígonos utilizando esse centroide. Podemos calculá-los com função sf::st_centroids() ou com função sf::st_point_on_surface() para garantir que esses centroides caiam dentro das geometrias. Aqui calcularemos o centroide município de Rio Claro/SP (Figura 15.38).\nFigura 15.38: Centroide limite município de Rio Claro/SP.\nPontos aleatóriosPor vezes precisamos criar algum padrão aleatório dentro de um contexto espacial. Isso pode ser realizado de diversas formas. Uma delas é criação de pontos aleatórios dentro de um polígono. Podemos realizar essa operação com função sf::st_sample(). Para essa função, dois argumentos são utilizados: x uma geometria de entrada e o size indicando o número de pontos à seres criados. Outro argumento bastante interessante é o type, indicando o tipo de amostragem espacial (aleatório, regular ou triangular). Para nosso exemplo, vamos fixar amostragem utilizando função set.seed() e sortear 30 pontos para o limite município de Rio Claro/SP (Figura 15.39). Para mais detalhes da função set.seed(), consultar o Capítulo 4.\nFigura 15.39: Sorteio de 30 pontos aleatório para Rio Claro/SP.\nBufferBuffers são polígonos que representam área dentro de uma determinada distância de um elemento geométrico, independentemente de ser um ponto, linha ou polígono. O buffer é comumente utilizado para análise de dados geoespaciais, geralmente sendo entendio como uma unidade amostral, delimitando uma porção entorno de algum elemento ou evento, como condições climáticas ou da estrutura da paisagem para uma amostragem, ou características de cobertura da terra ao longo de um corpo d’água, e.g., Áreas de Preservação Permanente (APPs).função utilizada para criar buffers é sf::st_buffer(), que requer pelo menos dois argumentos: x uma geometria de entrada e o dist uma distância para o buffer, fornecido nas unidades CRS da geometria de entrada. Em nosso exemplo, vamos criar buffers circulares de 1000 metros para os 30 pontos aleatórios criados anteriormente para o município de Rio Claro/SP (Figura 15.40).\nFigura 15.40: Buffers de 1000 metros para os 30 pontos aleatórios município de Rio Claro/SP.\nPodemos ainda criar buffers quadrados acrescentando o argumento uendCapStyle = \"SQUARE\" (Figura 15.41).\nFigura 15.41: Buffers quadrados de 1000 metros para os 30 pontos aleatórios município de Rio Claro/SP.\nPolígono convexoUma análise bastante comum, principalmente realizada pela IUCN, é criação de polígonos convexos, para definir extensão de ocorrência de uma espécie (Extent occurrence - EOO). Nesse sentido, essa operação liga os pontos externos de um conjunto de pontos e cria um polígono partir deles. Podemos criar esse polígono com função sf::st_convex_hull(). Um único passo que precisamos adiantar é utilizar função sf::st_union() para unir todos os pontos e criar um objeto sf MULTIPOINT, já iremos explicar com mais detalhes adiante. Vamos utilizar os pontos aleatórios que criamos anteriormente para criar o polígono convexo (Figura 15.42).\nFigura 15.42: Polígono convexo para os 30 pontos criados aleatoriamente para Rio Claro/SP.\nPolígonos de VoronoiUma outra forma de criar polígonos para resumir dados espaciais é através dos Polígonos de Voronoi ou Diagrama de Voronoi. Nele, polígonos irregulares são criados partir da proximidade de pontos, de modo estimar uma área de abrangência entorno dos mesmos (Okabe 2000). Esses polígonos podem ser criados com função sf::st_voronoi(), mas precisamos novamente utilizar função sf::st_union() para unir todos os pontos e criar um objeto sf tipo MULTIPOINT. Vamos utilizar os pontos aleatórios que criamos anteriormente para criar o polígono de Voronoi (Figura 15.43).\nFigura 15.43: Polígonos de Voronoi para os 30 pontos criados aleatoriamente para Rio Claro/SP.\nQuadrículas e hexágonosMuitas vezes precisamos criar unidades espaciais idênticas e igualmente espaçadas para resumir informações dispersas por toda nossa área de estudo. Uma prática muito comum é criação de um gride de pontos ou quadrículas em toda área de estudo, e depois utilizar essas geometrias para associar ou resumir informações espacializadas, como IUCN utiliza para análise de área de ocupação (occupancy - AOO). Além das quadrículas, uma outra geometria que se tornou bastante comum para finalidades descritas, é criação de hexágonos, que além de serem mais esteticamente atraentes, possuem uma explicação matemática de sua melhor funcionalidade para análises espaciais em Ecologia (Birch, Oom, Beecham 2007).função utilizada para criar esses grides é sf::st_make_grid(), que requer pelo menos dois argumentos: x uma geometria de entrada e o cellsize indicando o tamanho gride ser criado, fornecido nas unidades CRS da geometria de entrada. Há diversos outros argumentos, mas os mais importantes são o square que definirá se o gride será de quadriculas ou de hexágonos, e o que definirá se geraremos polígonos, cantos ou centroides.Em nosso exemplo, vamos criar quadrículas e hexágonos de 2000 metros (.e. 4000000 metros quadrados) para o município de Rio Claro/SP (Figura 15.44 e Figura 15.45). Podemos ainda utilizar funções de filtros espaciais (Tabela 15.12) para definir como selecionaremos esses elementos para área de estudo. Aqui utilizamos função sf::st_intersects().\nFigura 15.44: Quadrículas de 2000 metros de arestas e centroides para Rio Claro/SP.\n\nFigura 15.45: Hexágonos equivalentes quadrículas de 2000 metros de arestas e centroides para Rio Claro/SP.\nUnião (“dissolver”)Como vimos, na agregação por atributos podemos dissolver geometrias de polígonos mesmo grupo pelos valores da tabela de atributos, onde, naquele exemplo, contabilizamos quantas nascentes haviam por polígono de cobertura da terra para o município de Rio Claro/SP (Figura 15.29).Nesta seção, vamos utilizar função sf::st_union() para unir diversas feições em uma só, dissolvendo os limites entre elas. Vamos utilizar de exemplo os buffers que criamos partir dos 30 pontos aleatórios (Figura 15.46).\nFigura 15.46: União - dissolução - dos buffers criados partir dos 30 pontos aleatórios para Rio Claro/SP.\nRecorte e apagar (“clip” e “erase”)O recorte realiza um subconjunto espacial envolvendo dois objetos geoespaciais. O recorte é aplicado somente linhas e polígonos, ou seja, usaremos linhas e polígonos para recortar linhas ou polígonos. Esse recorte pode ser realizado de três formas: ) intersecção (subconjunto das geometrias sobrepostas entre os dois objetos), ii) diferença (subconjunto das geometrias primeiro objeto sem sobreposição com o segundo objeto), e iii) diferença simétrica (apenas geometrias não sobrepostas entre os dois objetos). Respectivamente para cada uma dessas operações temos funções específicas: sf::st_intersection(), sf::st_difference() e sf::st_sym_difference().Para nosso exemplo, faremos o recorte da hidrografia em relação aos buffers criados e unidos para os 30 pontos aleatórios em Rio Claro/SP. Primeiramente, faremos o recorte para dentro dos buffers com função sf::st_intersection() (Figura 15.47).\nFigura 15.47: Recorte da hidrografia para fora dos buffers dos 30 pontos aleatórios para Rio Claro/SP.\nPara nosso segundo exemplo, realizamos o recorte da hidrografia em relação aos buffers, mas agora para fora dos buffers utilizando função sf::st_difference() (Figura 15.48), que seria semelhando operação de apagar (“erase”).\nFigura 15.48: Recorte da hidrografia para fora dos buffers dos 30 aleatórios para Rio Claro/SP.\nTransformações de tipoEsse tópico possui muitas funcionalidades, que são exploradas tópico “5.2.7 Type transformations” de Lovelace et al. (2019). Aqui, nosso interesse principal é em relação à transformação dos tipos de objetos geoespaciais da classe sf: MULTIPOINT, MULTILINESTRING e MULTIPOLYGON, para POINT, LINESTRING e POLYGON. Muitas vezes feições de nossos objetos, .e., linhas da tabela de atributos estão agrupadas em apenas uma linha da tabela. Quando o objeto espacial está nesse formato, geralmente em alguma classe dessas (MULTIPOINT, MULTILINESTRING e MULTIPOLYGON), não temos como realizar operações espaciais ou geométricas para cada feição, e precisamos separá-las em linhas diferentes para que operações como o cálculo de comprimento ou área seja possível para cada feição.Dessa forma, podemos utilizar função sf::st_cast() para fazer essas transformações e atribuir cada feição uma linha da tabela de atributos. Como exemplo, vamos separar os fragmentos de floresta e calcular área para cada feição em hectares (Figura 15.49).\nFigura 15.49: Transformação vetor de florestas em POLYGON e cálculo da área para cada feição para Rio Claro/SP.\nRasterAs operações geométricas em rasters envolvem mudar posição, tamanho e número dos pixels e atribuir novos valores, geralmente aumentando ou diminuindo o tamanho desses pixels. Essas operações permitem alinhar rasters de diversas fontes, fazendo com que compartilhem uma correspondência entre seus pixels, permitindo que eles sejam processados todos juntos, ou simplesmente permite realização de análises que demorariam muito, caso os rasters possuam um tamanho de pixel muito pequeno. 📝 Importante \nEssas operações funcionam para três classes dos objetos raster: RasterLayer, RasterBrick e RasterStack.Para exemplificar operações geométricas com rasters, vamos utilizar os dados de elevação para o município de Rio Claro/SP e bioclimáticos para o mundo.AgregaçãoNa agregação de rasters, aumentamos o tamanho dos pixels (diminuindo resolução), agregando os valores dos pixels em um pixel maior. Podemos realizar essa operação com função raster::aggregate(), que possui três argumentos: x corresponde ao objeto raster de entrada, fact é o fator de agregação e corresponde ao número que definirá o novo tamanho pixel (e.g., se um raster tem resolução de 90 m, um fator de agregação de 10 fará com o novo raster tenha resolução de 900 m), e fun é função utilizada para realizar agregação dos pixels (Figura 15.50).Em nosso exemplo, vamos aumentar o tamanho dos pixels para 900 metros raster de elevação para Rio Claro/SP.\nFigura 15.50: Agregação (aumento pixel para 900 metros) utilizando média para o raster de elevação para Rio Claro/SP.\nDesagregaçãoDe modo contrário, na desagregação de rasters, diminuímos o tamanho dos pixels (aumentando resolução), preenchendo com novos valores. Podemos realizar essa operação com função raster::desaggregate(), que assim como função anterior, possui três argumentos: x corresponde ao objeto raster de entrada, fact é o fator de desagregação e corresponde ao número que definirá o novo tamanho pixel (e.g., se um raster tem resolução de 90 m, um fator de desagregação de 2 fará com que o novo raster tenha resolução de 45 m), e method é função utilizada para realizar desagregação dos pixels (Figura 15.51).Nesse exemplo, vamos diminuir o tamanho dos pixels para 45 metros raster de elevação para Rio Claro/SP.\nFigura 15.51: Desagregação (diminuição pixel para 45 metros) utilizando o método bilinear para o raster de elevação para Rio Claro/SP.\nAlinhamento de rastersMuitas vezes queremos ir além de ajustar o tamanho pixel, ajustando também extensão, número e origem dos pixels para várias camadas rasters, principalmente se precisamos criar objetos das classes RasterBrick ou RasterStack. Dessa forma, podemos utilizar função raster::compareRaster() para comparar os rasters em relação extensão, número de linhas e colunas, projeção, resolução e origem (ou um subconjunto dessas comparações).Podemos utilizar função raster::resample() para fazer esse alinhamento, ou ainda função gdalUtils::align_rasters(). Para primeira função, os argumentos são x para o raster de entrada, y para o raster de alinhamento e method para o método utilizado alinhamento. Para nosso exemplo, vamos ajustar uma camada bioclimática (BIO01) à camada de elevação para Rio Claro/SP (Figura 15.52).\nFigura 15.52: Reamostragem (alinhamento dos rasters) utilizando o método bilinear para alinhar o raster bioclimático (BIO01) ao de elevação para Rio Claro/SP.\nInterações raster-vetorPodemos fazer operações da interação entre objetos vetoriais e raster, como ajustes da extensão e limite raster para vetores (corte e máscara), extração dos valores dos pixels para vetores (pontos, linhas e polígonos), e estatísticas zonais dos valores dos pixels dos raster para um vetor (linhas e polígonos).Cortes e máscarasMuitas vezes precisamos ajustar o tamanho de um objeto raster uma área menor de interesse, geralmente definido por um objeto vetorial. Para realizar essa operação, dispomos de duas funções: raster::crop() e raster::mask(). 📝 Importante \nÉ fundamental que ambos os objetos raster ser reduzido e o vetor como molde precisam estar mesmo CRS.função raster::crop() ajusta o raster à extensão vetor. Como exemplo, vamos retomar o raster de elevação original baixado e importado anteriormente (Figura 15.14). Primeiramente, vamos usar função raster::crop() para ajustar esse raster à extensão limite município de Rio Claro/SP (Figura 15.53).\nFigura 15.53: Ajuste da extensão raster de elevação para extensão de Rio Claro/SP.\nPara ajustar o raster ao limite município de Rio Claro/SP, vamos usar função raster::mask(). É importante notar que essa função preenche com NAs os pixels que estão fora limite polígono e não ajusta extensão (Figura 15.53).\nFigura 15.54: Ajuste raster de elevação para o limite de Rio Claro/SP.\nPara ajustar o raster à extensão e ao limite município de Rio Claro/SP, precisamos utilizar conjuntamente funções raster::crop() e raster::mask() (Figura 15.55).\nFigura 15.55: Ajuste da extensão e limite raster de elevação para Rio Claro/SP.\nfunção raster::mask() possui ainda um argumento chamado inverse, que cria uma máscara inversa ao limite, preenchendo com NA o pixels internos ao limite polígono, como podemos ver para o raster de elevação e o limite de Rio Claro/SP (Figura 15.56).\nFigura 15.56: Ajuste da extensão e limite externo raster de elevação para Rio Claro/SP.\nExtraçãoA interação entre raster-vetor de extração é o processo que identifica e retorna valores associados de pixels de um raster com base em um objeto vetorial. É uma operação extremamente comum em análises espaciais, principalmente para associar valores de raster ambientais (contínuos ou categóricos) pontos de ocorrência ou amostragem. Os valores retornados dependerão tipo vetor (pontos, linhas ou polígonos) e de argumentos da função raster::extract() que alteram o funcionamento da extração.Em nosso exemplo, vamos extrair os valores raster de elevação para nascentes município de Rio Claro/SP (Figura 15.57).\nFigura 15.57: Extração dos valores de elevação para nascentes de Rio Claro/SP.\nAlém da extração dos valores totais, podemos resumir os valores dos pixels com mesma operação de extração, utilizando ainda função raster::extract(), mas utilizando uma outra função para resumir os valores dos pixels para um polígono, operação também denominada de estatística zonal (agora para vetores). Já vimos que ela pode ser realizada entre rasters na seção de operações zonais, mas aqui realizaremos para rasters e vetores.Para o exemplo, vamos calcular elevação média dos valores para os hexágonos que criamos para o limite de Rio Claro/SP( Figura 15.58).\nFigura 15.58: Extração dos valores de elevação e resumo pela média para os hexágonos de Rio Claro/SP.\nConversões raster-vetorPor fim, podemos ainda fazer operações de conversão entre objetos vetoriais para raster e vice-versa. Nessas operações, podemos resumir ou transformar objetos vetoriais (pontos, linhas ou polígonos) para rasters, escolhendo um raster previamente existente, processo denominado rasterização. Também podemos realizar o processo inverso, .e., transformar o raster em um vetor, podendo esse vetor ser pontos, linhas ou polígonos, operação chamada de vetorização.RasterizaçãoA conversão de vetor para raster pode ser realizada de pontos, linhas ou polígonos para rasters. Nesse processo, podemos utilizar uma função para resumir os dados pontuais para os pixels raster que criaremos. Para essa operação, podemos utilizar função raster::rasterize(), com o argumento x sendo o vetor de entrada, y o raster base, field coluna ou campo da tabela de atributos objeto vetorial para os quais os valores serão utilizados e fun função utilizada para agregação dos dados.Aqui, vamos contabilizar quantidade de nascentes por pixel, utilizando como base o raster para o qual mudamos resolução para 900 metros (Figura 15.59).\nFigura 15.59: Rasterização das nascentes, com operação de contabilização de pontos para Rio Claro/SP.\nAlém de pontos, podemos também rasterizar linhas. Aqui vamos contabilizar linhas da hidrografia simplificada para Rio Claro/SP (Figura 15.60).\nFigura 15.60: Rasterização da hidrografia, com operação de contabilização de linhas para Rio Claro/SP.\nPodemos ainda rasterizar polígonos, de modo que cada pixel raster ser criado receberá o valor da tabela de atributos, ou uma análise pelo vizinho mais próximo caso de um campo categórico, como cobertura da terra, que também vai depender da resolução raster base e tamanho da feição polígono. Para nosso exemplo, antes de criar o raster vamos transforma coluna de classe de cobertura da terra em factor (Figura 15.61). Entretanto, essa operação de rasterização tende demorar muito caso de polígonos muito detalhados ou um raster com pixels muito pequenos, sendo que dois pacotes aceleram esse processamento (fasterize e gdalUtils), com suas respectivas funções: fasterize::fasterize() e gdalUtils::gdal_rasterize().\nFigura 15.61: Rasterização da cobertura da terra para Rio Claro/SP.\nVetorizaçãoA operação inversa à rasterização é vetorização, na qual convertemos um raster em um vetor, sendo que esse vetor receberá os valores dos pixels. O vetor em questão pode ser pontos (geralmente um gride de pontos), linhas (geralmente isolinhas ou linhas de contorno), ou polígonos (podendo esses polígonos ser ou não dissolvidos pelos valores dos pixels). Existem funções específicas para cada uma dessas conversões, sendo elas: raster::rasterToPoints(), raster::rasterToContour() e raster::rasterToPolygons(), respectivamente. Para última função, ainda dispomos de uma alternativa mais veloz spex::polygonize().Em nosso exemplo, vamos vetorizar o raster de elevação para Rio Claro/SP, criando um gride de pontos, sendo os pontos os centroides de cada pixels (Figura 15.62).\nFigura 15.62: Vetorização raster de elevação criando um gride de pontos para Rio Claro/SP.\nNeste outro exemplo, vamos vetorizar o raster de elevação para Rio Claro/SP novamente, mas agora criando isolinhas (Figura 15.63).\nFigura 15.63: Vetorização raster de elevação criando isolinhas para Rio Claro/SP.\nPor fim, vamos vetorizar o raster de cobertura da terra criado anteriormente para Rio Claro/SP, criando polígonos não dissolvendo e dissolvidos (Figura 15.64).\nFigura 15.64: Vetorização raster de cobertura da terra para Rio Claro/SP, não dissolvendo e dissolvendos os polígonos gerados.\n","code":"\n## Simplificação\ngeo_vetor_hidrografia_simplificado <- sf::st_simplify(x = geo_vetor_hidrografia, dTolerance = 1000)\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_hidrografia$geometry, col = \"steelblue\", lwd = 2, add = TRUE)\nplot(geo_vetor_hidrografia_simplificado$geometry, col = adjustcolor(\"black\", .7), add = TRUE)\n## Centroides\ngeo_vetor_rio_claro_sirgas2000_utm23s_cent <- sf::st_centroid(geo_vetor_rio_claro_sirgas2000_utm23s)\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_cent$geom, cex = 3, pch = 20, add = TRUE)\n## Fixar amostragem\nset.seed(42)\n\n## Pontos aleatórios\ngeo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios <- sf::st_sample(geo_vetor_rio_claro_sirgas2000_utm23s, size = 30)\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios, pch = 20, add = TRUE)\n## Buffer\ngeo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios_buffer <- sf::st_buffer(\n    x = geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios, dist = 1000)\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios_buffer, col = NA, lwd = 2, border = \"red\", add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios, pch = 20, cex = 1, add = TRUE)\n## Buffer\ngeo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios_buffer_quad <- sf::st_buffer(\n    x = geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios, dist = 1000, endCapStyle = \"SQUARE\")\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios_buffer_quad, col = NA, lwd = 2, border = \"red\", add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios, pch = 20, cex = 1, add = TRUE)\n## Polígono convexo\ngeo_vetor_rio_claro_sirgas2000_utm23s_convexo <- geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios %>% \n    sf::st_union() %>% \n    sf::st_convex_hull()\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_convexo, col = NA, lwd = 2, border = \"red\", add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios, pch = 20, cex = 1, add = TRUE)\n## Polígonos de Voronoi\ngeo_vetor_rio_claro_sirgas2000_utm23s_voronoi <- geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios %>% \n    sf::st_union() %>% \n    sf::st_voronoi()\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_voronoi, col = NA, lwd = 2, border = \"red\", add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios, pch = 20, cex = 1, add = TRUE)\n## Quadrículas\ngeo_vetor_rio_claro_sirgas2000_utm23s_grid <- sf::st_make_grid(\n    x = geo_vetor_rio_claro_sirgas2000_utm23s, cellsize = 2000, what = \"polygons\") %>%\n    sf::st_as_sf() %>%\n    dplyr::filter(sf::st_intersects(x = ., y = geo_vetor_rio_claro_sirgas2000_utm23s, sparse = FALSE))\n\n## Centroides das quadrículas\ngeo_vetor_rio_claro_sirgas2000_utm23s_grid_cent <- geo_vetor_rio_claro_sirgas2000_utm23s %>% \n    sf::st_make_grid(cellsize = 2000, what = \"centers\") %>%\n    sf::st_as_sf() %>%\n    dplyr::filter(sf::st_intersects(x = ., y = sf::st_union(geo_vetor_rio_claro_sirgas2000_utm23s_grid), sparse = FALSE))\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_grid, col = NA, border = \"red\", lwd = 2, add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_grid_cent, pch = 20, add = TRUE)\n## Hexágonos\ngeo_vetor_rio_claro_sirgas2000_utm23s_hex <- geo_vetor_rio_claro_sirgas2000_utm23s %>% \n    sf::st_make_grid(cellsize = 2000, square = FALSE) %>% \n    sf::st_as_sf() %>%\n    dplyr::filter(sf::st_intersects(x = ., \n                                    y = geo_vetor_rio_claro_sirgas2000_utm23s, \n                                    sparse = FALSE))\n\n## Centroides de hexágonos\ngeo_vetor_rio_claro_sirgas2000_utm23s_hex_cent <- geo_vetor_rio_claro_sirgas2000_utm23s %>% \n    sf::st_make_grid(cellsize = 2000, square = FALSE, what = \"centers\") %>% \n    sf::st_as_sf() %>% \n    dplyr::filter(sf::st_intersects(x = ., \n                                    y = sf::st_union(geo_vetor_rio_claro_sirgas2000_utm23s_hex),\n                                    sparse = FALSE))\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_hex, col = NA, border = \"red\", lwd = 2, add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_hex_cent, pch = 20, add = TRUE)\n## União\ngeo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios_buffer_uniao <- sf::st_union(geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios_buffer)\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios_buffer_uniao, col = adjustcolor(\"blue\", .1), add = TRUE)\n## Recorte - intersecção\ngeo_vetor_hidrografia_interseccao <- sf::st_intersection(x = geo_vetor_hidrografia, y = geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios_buffer_uniao)\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios_buffer_uniao, col = adjustcolor(\"blue\", .1), add = TRUE)\nplot(geo_vetor_hidrografia_interseccao$geometry, col = \"blue\", add = TRUE)\n## Recorte - diferença\ngeo_vetor_hidrografia_diferenca <- sf::st_difference(x = geo_vetor_hidrografia, y = geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios_buffer_uniao)\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_pontos_aleatorios_buffer_uniao, col = adjustcolor(\"blue\", .1), add = TRUE)\nplot(geo_vetor_hidrografia_diferenca$geometry, col = \"blue\", add = TRUE)\n## Transformação de tipo\ngeo_vetor_cobertura_floresta_polygon <- geo_vetor_cobertura_floresta %>% \n    sf::st_cast(\"POLYGON\") %>% \n    dplyr::mutate(area_ha = sf::st_area(.)/1e4 %>% round(2))\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = \"gray\", main = NA, axes = TRUE, graticule = TRUE)\nplot(geo_vetor_cobertura_floresta_polygon[\"area_ha\"], col = viridis::viridis(100),  add = TRUE)\n## Agregação - aumentar o tamanho do pixel\ngeo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media <- raster::aggregate(x = geo_raster_srtm_rio_claro_sirgas2000_utm23s, fact = 10, fun = \"mean\")\ngeo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media\n#> class      : RasterLayer \n#> dimensions : 40, 37, 1480  (nrow, ncol, ncell)\n#> resolution : 900, 900  (x, y)\n#> extent     : 214554.4, 247854.4, 7502625, 7538625  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=utm +zone=23 +south +ellps=GRS80 +units=m +no_defs \n#> source     : memory\n#> names      : srtm_27_17 \n#> values     : 506.0024, 922.8709  (min, max)\n## Plot\nplot(geo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Desagregação - diminuir o tamanho do pixel\ngeo_raster_srtm_rio_claro_desg_bil <- raster::disaggregate(x = geo_raster_srtm_rio_claro_sirgas2000_utm23s, fact = 2, method = \"bilinear\")\ngeo_raster_srtm_rio_claro_desg_bil\n#> class      : RasterLayer \n#> dimensions : 792, 728, 576576  (nrow, ncol, ncell)\n#> resolution : 45, 45  (x, y)\n#> extent     : 214554.4, 247314.4, 7502985, 7538625  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=utm +zone=23 +south +ellps=GRS80 +units=m +no_defs \n#> source     : memory\n#> names      : srtm_27_17 \n#> values     : 493.7046, 986.5187  (min, max)\n## Plot\nplot(geo_raster_srtm_rio_claro_desg_bil, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Reamostragem\ngeo_raster_bioclim_rc <- raster::resample(x = geo_raster_bioclim$bio01, \n                                          y = geo_raster_srtm_rio_claro, \n                                          method = \"bilinear\")\ngeo_raster_bioclim_rc\n#> class      : RasterLayer \n#> dimensions : 370, 364, 134680  (nrow, ncol, ncell)\n#> resolution : 0.0008333333, 0.0008333333  (x, y)\n#> extent     : -47.765, -47.46167, -22.55167, -22.24333  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : memory\n#> names      : bio01 \n#> values     : 19.8383, 20.60492  (min, max)\n## Plot\nplot(geo_raster_bioclim_rc$bio01, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Crop - adjuste da extensão\ngeo_raster_srtm_rio_claro_crop <- raster::crop(geo_raster_srtm, geo_vetor_rio_claro)\n## Plot\nplot(geo_raster_srtm_rio_claro_crop, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Mask - adjuste ao limite\ngeo_raster_srtm_rio_claro_mask <- raster::mask(geo_raster_srtm, geo_vetor_rio_claro)\n## Plot\nplot(geo_raster_srtm_rio_claro_mask, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Crop e mask - ajuste da extensão e do limite\ngeo_raster_srtm_rio_claro_crop_mask <- geo_raster_srtm %>% \n    raster::crop(geo_vetor_rio_claro) %>% \n    raster::mask(geo_vetor_rio_claro)\n## Plot\nplot(geo_raster_srtm_rio_claro_crop_mask, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Crop e mask inversa - ajuste da extensão e do limite inverso\ngeo_raster_srtm_rio_claro_crop_mask_inv <- geo_raster_srtm %>% \n    raster::crop(geo_vetor_rio_claro) %>% \n    raster::mask(geo_vetor_rio_claro, inverse = TRUE)\n## Plot\nplot(geo_raster_srtm_rio_claro_crop_mask_inv, col = viridis::viridis(10))\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Extração\ngeo_vetor_nascentes_ele <- geo_vetor_nascentes %>% \n    dplyr::mutate(elev = raster::extract(x = geo_raster_srtm_rio_claro_sirgas2000_utm23s, y = .))\n## Plot\nplot(geo_vetor_nascentes_ele[\"elev\"], \n     pch = 20, main = NA, axes = TRUE, graticule = TRUE)\n## Extração - estatística por zonas\ngeo_vetor_rio_claro_sirgas2000_utm23s_hex_alt <- geo_vetor_rio_claro_sirgas2000_utm23s_hex %>% \n    dplyr::mutate(elev_mean = raster::extract(x = geo_raster_srtm_rio_claro_sirgas2000_utm23s, \n                                              y = geo_vetor_rio_claro_sirgas2000_utm23s_hex, \n                                              fun = mean, \n                                              na.rm = TRUE))\n## Plot\nplot(geo_vetor_rio_claro_sirgas2000_utm23s_hex_alt[\"elev_mean\"], \n     pch = 20, main = NA, axes = TRUE, graticule = TRUE)\n## Rasterizar pontos\ngeo_vetor_nascentes_rasterizacao <- raster::rasterize(x = geo_vetor_nascentes, y = geo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media, field = 1, fun = \"count\")\n## Plot\nplot(geo_vetor_nascentes_rasterizacao, col = viridis::viridis(10))\nplot(geo_vetor_nascentes$geometry, pch = 20, cex = .5, col = adjustcolor(\"gray\", .5), add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Rasterizar linhas\ngeo_vetor_hidrografia_rasterizacao <- raster::rasterize(\n    x = geo_vetor_hidrografia_simplificado,\n    y = geo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media,\n    field = 1, fun = \"count\")\n## Plot\nplot(geo_vetor_hidrografia_rasterizacao, col = viridis::viridis(10))\nplot(geo_vetor_hidrografia_simplificado$geom, col = \"gray\", add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Rasterizar polígonos\ngeo_vetor_cobertura_rasterizacao <- geo_vetor_cobertura %>% \n    dplyr::mutate(classe = as.factor(CLASSE_USO)) %>% \n    raster::rasterize(x = ., y = geo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media, field = \"classe\")\n## Plot\nplot(geo_vetor_cobertura_rasterizacao, col = viridis::viridis(10))\nplot(geo_vetor_cobertura$geom, add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Vetorização de pontos\ngeo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media_pontos <- raster::rasterToPoints(geo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media, spatial = TRUE) %>% \n    sf::st_as_sf()\n## Plot\nplot(geo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media, \n     col = viridis::viridis(10, alpha = .8))\nplot(geo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media_pontos, \n     pch = 20, cex = .7, main = FALSE, add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, col = NA, \n     border = \"red\", lwd = 2, add = TRUE)\n## Vetorização de linhas\ngeo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media_linhas <- raster::rasterToContour(\n    x = geo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media) %>% \n    sf::st_as_sf()\n## Plot\nplot(geo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media,\n     col = viridis::viridis(10, alpha = .8))\ncontour(geo_raster_srtm_rio_claro_sirgas2000_utm23s_agre_media, \n        labcex = 1, main = FALSE, add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, \n     col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Vetorização de polígonos\ngeo_vetor_cobertura_rasterizacao_poligonos <- raster::rasterToPolygons(geo_vetor_cobertura_rasterizacao) %>%\n    sf::st_as_sf()\n\n## Vetorização de polígonos dissolvendo\ngeo_vetor_cobertura_rasterizacao_poligonos_dissolvidos <- raster::rasterToPolygons(geo_vetor_cobertura_rasterizacao, dissolve = TRUE) %>% \n    sf::st_as_sf()\n## Plot\nold_par <- par(mfrow = c(1, 2))\nplot(geo_vetor_cobertura_rasterizacao, col = viridis::viridis(10))\nplot(geo_vetor_cobertura_rasterizacao_poligonos$geometry, \n     col = NA, border = \"gray\", lwd = 1, main = FALSE, add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, \n     col = NA, border = \"red\", lwd = 2, add = TRUE)\n\nplot(geo_vetor_cobertura_rasterizacao, col = viridis::viridis(10))\nplot(geo_vetor_cobertura_rasterizacao_poligonos_dissolvidos$geometry, \n     col = NA, border = \"gray\", lwd = 1, main = FALSE, add = TRUE)\nplot(geo_vetor_rio_claro_sirgas2000_utm23s$geom, \n     col = NA, border = \"red\", lwd = 2, add = TRUE)\npar(old_par)"},{"path":"cap15.html","id":"visualização-de-dados-geoespaciais","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.10 Visualização de dados geoespaciais","text":"Um dos pontos finais de toda análise envolvendo manipulação de dados geoespaciais será apresentação de um mapa com informações de interesse geoespacializadas. Mas antes, é necessário ter conhecimento de alguns dos elementos principais para composição de um mapa relativamente bem informativo. Além disso, o R nos permite criar tipos diferentes de mapas: estáticos, animados e interativos. Os mais comuns são os estáticos, mas podemos por vezes melhorar apresentação dos dados geoespaciais criando mapas animados e/ou interativos, com o auxílio de páginas web. Por fim, veremos melhores formas de exportar mapas para diferentes formatos.","code":""},{"path":"cap15.html","id":"principais-elementos-de-um-mapa","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.10.1 Principais elementos de um mapa","text":"Um mapa pode ser composto de vários elementos, tendo estes o intuito de auxiliar visualização e entendimento de seu conteúdo. Apesar disso, nem todos os elementos necessitam estar presentes em todos os layouts de mapas, sendo que os mesmos devem atendem à necessidade das representações, podendo ser muitas vezes omitidos ou outros podem ser acrescentados.Os principais elementos de um mapa geralmente são compostos por:Mapa principal (ocupando quase toda área da figura)Mapa secundário (geralmente muito menor que o mapa principal e com o intuito de mostrar localização mapa principal num contexto mais amplo, como país ou continente)Título (para resumir o intuito mapa)Legenda (apresentando informações detalhadas das classes ou escala de valores, geralmente identificando cores e/ou texturas),Barra de escala (representando relação entre unidades mapa e o mundo real)Indicador de orientação (Norte) (indicando o norte geográfico, podendo ser representado por uma flecha, bússola ou compasso)Gride de coordenadas (coordenadas presentes nas laterais)Descrição CRS (indicando qual o Sistema de Referência de Coordenadas)Informações de origem (informações sobre fonte dos dados representados mapa)Outros elementos auxiliares (como elementos textuais e figuras extras)Podemos visualizar todos esses elementos resumidos na Figura 15.65.\nFigura 15.65: Principais elementos de um mapa.\n","code":""},{"path":"cap15.html","id":"principais-pacotes-para-a-composição-de-mapas","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.10.2 Principais pacotes para a composição de mapas","text":"Há uma grande quantidade de pacotes para composição de mapas R. Aqui listamos os principais (Tabela 15.13).\nTabela 15.13: Principais pacotes para composição de mapas R.\n","code":""},{"path":"cap15.html","id":"mapas-estáticos","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.10.3 Mapas estáticos","text":"Mapas estáticos são mapas simples e fixos para visualização de dados, sendo o tipo mais comum de saída visual. início da composição de mapas R, esse era o único tipo de mapa que linguagem permitia produzir, principalmente utilizando o pacote sp (E. J. Pebesma Bivand 2005). entanto, com o advento de ferramentas de visualização dinâmicas R, como componentes HTML, os mapas puderam ser compostos de forma dinâmica (animados e interativos).Neste tópico abordaremos funções simples para composição de mapas estáticos, como o plot(), além de pacotes para composição de mapas mais elaborados, como os pacotes ggplot2 (Wickham et al. 2020) e tmap (Tennekes 2021).Função plot()função genérica plot() é maneira mais rápida de compor mapas estáticos utilizando objetos geoespaciais vetoriais e raster, funcionando para ambos os pacotes que apresentamos anteriormente (sf e raster). Apesar da simplicidade, essa função geralmente tende criar mapas com relativa velocidade, nos auxiliando principalmente em fases iniciais de desenvolvimento de um projeto. Essa função oferece dezenas de argumentos em R Base, permitindo alguns ajustes limitados, com resultados bastante interessantes.Como dito anteriormente, função plot() vai funcionar diferentemente dependendo da classe objeto geoespacial. Para objetos geoespaciais sf, função vai plotar um mapa para cada coluna da tabela de atributos. Vamos usar de exemplo nosso mapa de biomas mostrado com os principais elementos de um mapa, podendo inclusive selecionar apenas coluna de características geoespaciais (geom).Primeiramente, vamos fazer o download dos dados de limites de biomas, retirando os sistemas costeiros, usando o pacote geobr (Pereira Goncalves 2021).Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.Agora, quando utilizamos função plot() para um objeto da classe sf, temos os três mapas, cada um indicando uma coluna da tabela de atritos (Figura 15.66).\nFigura 15.66: Mapa feito com função plot() de um objeto sf para os Biomas Brasil.\nSelecionando colunas desse objeto, podemos escolher informação que queremos plotar, por exemplo, apenas geometria geom. Além disso, podemos acrescentar os argumentos col para colorir e main para o título, além dos argumentos axes e graticule para adicionar coordenadas e quadrículas, respectivamente. legenda pode ser adicionada com função legend() (Figura 15.67).\nFigura 15.67: Mapa feito com função plot() de um objeto vetor.\nPara classe dos objetos geoespaciais raster, função plot() vai plotar um mapa para o tipo RasterLayer e quantos mapas houverem objeto e couberem espaço de plot para RasterBrick e RasterStack. Além disso, para essas classes objeto raster, essa função provê também uma legenda e uma escala de cores automática (terrain). Vamos fazer o mapa da camada raster de elevação para o limite município de Rio Claro/SP (Figura 15.68).\nFigura 15.68: Mapa feito com função plot() de um objeto raster com uma camada.\nAgora vamos plotar objetos da classe RasterStack, alterando cor para viridis, usando função viridis::viridis() pacote homônimo. Vamos fazer o mapa de duas camadas raster bioclimáticas para o mundo (Figura 15.69).\nFigura 15.69: Mapa feito com função plot() de um objeto raster com várias camadas.\nPara exportar esses mapas podemos utilizar funções png() ou pdf(), indicando os argumentos para ter configurações que desejamos, e finalizando com função dev.(). Vamos exportar, título de exemplo, última figura (mais detalhes Capítulo 6).Pacotes ggplot2 e ggspatialComo discutimos Capítulo 6 sobre gráficos, o pacote ggplot2 utiliza gramática de gráficos para composição de figuras R (Wilkinson Wills 2005; Wickham 2016). Para cada classe de objeto geográfico há funções específicas para os dados: para objetos sf geom_sf() e para objetos raster geom_raster() e geom_tile().Além pacote ggplot2, podemos utilizar o pacote ggspatial para acrescentar elementos geoespaciais como barra de escala e o indicador de orientação (Norte), através das funções annotation_scale() e annotation_north_arrow(), respectivamente, além de outras funções específicas que não abordaremos nesta seção.estrutura de composição das funções pacote ggplot2 vai funcionar parecido com estruturação de gráficos já vista Capítulo 6, de modo que cada função iremos utilizando o sinal de + para acrescentar outra camada. Indicaremos os dados com função ggplot() e coluna da tabela de atributos que queremos representar com função aes(). Em seguida, utilizamos função geom_sf() para indicar que trata-se de um objeto sf.Além dessas funções, podemos ainda fazer alterações nos mapas através das funções: scale_*() que vai alterar características indicadas em aes(), coord_*() que vai alterar construção mapa em relação às coordenadas, facet_*() que altera disposição de vários mapas, e theme_*() e theme() que alterarão características relacionadas ao tema, como cores de fundo, fontes e legenda. Podemos ainda utilizar funções annotate() para adicionar textos e labs() para alterar o texto título, legenda e eixos.Vamos demonstrar esse funcionamento para compor o mapa de biomas, apresentado início desta seção (Figura 15.70).\nFigura 15.70: Mapa de Biomas Brasil com o pacote ggplot2.\nPara objetos raster, o uso pacote ggplot2 para compor mapas requer um passo preliminar. Primeiramente, vamos criar um data frame com os dados raster, com linhas sendo os pixels e colunas sendo coordenadas centrais da longitude e latitude, além dos valores de cada camada em cada coluna. Esse passo pode ser realizado com função raster::rasterToPoints() ou raster::.data.frame().Uma vez que temos esses dados organizados, podemos utilizar funções ggplot() para indicar o data frame, e colunas com função aes(). Em seguida, utilizamos função geom_raster() para indicar que trata-se de um objeto raster. Além dessas funções, podemos ainda utilizar demais funções para alterar características mapa, como comentamos acima. Entretanto, devemos nos atentar para função coord_*() e escolher aquela que vai fazer construção mapa em relação à coordenadas e resolução das células.Como exemplo, vamos compor o mapa de elevação para Rio Claro/SP, adicionando também o limite município (Figura 15.71).\nFigura 15.71: Mapa raster com o pacote ggplot2.\nPara exportar mapas criados com o pacote ggplot2, podemos utilizar função ggplot2::ggsave(), indicando os argumentos para ter configurações que desejamos. Vamos exportar, título de exemplo, última figura.Pacote tmapO pacote tmap é um pacote direcionado à criação de mapas temáticos, com uma sintaxe concisa que permite criação de mapas com o mínimo de códigos, mas muito similar à sintaxe pacote ggplot2 (Tennekes 2018). Ele também pode gerar mapas estáticos ou interativos usando o mesmo código, apenas mudando forma de visualização com função tmap_mode(), com o argumento mode igual “plot” para estático e “view” para interativo. Por fim, o pacote tmap aceita diversas classes espaciais, incluindo objetos raster, de forma bastante simples. Mais sobre o pacote pode ser lido aqui. 📝 Importante \nAtentar para instalação extra em Sistemas Operacionais GNU/Linux e MacOS.Todas funções pacote tmap iniciam-se com tm_*, facilitando seu uso. cada função iremos utilizar o sinal de + para acrescentar outra camada, da mesma forma que o pacote ggplot2. principal função, em que todos os objetos geoespaciais são dados de entrada, é tm_shape(). partir dela, podemos seguir com funções específicas para visualização de objetos sf, como tm_polygons(), tm_borders(), tm_fill(), tm_lines(), tm_dots() ou tm_bubbles(), ou com funções para objetos raster como tm_raster(). Ainda há funções como tm_text() para representação de textos das colunas da tabela de atributos, e tm_scale_bar(), tm_compass() e tm_graticules(), para adicionar barra de escala, indicador de orientação (Norte) e gride de coordenadas, respectivamente. Por fim, função tm_credits() adiciona um texto descritivo e função tm_layout() faz diversas mudanças nos detalhes e apresentação mapa.Uma funcionalidade muito interessante pacote tmap é o uso da função tmaptools::palette_explorer() para escolher paletas de cores disponíveis. Essa função requer que os pacotes shiny e shinyjs estejam instalados, e quando executada, retorna uma aba onde é possível editar e escolher algumas paletas de cores nativas tmap.Diversos parâmetros podem ser acrescentados às funções de composição tmap, mas não detalharemos aqui, pois todas são descritas nos vignettes pacote: tmap: get started! e tmap: version changes.Vamos seguir com composição mapa de biomas para o Brasil apresentado início dessa seção (Figura 15.72).\nFigura 15.72: Mapa de Biomas Brasil com o pacote tmap.\nAlém disso, o pacote tmap nos permite adicionar de forma simples um mapa secundário, provendo uma localização regional de interesse (Figura 15.73).Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.\nFigura 15.73: Mapa vetorial primário e secundário com o pacote tmap.\nComo exemplo de mapa raster, vamos compor novamente o mapa de elevação para Rio Claro/SP, adicionando também o limite município (Figura 15.74).\nFigura 15.74: Mapa raster de elevação com o pacote tmap.\nPara exportar mapas criados com o pacote tmap podemos utilizar função tmap::tmap_save(), indicando os argumentos para ter configurações que desejamos. Vamos exportar, título de exemplo, última figura.","code":"\n## Download de polígonos dos geo_vetor_biomas Brasileiros\ngeo_vetor_biomas <- geobr::read_biomes(showProgress = FALSE) %>%\n    dplyr::filter(name_biome != \"Sistema Costeiro\") %>% \n    dplyr::rename(nome_bioma = name_biome,\n                  codigo_bioma = code_biome,\n                  ano = year)\n## Importar os dados pelo pacote ecodados\necodados::geo_vetor_biomas\n## Plot\nplot(geo_vetor_biomas)\n## Plot\nplot(geo_vetor_biomas$geom, \n     col = c(\"darkgreen\", \"orange\", \"orange4\", \"forestgreen\",\n             \"yellow\", \"yellow3\"),\n     main = \"Biomas do Brasil\", axes = TRUE, graticule = TRUE)\nlegend(x = -75, y = -20, pch = 15, cex = .7, pt.cex = 2.5,\n       legend = geo_vetor_biomas$nome_bioma, \n       col = c(\"darkgreen\", \"orange\", \"orange4\", \"forestgreen\", \n               \"yellow\", \"yellow3\"))\n## Plot\nplot(geo_raster_srtm_rio_claro)\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\n## Plot\nplot(geo_raster_bioclim[[1:2]], col = viridis::viridis(10))\n## Criar diretório\ndir.create(here::here(\"dados\", \"mapas\"))\n\n## Exportar mapa\npng(filename = here::here(\"dados\", \"mapas\", \"elev_rc.png\"), \n    width = 20, height = 20, units = \"cm\", res = 300)\nplot(geo_raster_srtm_rio_claro)\nplot(geo_vetor_rio_claro$geom, col = NA, border = \"red\", lwd = 2, add = TRUE)\ndev.off()\n## Plot\nmapa_vetor_biomas_ggplot2 <- ggplot(data = geo_vetor_biomas) +\n    aes(fill = nome_bioma) +\n    geom_sf(color = \"black\") +\n    scale_fill_manual(values = c(\"darkgreen\", \"orange\", \"orange4\", \n                                 \"forestgreen\", \"yellow\", \"yellow3\")) +\n    annotation_scale(location = \"br\") +\n    annotation_north_arrow(location = \"br\", which_north = \"true\",\n                           pad_x = unit(0, \"cm\"), pad_y = unit(.5, \"cm\"),\n                           style = north_arrow_fancy_orienteering) +\n    annotate(geom = \"text\", label = \"CRS: SIRGAS2000/Geo\", x = -38, y = -31, size = 2.5) +\n    annotate(geom = \"text\", label = \"Fonte: IBGE (2019)\", x = -39, y = -32.5, size = 2.5) +\n    labs(title = \"Biomas do Brasil\", fill = \"Legenda\", x = \"Longitude\", y = \"Latitude\") +\n    theme_bw() +\n    theme(title = element_text(size = 15, face = \"bold\"),\n          legend.title = element_text(size = 10, face = \"bold\"),\n          legend.position = c(.15, .25),\n          legend.background = element_rect(colour = \"black\"),\n          axis.title = element_text(size = 10, face = \"plain\"))\nmapa_vetor_biomas_ggplot2\n## Dados\ngeo_raster_srtm_rio_claro_dados <- raster::rasterToPoints(geo_raster_srtm_rio_claro) %>% \n    tibble::as_tibble()\nhead(geo_raster_srtm_rio_claro_dados)\n#> # A tibble: 6 × 3\n#>       x     y elevacao\n#>   <dbl> <dbl>    <dbl>\n#> 1 -47.8 -22.2      859\n#> 2 -47.8 -22.2      856\n#> 3 -47.8 -22.2      856\n#> 4 -47.8 -22.2      856\n#> 5 -47.8 -22.2      853\n#> 6 -47.8 -22.2      852\n\n## Mapa\nmapa_srtm_rio_claro_ggplot2 <- ggplot() +\n    geom_raster(data = geo_raster_srtm_rio_claro_dados, aes(x = x, y = y, fill = elevacao)) +\n    geom_sf(data = geo_vetor_rio_claro, color = \"red\", fill = NA, size = 1.3) +\n    scale_fill_viridis_c() +\n    coord_sf() +\n    annotation_scale(location = \"br\",\n                     pad_x = unit(.5, \"cm\"), pad_y = unit(.7, \"cm\"),) +\n    annotation_north_arrow(location = \"br\", which_north = \"true\",\n                           pad_x = unit(.4, \"cm\"), pad_y = unit(1.3, \"cm\"),\n                           style = north_arrow_fancy_orienteering) +\n    annotate(geom = \"text\", label = \"CRS: WGS84/Geo\", \n             x = -47.51, y = -22.53, size = 3) +\n    labs(title = \"Elevação de Rio Claro/SP\", fill = \"Elevação (m)\", \n         x = \"Longitude\", y = \"Latitude\") +\n    theme_bw() +\n    theme(title = element_text(size = 15, face = \"bold\"),\n          legend.title = element_text(size = 10, face = \"bold\"),\n          legend.position = c(.2, .25),\n          legend.background = element_rect(colour = \"black\"),\n          axis.title = element_text(size = 10, face = \"plain\"),\n          axis.text.y = element_text(angle = 90, hjust = .4))\nmapa_srtm_rio_claro_ggplot2\n## Exportar mapa ggplot2\nggsave(filename = here::here(\"dados\", \"mapas\", \"srtm_rio_claro_ggplot2.png\"),\n       plot = mapa_srtm_rio_claro_ggplot2, width = 20, height = 20, units = \"cm\", dpi = 300)\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa\nmapa_vetor_biomas_tmap <- tm_shape(geo_vetor_biomas, bbox = c(-74, -35, -27, 10)) +\n    tm_polygons(col = \"nome_bioma\",\n                pal = c(\"darkgreen\", \"orange\", \"orange4\", \"forestgreen\", \"yellow\", \"yellow3\"),\n                border.col = \"black\",\n                title = \"Legenda\") +\n    tm_compass() +\n    tm_scale_bar(text.size = .6) +\n    tm_graticules(lines = FALSE) +\n    tm_credits(\"CRS: SIRGAS2000/Geo\", position = c(.63, .13)) +\n    tm_credits(\"Fonte: IBGE (2019)\", position = c(.63, .09)) +\n    tm_layout(title = \"Biomas do Brasil\",\n              title.position = c(.25, .95),\n              title.size = 1.8,\n              title.fontface = \"bold\",\n              legend.frame = TRUE,\n              legend.position = c(\"left\", \"bottom\"),\n              legend.title.fontface = \"bold\")\nmapa_vetor_biomas_tmap\n## Dados\ngeo_vetor_am_sul <- rnaturalearth::ne_countries(continent = \"South America\")\ngeo_vetor_brasil <- rnaturalearth::ne_countries(country = \"Brazil\")\ngeo_vetor_biomas <- geobr::read_biomes(showProgress = FALSE) %>%\n    dplyr::filter(name_biome != \"Sistema Costeiro\")\n## Importar os dados pelo pacote ecodados\necodados::geo_vetor_am_sul\necodados::geo_vetor_brasil\necodados::geo_vetor_biomas\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa secundário\nmapa_am_sul <- tm_shape(geo_vetor_am_sul) +\n    tm_polygons() +\n    tm_shape(geo_vetor_brasil) +\n    tm_polygons(col = \"gray50\")\n\n## Juntando os mapas\nmapa_vetor_biomas_tmap\nprint(mapa_am_sul, vp = grid::viewport(.815, .875, wi = .2, he = .2))\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa\nmapa_srtm_rio_claro_tmap <- tm_shape(geo_raster_srtm_rio_claro) +\n    tm_raster(pal = \"viridis\", title = \"Elevação (m)\") +\n    tm_shape(geo_vetor_rio_claro) +\n    tm_borders(col = \"red\", lwd = 2) +\n    tm_compass(position = c(.9, .08)) +\n    tm_scale_bar(text.size = .6, position = c(.67, 0)) +\n    tm_graticules(lines = FALSE) +\n    tm_credits(\"CRS: WGS84/Geo\", position = c(.67, .06)) +\n    tm_layout(title = \"Elevação Rio Claro/SP\",\n              title.size = 1,\n              title.fontface = \"bold\",\n              legend.title.size = .7,\n              legend.text.size = .6,\n              legend.frame = TRUE,\n              legend.position = c(.01, .01),\n              legend.title.fontface = \"bold\")\nmapa_srtm_rio_claro_tmap\n## Exportar mapa tmap\ntmap::tmap_save(tm = mapa_srtm_rio_claro_tmap, \n                filename = here::here(\"dados\", \"mapas\", \"srtm_rio_claro_tmap.png\"),\n                width = 20, height = 20, units = \"cm\", dpi = 300)"},{"path":"cap15.html","id":"mapas-animados","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.10.4 Mapas animados","text":"Podemos montar mapas facetados para mostrar como padrões espaciais variam ao longo tempo, como por exemplo, os limites Brasil ao longo dos anos (Figura 15.75). Entretanto, essa abordagem possui algumas desvantagens, de modo que facetas podem ficar muito pequenas quando há muitas delas.Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.\nFigura 15.75: Mapa vetor facetado dos estados brasileiros ao longo tempo com o pacote tmap.\nUma solução é composição de mapas animados. Apesar de dependerem da publicação digital, os mapas animados podem aprimorar relatórios físicos à medida que o vínculo uma página da web contendo versão animada torna-se simples. Existem várias maneiras de gerar animações em R, por exemplo, com o pacote gganimate. Entretanto, aqui veremos criação de mapas animados com tmap.Podemos criar mapas animados alterando dois argumentos da função tm_facets():trocando o = year por along = yearindicando o free.coords = FALSEPor fim, podemos exportar o mapa animado formato de .gif utilizando função tmap::tmap_animation(), indicando taxa de atualização com o argumento delay (Figura 15.76). Para visualização .gif animado, acessar o livro -line. Alguns pacotes extras são requeridos dependendo sistema operacional utilizado.\nFigura 15.76: Mapa vetorial animado mostrando os estados brasileiros ao longo tempo com o pacote tmap.\n","code":"\n## Dados\ngeo_vetor_brasil_anos <- NULL\nfor(i in c(1872, 1900, 1911, 1920, 1933, 1940, 1950, 1960, 1970,\n           1980, 1991, 2001, 2010, 2019)){\n    geo_vetor_brasil_anos <- geobr::read_state(code_state = \"all\",\n                                               year = i, showProgress = FALSE) %>% \n        sf::st_geometry() %>% \n        sf::st_as_sf() %>%\n        dplyr::mutate(year = i) %>% \n        dplyr::bind_rows(geo_vetor_brasil_anos, .)\n}\n## Importar os dados pelo pacote ecodados\necodados::geo_vetor_brasil_anos\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa facetado\nmapa_brasil_tmap <- tm_shape(geo_vetor_brasil_anos) + \n    tm_polygons() + \n    tm_facets(by = \"year\", nrow = 4)\nmapa_brasil_tmap\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa animado\nmapa_brasil_tmap_ani <- tm_shape(geo_vetor_brasil_anos) + \n    tm_polygons() + \n    tm_facets(along = \"year\", free.coords = FALSE)\n\n## Exportar mapa tmap animado\ntmap::tmap_animation(tm = mapa_brasil_tmap_ani, \n                     filename = here::here(\"dados\", \"mapas\",\n                                           \"srtm_rio_claro_tmap_ani.gif\"), \n                     delay = 30)"},{"path":"cap15.html","id":"mapas-interativos","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.10.5 Mapas interativos","text":"Mapas interativos podem assumir muitas formas, sendo que mais comum e útil é capacidade de deslocar e ampliar qualquer parte de um conjunto de dados geoespaciais sobreposto em um “mapa da web”. Diversos pacotes nos permitem criar esse tipo de mapa, sendo os mais comuns o tmap, mapview e leaflet. Para visualização dos mapas interativos, acessar o livro -line. 📝 Importante \nDestacamos que esses mapas irão ser compostos numa janela especial de “Viewer”.pacote tmapUm recurso exclusivo tmap é sua capacidade de criar mapas estáticos e interativos usando o mesmo código. Os mapas podem ser visualizados interativamente em qualquer ponto mudando para o modo de visualização, usando função tmap::tmap_mode(mode = \"view\") (Figura 15.77).\nFigura 15.77: Mapa vetorial interativo com o pacote tmap.\nPara exportar mapas interativos criados com o pacote tmap, podemos utilizar novamente função tmap::tmap_save(), indicando extensão como .html.Pacote mapviewO pacote mapview cria rapidamente mapas interativos simples com função mapvew::mapview() (Figura 15.78). Entretanto, outras características podem ser mudadas para criar mapas mais elaborados, como pode ser visto através site pacote.\nFigura 15.78: Mapa vetorial interativo com o pacote mapview.\nPara exportar mapas interativos criados com o pacote mapview, podemos utilizar função mapivew::mapshot(), indicando extensão como .html.Pacote leafletO leaflet é um dos pacotes de mapeamento interativo mais utilizados e completos em R. Esse pacote fornece uma interface utilizando biblioteca JavaScript e muitos argumentos podem ser compreendidos lendo documentação da biblioteca original.Mapas interativos usando esse pacote são criados utilizando função leaflet::leaflet(). O resultado dessa função é um objeto da classe leaflet, que pode ser alterado por outras funções deste mesmo pacote, permitindo que várias camadas e configurações de controle sejam adicionadas interativamente (Figura 15.79).Mais sobre o pacote leaflet pode ser consultado em seu site e CheatSheet.\nFigura 15.79: Mapa vetorial interativo com o pacote leaflet.\nPara exportar mapas interativos criados com o pacote leaflet, podemos utilizar novamente função mapivew::mapshot(), indicando extensão como .html.","code":"\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"view\")\n\n## Atribuir novo mapa interativo\nmapa_srtm_rio_claro_tmap_int <- mapa_srtm_rio_claro_tmap\n## Exportar mapa tmap interativo\ntmap::tmap_save(tm = mapa_srtm_rio_claro_tmap_int, \n                filename = here::here(\"dados\", \"mapas\", \"srtm_rio_claro_tmap_int.html\"))\n## Mapa\nmapa_srtm_rio_claro_mapview_int <- mapview::mapview(\n    geo_raster_srtm_rio_claro, col.regions = viridis::viridis(100))\n## Exportar mapa mapview interativo\nmapview::mapshot(x = mapa_srtm_rio_claro_mapview_int, \n                 url = here::here(\"dados\", \"mapas\", \"srtm_rio_claro_mapview_int.html\"))\n## Paleta de cores\npal <- colorNumeric(viridis::viridis(10), raster::values(geo_raster_srtm_rio_claro))\n\n## Mapa\nmapa_srtm_rio_claro_leaflet_int <- leaflet() %>%\n    addProviderTiles(\"CartoDB.Positron\") %>% \n    addRasterImage(geo_raster_srtm_rio_claro, colors = pal, opacity = .8) %>%\n    addLegend(pal = pal, values = raster::values(geo_raster_srtm_rio_claro), \n              title = \"Elevação (m)\") %>% \n    addPolygons(data = geo_vetor_rio_claro, col = \"red\", fill = NA)\nknitr::include_url(\"img/cap15_fig78.png\")\n## Exportar mapa leaflet interativo\nmapview::mapshot(x = mapa_srtm_rio_claro_leaflet_int, \n                 url = here::here(\"dados\", \"mapas\", \"srtm_rio_claro_leaflet_int.html\"))"},{"path":"cap15.html","id":"exemplos-de-aplicações-de-análises-geoespaciais-para-dados-ecológicos","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.11 Exemplos de aplicações de análises geoespaciais para dados ecológicos","text":"Agora que vimos os principais conceitos e aplicações manejo e visualização de dados geoespaciais, podemos avançar para realizar quatro exemplos de aplicações para dados ecológicos. Para isso, usaremos novamente os dados de comunidades de anfíbios da Mata Atlântica (Vancine et al. 2018). Primeiramente, veremos como resumir informações de biodiversidade (número de ocorrências e riqueza) para hexágonos. Num segundo momento, veremos como associar dados ambientais coordenadas de espécies ou comunidades. Depois, como resumir dados de rasters para buffers. Por fim, realizaremos predições espaciais contínuas de adequabilidade de habitat para uma espécie e número de espécies.","code":""},{"path":"cap15.html","id":"resumir-informações-de-biodiversidade-para-unidades-espaciais","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.11.1 Resumir informações de biodiversidade para unidades espaciais","text":"Resumir informações para unidades espaciais é um passo muito frequente em análises de Macroecologia, Biogeografia ou Ecologia da Paisagem. Nesta seção, contabilizaremos o número de ocorrências e riqueza de anfíbios para hexágonos na Mata Atlântica.Primeiramente, vamos importar e preparar os dados de biodiversidade que usaremos nesses exemplos. Vamos começar importando os locais de amostragens de anfíbios na Mata Atlântica e selecionando apenas colunas de interesse.Agora vamos importar espécies das comunidades, selecionando apenas espécies com nomes válidos e transformando coluna de indivíduos para 1, para compor posteriormente uma matriz de comunidade de espécies.Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.Podemos agora juntar tabela de locais, que possui coordenadas à tabela de espécies. Em seguida convertemos essa única tabela na classe vetor sf.Agora vamos baixar o limite Bioma da Mata Atlântica para o Brasil, converter o CRS para WGS84/Geo e ajustar sua extensão para remover ilhas Oceano Atlântico.Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.Podemos verificar se coordenadas e o limite bioma estão todos corretos compondo um mapa preliminar, usando o pacote tmap (Figura 15.80).\nFigura 15.80: Mapa dos locais Atlantic Amphibians e limite da Mata Atlântica.\nComo o limite utilizado para reunir informações das comunidades de anfíbios foi o mais abrangente possível (Muylaert et al. 2018; Vancine et al. 2018), selecionaremos apenas os locais que caem dentro limite da Mata Atlântica que estamos utilizando aqui.Podemos refazer o mapa mostrando coordenadas retiradas em vermelho e que ficaram em verde (Figura 15.81).\nFigura 15.81: Mapa dos locais Atlantic Amphibians que caem dentro limite da Mata Atlântica.\nO próximo passo é criar um gride de hexágonos para o Bioma da Mata Atlântica. Usaremos função sf::st_make_grid() que pode criar quadrículas ou hexágonos. Esses hexágonos terão área equivalente à quadrículas de 1º de tamanho (aproximadamente 10000 km²). Usaremos função sf::st_area() para calcular áreas dos hexágonos e função tibble::rowid_to_column() para criar uma identificação para cada feição.Podemos conferir os hexágonos criados fazendo um mapa preliminar (Figura 15.82).\nFigura 15.82: Mapa dos hexágonos para o limite da Mata Atlântica.\nPodemos ser mais restritos e selecionar apenas os hexágonos dentro limite Bioma da Mata Atlântica utilizando o operador st_within() (Figura 15.83).\nFigura 15.83: Mapa dos hexágonos totalmente dentro limite da Mata Atlântica.\nPodemos agora associar espécies aos hexágonos fazendo um “join” espacial, utilizando função sf::st_join().Por fim, podemos agregar os dados para ter o número de ocorrências e de espécies por hexágono.Finalmente podemos compor os mapas finais, mostrando os hexágonos com cores e valores número de ocorrências e de espécies (Figura 15.84).\nFigura 15.84: Mapa com o número de ocorrências e riqueza de anfíbios para hexágonos limite da Mata Atlântica.\n","code":"\n## Importar locais\ngeo_anfibios_locais <- readr::read_csv(\n    here::here(\"dados\", \"tabelas\", \"ATLANTIC_AMPHIBIANS_sites.csv\"),\n    col_types = cols()) %>%\n    dplyr::select(id, longitude, latitude, species_number)\n## Importar espécies\ngeo_anfibios_especies <- readr::read_csv(\n    here::here(\"dados\", \"tabelas\", \"ATLANTIC_AMPHIBIANS_species.csv\"), \n    col_types = cols()) %>%\n    tidyr::drop_na(valid_name) %>% \n    dplyr::select(id, valid_name, individuals) %>% \n    dplyr::distinct(id, valid_name, .keep_all = TRUE) %>% \n    dplyr::mutate(individuals = tidyr::replace_na(individuals, 1),\n                  individuals = ifelse(individuals > 0, 1, 1))\n## Importar os dados pelo pacote ecodados\necodados::geo_anfibios_locais\necodados::geo_anfibios_especies\n## Junção das coordenadas e conversão para classe sf\ngeo_anfibios_especies_locais_vetor <- geo_anfibios_especies %>% \n    dplyr::left_join(geo_anfibios_locais, by = \"id\") %>% \n    dplyr::relocate(longitude, latitude, .after = 1) %>% \n    dplyr::mutate(lon = longitude, lat = latitude) %>% \n    sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n## Download do Bioma da Mata Atlântica\ngeo_vetor_mata_atlantica <- geobr::read_biomes(year = 2019, showProgress = FALSE) %>% \n    dplyr::filter(name_biome == \"Mata Atlântica\") %>% \n    sf::st_transform(crs = 4326) %>% \n    sf::st_crop(xmin = -55, ymin = -30, xmax = -34, ymax = -5)\n## Importar os dados pelo pacote ecodados\necodados::geo_vetor_mata_atlantica\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa\ntm_shape(geo_vetor_mata_atlantica, \n         bbox = geo_anfibios_especies_locais_vetor) +\n    tm_polygons() +\n    tm_shape(geo_anfibios_especies_locais_vetor) +\n    tm_dots(size = .1, col = \"forestgreen\")\n## Selecionar os locais dentro do limite\ngeo_anfibios_especies_locais_vetor_mata_atlantica <- geo_anfibios_especies_locais_vetor[geo_vetor_mata_atlantica, ]\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa\ntm_shape(geo_vetor_mata_atlantica,\n         bbox = geo_anfibios_especies_locais_vetor) +\n    tm_polygons() +\n    tm_shape(geo_anfibios_especies_locais_vetor) +\n    tm_bubbles(size = .1, col = \"red\") +\n    tm_shape(geo_anfibios_especies_locais_vetor_mata_atlantica) +\n    tm_bubbles(size = .1, col = \"forestgreen\")\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Criar hexágonos de ~110 km\ngeo_vetor_mata_atlantica_hex <- sf::st_make_grid(\n    x = geo_vetor_mata_atlantica, cellsize = 1, square = FALSE) %>% \n    sf::st_as_sf() %>% \n    dplyr::mutate(areakm2 = sf::st_area(.)/1e6) %>% \n    tibble::rowid_to_column(\"id_hex\")\n\n## Selecionar os hexágonos dentro do limite da Mata Atlântica\ngeo_vetor_mata_atlantica_hex <- geo_vetor_mata_atlantica_hex[geo_vetor_mata_atlantica, ]\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa\ntm_shape(geo_vetor_mata_atlantica, \n         bbox = geo_vetor_mata_atlantica_hex) +\n    tm_polygons() +\n    tm_shape(geo_vetor_mata_atlantica_hex) +\n    tm_borders()\n## Selecionar os hexágonos totalmente dentro do limite da Mata Atlântica\ngeo_vetor_mata_atlantica_hex_total <- geo_vetor_mata_atlantica_hex[geo_vetor_mata_atlantica, , op = st_within]\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa\ntm_shape(geo_vetor_mata_atlantica, \n         bbox = geo_vetor_mata_atlantica_hex) +\n    tm_polygons() +\n    tm_shape(geo_vetor_mata_atlantica_hex_total) +\n    tm_borders()\n## Junção espacial dos locais com os hexágonos\ngeo_vetor_mata_atlantica_hex_especies <- sf::st_join(\n    x = geo_vetor_mata_atlantica_hex, \n    y = geo_anfibios_especies_locais_vetor_mata_atlantica,\n    left = TRUE)\n## Agregar dados de ocorrências e número de espécies por hexágono\ngeo_vetor_mata_atlantica_hex_especies_oco_riq <- geo_vetor_mata_atlantica_hex_especies %>% \n    dplyr::group_by(id_hex) %>% \n    dplyr::summarise(ocorrencias = length(valid_name[!is.na(valid_name)]),\n                     riqueza = n_distinct(valid_name, na.rm = TRUE))\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa de ocorrências\nmapa_oco <- tm_shape(geo_vetor_mata_atlantica_hex_especies_oco_riq) +\n    tm_polygons(title = \"Ocorrência de anfíbios\", col = \"ocorrencias\", \n                pal = \"viridis\", style = \"pretty\") +\n    tm_text(\"ocorrencias\", size = .4) +\n    tm_graticules(lines = FALSE) +\n    tm_compass() +\n    tm_scale_bar() +\n    tm_layout(legend.title.size = 2,\n              legend.title.fontface = \"bold\",\n              legend.position = c(\"left\", \"top\"))\n\n## Mapa de riqueza\nmapa_riq <- tm_shape(geo_vetor_mata_atlantica_hex_especies_oco_riq) +\n    tm_polygons(title = \"Riqueza de anfíbios\", col = \"riqueza\", \n                pal = \"viridis\", style = \"pretty\") +\n    tm_text(\"riqueza\", size = .4) +\n    tm_graticules(lines = FALSE) +\n    tm_compass() +\n    tm_scale_bar() +\n    tm_layout(legend.title.size = 2,\n              legend.title.fontface = \"bold\",\n              legend.position = c(\"left\", \"top\"))\n\n## União dos mapas\ntmap_arrange(mapa_oco, mapa_riq)"},{"path":"cap15.html","id":"extrair-dados-raster-para-pontos","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.11.2 Extrair dados raster para pontos","text":"Atribuir informações ambientais ocorrências é um passo fundamental para diversas análises. Nesta seção, atribuiremos os valores das variáveis bioclimáticas aos locais de amostragem de anfíbios na Mata Atlântica.Já realizamos o download das variáveis bioclimáticas na seção de raster. Vamos importar novamente esses dados, primeiramente listando camadas e depois importando com função raster:stack().Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.Da seção anterior, já temos o objeto com tabela de coordenadas dos locais de amostragem das comunidades de anfíbios. Vamos agora criar um objeto vetorial das coordenadas e em seguida selecionar os locais dentro limite bioma da Mata Atlântica.Usaremos agora função raster::extract() para extrair e associar os valores das variáveis bioclimáticas para os locais de amostragem.Podemos ver esses dados na Tabela 15.14.\nTabela 15.14: Dados extraídos e atribuídos aos locais\nde amostragens de comunidades de anfíbios na Mata Atlântica\nPodemos ainda fazer alguns mapas para espacializar essas variáveis (Figura 15.85).\nFigura 15.85: Mapa com os valores das variáveis bioclimáticas (BIO01:BIO06) para os locais amostrados de comunidades de anfíbios para o limite da Mata Atlântica.\n","code":"\n## Listar arquivos\narquivos_raster <- dir(path = here::here(\"dados\", \"raster\"), pattern = \"wc\") %>% \n    grep(\".tif\", ., value = TRUE)\n\n## Importar rasters\ngeo_raster_bioclim <- raster::stack(here::here(\"dados\", \"raster\", arquivos_raster))\n\n## Renomear rasters\nnames(geo_raster_bioclim) <- c(\"bio01\", paste0(\"bio\", 10:19), paste0(\"bio0\", 2:9))\n## Importar os dados pelo pacote ecodados\necodados::geo_raster_bioclim\n## Importar locais e converter em sf\ngeo_anfibios_locais_vetor <- geo_anfibios_locais %>% \n    dplyr::mutate(lon = longitude, lat = latitude) %>% \n    sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\ngeo_anfibios_locais_vetor\n#> Simple feature collection with 1163 features and 4 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: -56.74194 ymin: -33.51083 xmax: -34.79667 ymax: -3.51525\n#> Geodetic CRS:  WGS 84\n#> # A tibble: 1,163 × 5\n#>    id      longitude latitude species_number              geometry\n#>  * <chr>       <dbl>    <dbl>          <dbl>           <POINT [°]>\n#>  1 amp1001     -43.4    -8.68             19     (-43.42194 -8.68)\n#>  2 amp1002     -38.9    -3.55             16 (-38.85783 -3.545527)\n#>  3 amp1003     -38.9    -3.57             14 (-38.88869 -3.574194)\n#>  4 amp1004     -38.9    -3.52             13   (-38.9188 -3.51525)\n#>  5 amp1005     -38.9    -4.28             30 (-38.91083 -4.280556)\n#>  6 amp1006     -36.4    -9.23             42 (-36.42806 -9.229167)\n#>  7 amp1007     -40.9    -3.85             23 (-40.89444 -3.846111)\n#>  8 amp1008     -40.9    -3.83             19 (-40.91944 -3.825833)\n#>  9 amp1009     -40.9    -3.84             13   (-40.91028 -3.8375)\n#> 10 amp1010     -35.2    -6.14              1 (-35.22944 -6.136944)\n#> # … with 1,153 more rows\n## Extrair valores das variáveis para os locais\ngeo_anfibios_locais_vetor_bioclim <- geo_anfibios_locais_vetor %>% \n    dplyr::mutate(raster::extract(geo_raster_bioclim, ., df = TRUE)) %>% \n    dplyr::select(-ID) %>% \n    dplyr::relocate(bio02:bio09, .after = bio01)\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa\ngeo_anfibios_locais_vetor_bioclim %>% \n    dplyr::select(bio01:bio06) %>% \n    tidyr::gather(var, val, -geometry) %>% \n    tm_shape() +\n    tm_bubbles(size = .1, col = \"val\", pal = \"viridis\") +\n    tm_facets(\"var\", free.scales = TRUE) +\n    tm_layout(legend.outside = FALSE)"},{"path":"cap15.html","id":"resumir-dados-de-rasters-para-buffers","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.11.3 Resumir dados de rasters para buffers","text":"Muitas análises requerem que façamos um resumo da composição da paisagem para buffers, sendo o buffer uma unidade de análise espacial entorno de um ponto de amostragem.Aqui, usaremos os dados GlobCover v.2.3 de 2009 (Arino et al. 2012) como raster de cobertura da terra. O arquivo é grande (~400 Mb) e pode demorar muito, dependendo da velocidade da sua internet.Depois de fazer o download, vamos importar e ajustar esse raster para o limite da Mata Atlântica (Figura 15.86).Caso o download não funcione ou haja problemas com importação, disponibilizamos os dados também pacote ecodados.\nFigura 15.86: Camada raster GlobCover 2.3 para o Bioma da Mata Atlântica.\nVamos agora transformar tabela de locais em vetor, selecionar aleatoriamente 50 amostragens das comunidades de anfíbios e criar buffers de ~10 km.Podemos conferir mapa da Figura 15.87, atente para o aumento artificial tamanho dos buffers para permitir visualização.\nFigura 15.87: Distribuição de 50 localidades aleatórios e buffers de ~1 km (fora de escala).\nAgora podemos utilizar função raster::extract() para fazer contabilização, já em porcentagem, de pixels de cada classe para cada buffer.Agora podemos combinar esses dados aos dados dos buffers.","code":"\n## Aumentar o tempo de download\noptions(timeout = 1e5)\n\n## Download dos dados do GlobCover\ndownload.file(url = \"http://due.esrin.esa.int/files/Globcover2009_V2.3_Global_.zip\",\n              destfile = here::here(\"dados\", \"raster\", \"Globcover2009_V2.3_Global.zip\"), \n              mode = \"wb\", extra = \"c\")\n\n## Unzip\nunzip(zipfile = here::here(\"dados\", \"raster\", \"Globcover2009_V2.3_Global.zip\"),\n      exdir = here::here(\"dados\", \"raster\"))\n## Importar raster do GlobCover\ngeo_raster_globcover <- raster::raster(\n    here::here(\"dados\", \"raster\", \"GLOBCOVER_L4_200901_200912_V2.3.tif\"))\n\n## Ajustar para o limite do bioma da Mata Atlântica\ngeo_raster_globcover_mata_atlantica <- geo_raster_globcover %>% \n    raster::crop(geo_vetor_mata_atlantica) %>% \n    raster::mask(geo_vetor_mata_atlantica)\ngeo_raster_globcover_mata_atlantica\n#> class      : RasterLayer \n#> dimensions : 8940, 7274, 65029560  (nrow, ncol, ncell)\n#> resolution : 0.002777778, 0.002777778  (x, y)\n#> extent     : -54.99861, -34.79306, -29.98194, -5.148611  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : memory\n#> names      : GLOBCOVER_L4_200901_200912_V2.3 \n#> values     : 14, 220  (min, max)\n## Importar os dados pelo pacote ecodados\necodados::geo_raster_globcover_mata_atlantica\n## Plot\nplot(geo_raster_globcover_mata_atlantica, col = viridis::viridis(n = 200))\n## Fixar a amostragem\nset.seed(42)\n\n## Pontos\ngeo_anfibios_especies_locais_vetor_mata_atlantica <- geo_anfibios_locais %>% \n    sf::st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %>%\n    dplyr::filter(sf::st_intersects(x = ., y = geo_vetor_mata_atlantica, sparse = FALSE)) %>% \n    dplyr::slice_sample(n = 50)\n\n## Buffers de ~10 km\ngeo_anfibios_especies_locais_vetor_mata_atlantica_buffer10km <-\nsf::st_buffer(geo_anfibios_especies_locais_vetor_mata_atlantica,\ndist = 10000)\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa\ntm_shape(geo_vetor_mata_atlantica) +\n    tm_polygons() +\n    tm_shape(geo_anfibios_especies_locais_vetor_mata_atlantica_buffer10km) +\n    tm_bubbles(size = .3, border.col = \"red\", alpha = 0) +\n    tm_shape(geo_anfibios_especies_locais_vetor_mata_atlantica) +\n    tm_dots(size = .01, col = \"forestgreen\")\n## Estatística zonal \ngeo_anfibios_locais_vetor_ma_buffer10km_ext <- raster::extract(\n    x = geo_raster_globcover_mata_atlantica,\n    y = geo_anfibios_especies_locais_vetor_mata_atlantica_buffer10km,\n    df = TRUE, na.rm = TRUE) %>% \n    dplyr::rename(id = ID, classe = 2) %>% \n    tidyr::drop_na(classe) %>% \n    dplyr::mutate(classe = paste0(\"classe_\", classe)) %>%\n    dplyr::group_by(id, classe) %>% \n    dplyr::summarise(n = n()) %>% \n    tidyr::pivot_wider(id_cols = id, names_from = classe, values_from = n) %>% \n    dplyr::mutate(across(everything(), ~replace_na(.x, 0))) %>% \n    janitor::adorn_totals(\"col\") %>% \n    janitor::adorn_percentages(\"row\") %>% \n    janitor::adorn_pct_formatting(rounding = \"half up\", digits = 1)\nhead(geo_anfibios_locais_vetor_ma_buffer10km_ext)\n#>  id classe_100 classe_110 classe_120 classe_130 classe_14 classe_150 classe_170 classe_180 classe_20 classe_210 classe_30 classe_40 classe_50\n#>   1       0.1%       0.2%       0.7%      12.0%      0.5%       0.1%       8.7%       0.6%      0.6%      24.8%      5.0%     43.7%      3.1%\n#>   2       0.0%       1.7%       0.3%       8.4%     26.6%       0.5%       0.0%       0.0%     14.7%       1.5%     19.2%     17.9%      8.9%\n#>   3       0.0%       0.0%       0.0%       4.9%     20.2%       0.0%       0.0%       0.0%     17.6%       0.2%     28.6%     27.2%      0.6%\n#>   4       0.0%       2.3%       0.0%      26.0%      3.6%       0.0%       0.0%       0.0%     23.0%       0.2%     41.2%      3.6%      0.0%\n#>   5       0.0%       0.0%       0.0%      13.5%      0.3%       0.0%       0.0%       0.1%     14.2%       0.0%      5.2%     53.6%     12.9%\n#>   6       0.0%       0.4%       0.3%       7.2%      7.0%       0.0%       0.0%       0.0%     21.2%       0.0%     21.1%     35.8%      1.4%\n#>  classe_190 classe_60 classe_200 classe_220 classe_140  Total\n#>        0.0%      0.0%       0.0%       0.0%       0.0% 100.0%\n#>        0.4%      0.0%       0.0%       0.0%       0.0% 100.0%\n#>        0.0%      0.6%       0.0%       0.0%       0.0% 100.0%\n#>        0.0%      0.0%       0.0%       0.0%       0.0% 100.0%\n#>        0.0%      0.1%       0.0%       0.0%       0.0% 100.0%\n#>        5.2%      0.2%       0.1%       0.0%       0.0% 100.0%\n## Combinação\ngeo_anfibios_locais_vetor_ma_buffer10km_ext_bind  <- dplyr::bind_cols(\n    x = geo_anfibios_especies_locais_vetor_mata_atlantica_buffer10km,\n    y = geo_anfibios_locais_vetor_ma_buffer10km_ext[, -1]) %>% \n    sf::st_drop_geometry()\ngeo_anfibios_locais_vetor_ma_buffer10km_ext_bind\n#> # A tibble: 50 × 21\n#>    id      species_number classe_100 classe_110 classe_120 classe_130 classe_14 classe_150 classe_170 classe_180 classe_20 classe_210 classe_30\n#>  * <chr>            <dbl> <chr>      <chr>      <chr>      <chr>      <chr>     <chr>      <chr>      <chr>      <chr>     <chr>      <chr>    \n#>  1 amp1716             10 0.1%       0.2%       0.7%       12.0%      0.5%      0.1%       8.7%       0.6%       0.6%      24.8%      5.0%     \n#>  2 amp1351             16 0.0%       1.7%       0.3%       8.4%       26.6%     0.5%       0.0%       0.0%       14.7%     1.5%       19.2%    \n#>  3 amp1168             19 0.0%       0.0%       0.0%       4.9%       20.2%     0.0%       0.0%       0.0%       17.6%     0.2%       28.6%    \n#>  4 amp1085             20 0.0%       2.3%       0.0%       26.0%      3.6%      0.0%       0.0%       0.0%       23.0%     0.2%       41.2%    \n#>  5 amp1258              3 0.0%       0.0%       0.0%       13.5%      0.3%      0.0%       0.0%       0.1%       14.2%     0.0%       5.2%     \n#>  6 amp1160              9 0.0%       0.4%       0.3%       7.2%       7.0%      0.0%       0.0%       0.0%       21.2%     0.0%       21.1%    \n#>  7 amp1843             14 0.0%       0.0%       0.1%       3.9%       4.3%      0.0%       0.0%       0.0%       22.6%     0.6%       6.2%     \n#>  8 amp1060              8 0.0%       0.8%       0.0%       8.4%       11.5%     0.0%       0.0%       0.0%       53.3%     0.5%       22.7%    \n#>  9 amp1141              5 0.0%       0.2%       0.1%       2.6%       24.5%     0.0%       0.0%       0.1%       42.9%     0.0%       21.9%    \n#> 10 amp1333              7 0.0%       1.2%       0.2%       9.9%       8.5%      0.0%       0.0%       0.2%       21.9%     1.6%       21.0%    \n#> # … with 40 more rows, and 8 more variables: classe_40 <chr>, classe_50 <chr>, classe_190 <chr>, classe_60 <chr>, classe_200 <chr>,\n#> #   classe_220 <chr>, classe_140 <chr>, Total <chr>"},{"path":"cap15.html","id":"predições-espaciais-de-objetos-raster","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.11.4 Predições espaciais de objetos raster","text":"O pacote raster além de permitir realizar manipulação e visualização de dados raster R, também permite extrapolação ajuste de análises, como LMs, GLMs, GAMs dentre outras (Capítulos 7 e 8). Aqui, faremos uma pequena demonstração utilizando função raster::predict(), predizendo o resultado de dois ajustes de GLMs para presença/ausência de uma espécie de anuro e extrapolação número de espécies de anfíbios para o Bioma da Mata Atlântica.Para ajustar um GLM para dados de presença/ausência, podemos usar o conjunto de dados já criado anteriormente, com espécies e coordenadas, e fazer uma junção com última tabela que criamos com os dados bioclimáticos.Agora, vamos selecionar ocorrências da espécie Haddadus binotatus, atribuindo 1 quando ela ocorre e 0 quando ela não ocorre. Essa espécie é relativamente comum na serrapilheira de fragmentos florestais da Mata Atlântica, e recebe esse nome em homenagem um grande pesquisador de anfíbios da Mata Atlântica, o Prof. Célio Fernando Baptista Haddad.Vamos utilizar apenas variáveis não correlacionadas para o índice de correlação de Pearson para r < 0,7 (Capítulo 7).Agora sim, podemos ajustar um modelo simples da presença e ausência dessa espécie, utilizando variáveis não correlacionadas, através de um GLM para família binomial. Nosso intuito não é analisar se o modelo atende à todos os pressupostos, e sim exemplificar predição espacial, para esses detalhes, consulte o Capítulo 8.Antes de fazermos predição da distribuição potencial da espécie é fundamental que o objeto raster esteja ajustado para o limite da Mata Atlântica. Para isso vamos utilizar funções raster::crop() e raster::mask() para fazer esse ajuste (Figura 15.88).\nFigura 15.88: Mapa de dois rasters (BIO01 e BIO12) ajustados ao limite da Mata Atlântica.\nAgora podemos fazer predição desse modelo para todo o Bioma da Mata Atlântica. Essa função vai utilizar os coeficientes modelo ajustado para gerar um raster de predição para todos os pixels da Mata Atlântica. Vamos usar o argumento type = \"response\" para que os valores da predição sejam ajustados 0 1. 📝 Importante \nPara que predição funcione, os nomes das camadas raster geo_raster_bioclim_mata_atlantica devem possuir o mesmo nome das colunas das variáveis preditoras ajustadas modelo modelo_pa.Por fim, último passo podemos tornar esse modelo binário, ou seja, apenas com valores 0 ou 1. Para isso vamos adotar arbitrariamente o valor de 0,01 como ponto de corte. partir desse valor consideraremos os pixels acima como 1 e abaixo como 0.Por fim, vamos produzir dois mapas mostrando os valores das predições e o mapa binário da espécie (Figura 15.89).\nFigura 15.89: Predição contínua e binária modelo ajustado para presença/ausência da espécie Haddadus binotatus na Mata Atlântica.\n 📝 Importante \nEssa análise foi realizada com o intuito de exemplificar o funcionamento da função raster::predict(), para mais detalhes, consultar livros específicos da área de Modelagem de Distribuição de Espécies ou Modelagem de Nicho Ecológico Fletcher Fortin (2018).Em nossa segunda análise, vamos predizer os dados de riqueza de anfíbios (.e. número de espécies) para todo o bioma da Mata Atlântica. Para isso, temos de retirar novamente variáveis correlacionadas.Agora sim, podemos criar os GLMs com famílias de distribuição apropriadas para dados de contagem como Poisson e Binomial Negativa. Novamente, nosso intuito não é analisar se o modelo atende todos os pressupostos, e sim exemplificar predição espacial; para esses detalhes, consulte o Capítulo 8.Com os modelos ajustados, podemos fazer predições utilizando os objetos raster com variáveis ambientais.Por fim, podemos compor os dois mapas de predições (Figura 15.90).\nFigura 15.90: Mapa da predição de riqueza utilizando o modelo Poisson e Binomial Negativa para Mata Atlântica.\n 📝 Importante \nEssa análise foi realizada com o intuito de exemplificar o funcionamento da função raster::predict(), para mais detalhes, consultar livros específicos da área de Ecologia Espacial (Fletcher Fortin 2018).","code":"\n## Junção dos dados ambientais aos dados de espécies\ngeo_anfibios_locais_especies_vetor_bioclim <- geo_anfibios_especies %>% \n    dplyr::left_join(., sf::st_drop_geometry(geo_anfibios_locais_vetor_bioclim), by = \"id\")\n## Seleção da espécie Haddadus binotatus\ngeo_anfibios_locais_especies_vetor_bioclim_hb <- geo_anfibios_locais_especies_vetor_bioclim %>% \n    dplyr::mutate(pa = ifelse(valid_name == \"Haddadus binotatus\", 1, 0), .after = individuals) %>% \n    dplyr::distinct(id, .keep_all = TRUE)\n## Correlação entre as variáveis\ncorr <- geo_anfibios_locais_especies_vetor_bioclim_hb %>% \n    dplyr::select(bio01:bio19) %>% \n    cor() %>% \n    caret::findCorrelation(.7, names = TRUE)\n\n## Seleção das variáveis não correlacionadas\ngeo_anfibios_locais_especies_vetor_bioclim_hb_cor <- geo_anfibios_locais_especies_vetor_bioclim_hb %>% \n    dplyr::select(pa, bio01:bio19) %>% \n    dplyr::select(-c(corr))\n## Ajustar um modelo GLM binomial\nmodelo_pa <- glm(formula = pa ~ ., \n                 data = geo_anfibios_locais_especies_vetor_bioclim_hb_cor, \n                 family = binomial(\"logit\"))\n## Ajuste da extensão e limite\ngeo_raster_bioclim_mata_atlantica <- geo_raster_bioclim %>% \n    raster::crop(geo_vetor_mata_atlantica) %>% \n    raster::mask(geo_vetor_mata_atlantica)\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Mapa\ntm_shape(geo_raster_bioclim_mata_atlantica[[c(1, 4)]]) +\n    tm_raster(pal = \"viridis\", title = c(\"bio01\", \"bio12\")) +\n    tm_facets(free.scales.raster = TRUE)\n## Predições\nmodelo_pa_pred <- raster::predict(\n    object = geo_raster_bioclim_mata_atlantica, \n    model = modelo_pa,\n    type = \"response\")\nmodelo_pa_pred\n#> class      : RasterLayer \n#> dimensions : 149, 121, 18029  (nrow, ncol, ncell)\n#> resolution : 0.1666667, 0.1666667  (x, y)\n#> extent     : -55, -34.83333, -30, -5.166667  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : memory\n#> names      : layer \n#> values     : 1.255833e-12, 0.1983049  (min, max)\n## Seleção dos pixels de presença/ausência potencial\nmodelo_pa_pred_corte <- modelo_pa_pred >= .01\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Combinar os dois raster\nmodelo_pa_pred <- raster::stack(modelo_pa_pred, modelo_pa_pred_corte)\nnames(modelo_pa_pred) <- c(\"Contínuo\", \"Binário\")\n\n## Mapas de predição contínua e binária\ntm_shape(modelo_pa_pred) +\n    tm_raster(pal = \"viridis\", title = c(\"Predição\", \"Predição\"),\n              style = \"fisher\") +\n    tm_facets(free.scales.raster = TRUE)\n## Correlação\ncorr <- geo_anfibios_locais_vetor_bioclim %>% \n    sf::st_drop_geometry() %>% \n    dplyr::select(bio01:bio19) %>% \n    cor() %>% \n    caret::findCorrelation(.7, names = TRUE)\n\n## Seleção das variáveis não correlacionadas\ngeo_anfibios_locais_bioclim_cor <- geo_anfibios_locais_vetor_bioclim %>% \n    sf::st_drop_geometry() %>% \n    dplyr::select(species_number, bio01:bio19) %>% \n    dplyr::select(-c(corr))\n## Modelo Poisson\nmodelo_riq_pois <- glm(\n    formula = species_number ~ ., \n    data = geo_anfibios_locais_bioclim_cor, \n    family = poisson)\n\n## Modelo Binomial Negativo\nmodelo_riq_nb <- MASS::glm.nb(\n    formula = species_number ~ .,\n    data = geo_anfibios_locais_bioclim_cor)\n## Predição do modelo Poisson\nmodelo_riq_pois_pred <- raster::predict(\n    object = geo_raster_bioclim_mata_atlantica,\n    model = modelo_riq_pois,\n    type = \"response\")\nmodelo_riq_pois_pred\n#> class      : RasterLayer \n#> dimensions : 149, 121, 18029  (nrow, ncol, ncell)\n#> resolution : 0.1666667, 0.1666667  (x, y)\n#> extent     : -55, -34.83333, -30, -5.166667  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : memory\n#> names      : layer \n#> values     : 8.249131, 24.76082  (min, max)\n\n## Predição do modelo Binomial Negativo\nmodelo_riq_nb_pred <- raster::predict(\n    object = geo_raster_bioclim_mata_atlantica,\n    model = modelo_riq_nb,\n    type = \"response\")\nmodelo_riq_nb_pred\n#> class      : RasterLayer \n#> dimensions : 149, 121, 18029  (nrow, ncol, ncell)\n#> resolution : 0.1666667, 0.1666667  (x, y)\n#> extent     : -55, -34.83333, -30, -5.166667  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs \n#> source     : memory\n#> names      : layer \n#> values     : 9.262095, 24.45297  (min, max)\n## Mudar o modo de exibição do tmap\ntmap::tmap_mode(mode = \"plot\")\n\n## Combinar os dois raster\nmodelo_riq_pred <- raster::stack(modelo_riq_pois_pred, modelo_riq_nb_pred)\nnames(modelo_riq_pred) <- c(\"Poisson\", \"Binomial Negativa\")\n\n## Mapas da predição Poisson e Binomial Negativa\ntm_shape(modelo_riq_pred) +\n    tm_raster(pal = \"viridis\", title = c(\"Número de espécies\", \"Número de espécies\"),\n              style = \"fisher\") +\n    tm_facets(free.scales.raster = TRUE)"},{"path":"cap15.html","id":"para-se-aprofundar-11","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.12 Para se aprofundar","text":"","code":""},{"path":"cap15.html","id":"livros-10","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.12.1 Livros","text":"Listamos aqui principais referências sobre manipulação, visualização de dados geoespaciais e análises geoespaciais R. Recomendamos aos (às) interessados() os livros: ) Lovelace, Nowosad & Muenchow (2019) Geocomputation R, ii) Mas et al. (2019) Análise espacial com R, iii) Olaya (2020) Sistemas de Información Geográfica, iv) Moraga (2019) Geospatial Health Data: Modeling Visualization R-INLA Shiny, v) Brunsdon & Comber (2015) Introduction Spatial Analysis Mapping R, vi) Wegmann, Leutner & Dech (2016) Remote Sensing GIS Ecologists: Using Open Source Software, vii) Wegmann, Schwalb-Willmann & Dech (2020) Introduction Spatial Data Analysis Remote Sensing GIS Open Source Software, viii) Fletcher & Fortin (2018) Spatial ecology conservation modeling: Applications R, ix) Lapaine, Miljenko, Usery, E. Lynn (2017) Choosing Map Projection.","code":""},{"path":"cap15.html","id":"links-10","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.12.2 Links","text":"Awesome GeospatialSpatial Data ScienceIntro GIS Spatial AnalysisIntroduction Spatial Data Programming RR Geographic Data ScienceSpatial Data Science RSpatial Modelling Data ScientistsPredictive Soil Mapping R","code":""},{"path":"cap15.html","id":"exercícios-11","chapter":"Capítulo 15 Dados geoespaciais","heading":"15.13 Exercícios","text":"15.1\nImporte o limite dos estados brasileiros formato sf com o nome br. Para isso, use função ne_states pacote rnaturalearth. Crie um mapa simples cinza utilizando função plot(), selecionando coluna geometry com o operador $ e com os argumentos axes e graticule verdadeiros.15.2\nDados vetoriais podem ser criados com diversos erros de topologia, e.g., sobreposição de linhas ou polígonos ou buracos. Algumas funções exigem que os objetos vetoriais aos quais são atribuídos esses dados não possuam esses erros para que o algoritmo funcione. Para verificar se há erros, podemos usar função st_is_valid() pacote sf. Há diversas forma de correções desses erros, mas vamos usar uma correção simples R, com função st_make_valid(). Vamos fazer essa correção para o br importado anteriormente e atribuindo ao objeto br_valid. Podemos conferir para saber se há erros e fazer um plot.15.3\nCrie um objeto RasterLayer vazio chamado ra com resolução de 5º (~600 km). Atribua um sistema de referência de coordenadas com o código 4326. Atribua valores aleatórios de uma distribuição normal e plote o mesmo.15.4\nReprojete o limite dos estados brasileiros exercício anterior para o CRS SIRGAS 2000/Brazil Polyconic, utilizando o código EPSG:5880 e chamando de br_poly. Faça um mapa simples como exercício 1. Atente para curvaturas das linhas.15.5\nUtilizando função st_centroid pacote sf, crie um vetor chamado br_valid_cen que armazenará o centroide de cada estado brasileiro objeto br_valid exercício 2 e plot o resultado.15.6\nAjuste o limite e máscara objeto raster criado exercício 3 para o limite Brasil, atribuindo ao objeto ra_br. Depois reprojete esse raster para mesma projeção utilizada exercício 4 com o nome ra_br_poly e plote o mapa resultante.15.7\nExtraia os valores de cada pixel raster criado exercício 6 para os centroides dos estados Brasil criado exercício 5, atribuindo à coluna val objeto espacial chamado br_valid_poly_cent_ra.15.8\nCrie um mapa final usando os resultados dos exercícios 4, 5 e 6. Utilize o pacote tmap e inclua todos os principais elementos de um mapa.Soluções dos exercícios.","code":""},{"path":"glossário.html","id":"glossário","chapter":"Glossário","heading":"Glossário","text":"Neste glossário, resumimos os principais conceitos e definições dos termos utilizados texto principal que possam ser desconhecidos ou pouco claros para alguns leitores().Abundance-based Coverage Estimator (ACE): estimador de riqueza de espécies baseado na abundância de espécies rarasAgregação (raster): aumentar o tamanho dos pixels (diminuindo resolução) de um raster, agregando os valores dos pixels em um pixel maiorAgrupamento filogenético: espécies coexistindo nas comunidades são mais aparentadas que esperado pelo acasoAlinhamento (raster): ajusta o tamanho pixel, extensão, número e origem dos pixels para várias camadas rastersAmbiente ou Environment (RStudio): porção onde os objetos criados são armazenadosAmostra: subconjunto da população, selecionados por um processo adequado de amostragem, utilizado para estimar características de toda populaçãoAnálise de covariância (ANCOVA): é uma extensão da ANOVA com adição de uma covariável medida em todas unidades amostraisAnálise de variância (ANOVA): é um teste estatístico que avalia se há diferenças entre médias de três ou mais grupos independentes. Existem uma variedade de delineamentos experimentais como - ANOVA de um fator, ANOVA de dois fatores, ANOVA em blocos aleatorizados, ANOVA de medidas repetidas e ANOVA split-splot - que diferem na maneira que o teste estatístico e os graus de liberdade são calculadosAnálise paramétrica: assume que os dados foram amostrados de uma distribuição de forma conhecida (e.g., gaussiana, poisson, etc) e estima os parâmetros (e.g., média, desvio padrão, etc) da distribuição partir dos dadosAnálise não paramétrica: não pressupõe que os dados foram amostrados de uma distribuição de forma conhecida (e.g., gaussiana, poisson, etc)Array: classe de objetos que representa elementos de um único modo formato de combinação de tabelas, com linhas, colunas e dimensõesÁrvore filogenética: são hipóteses que representam relação de parentesco entre espécies (pode ser também indivíduos, genes, etc.) com informações sobre quais espécies compartilham um ancestral comum e distância (tempo, genética, ou diferenças nos caracteres) que separamAtributo funcional: uma propriedade mensurável dos organismos (geralmente em nível individual) que representa características morfológicas, fisiológicas ou fenológicas que afetam aptidão alterando aspectos crescimento, reprodução e sobrevivênciaAtributos dos objetos: são o modo (natureza) e estrutura (organização) dos elementos nos objetosAutovalor(Eigenvalue): número inteiro (escalar) que multiplica um vetor, sendo portanto múltiplo deste. Esses valores representam variância dos eixos e, se convertidos em valores relativos, medem porcentagem de variância contida em cada eixo.Autovetor (Eigenvector): vetor não nulo que muda somente quando é multiplicado por um escalar. O autovetor de uma matriz é encontrado pelo resultado da\nmultiplicação de um vetor pela matriz, que é igual lambda vezes o vetor. Esse vetor resultado passa ser o autovetor, e o lambda o seu respectivo autovalorBloco (ANOVA): é uma área ou período de tempo dentro qual condições ambientais são relativamente homogêneas. O objetivo uso dos blocos é controlar fontes de variações indesejadas na variável dependente que não são de interesse pesquisadorBootstrap: estimador de riqueza de espécies que estima os parâmetros de uma população por reamostragensBuffer (vetor): polígono que representa área dentro de uma determinada distância de um dado vetorial, podendo ser partir de um ponto, linha ou polígonoCamada (vetor ou raster): termo geral, mas geralmente associada à diferentes rasters reunidos num mesmo arquivoCentroide (vetor ou raster): ponto central de uma feição vetorial ou o centro pixel rasterCentróide (multivariado): média ponderada de um conjunto multivariado, menor distância média de todos os objetos num espaço multivariadoChao 1: estimador de riqueza de espécies baseado na abundância das espécies singleton e doubletons dentro de uma amostraChao 2: estimador de riqueza de espécies baseado na incidência (presença e ausência) das espécies singleton e doubletons dentro de uma amostraClado: um grupo de espécies aparentadas descendendo de um único nó na filogeniaCoeficiente de correlação: indica força da relação linear entre duas variáveisCoerção (linguagem R): transformação dos modos dos elementos seguindo uma hierarquia: character > double > integer > logicalCombinação linear: Combinação de várias variáveis (vetores) que são multiplicadas por constantes e adicionadas outras variáveis. Por exemplo: combinação linear de x, y, z pode ser escrita como ax++cz.Community Mean Nearest Taxon Distance (COMDISTNT): métrica de diversidade beta que calcula média da distância filogenética/funcional entre o táxon mais próximo das espécies de duas comunidadesCommunity Mean Pairwise Distance (COMDIST): métrica de diversidade beta que calcula média da distância filogenética/funcional entre espécies de duas comunidadesConsole (RStudio): onde versão da linguagem R instalada é carregada para executar os códigos RStudioConversão (linguagem R): transformação dos modos ou estrutura dos elementos de um objeto partir de funções específicasConversões raster-vetor: transformar dados vetoriais em rasters (rasterização) e transformar rasters em vetores (vetorização)Convex Hull: medida multivariada derivada da computação geométrica que calcula o espaço dos atributos de uma espécie ou de várias espécies em uma comunidadeCorrelação: é um teste que mede força relativa da relação linear entre duas variáveis contínuas. análise de correlação não assume que variável X influencie variável Y, ou que exista uma relação de causa e efeito entre elasCortes e máscaras (raster): ajusta o tamanho de um dado raster uma área menor de interesse, geralmente definido por um dado vetorialCovariável: variável contínua que potencialmente afeta variável resposta, mas não é necessariamente controlada ou manipulada pelo pesquisadorCritério de Informação de Akaike (Akaike Information Criterion - AIC): é uma métrica para seleção de modelos. Ele é usado para determinar qual entre os múltiplos modelos é o mais provável de prever os dados observados, ponderando pelo número de parâmetros dos modelos. Os menores valores de AIC representam modelos com melhores ajustes aos dadosCurva de dominância: veja Diagrama de WhittakerDados geoespaciais: são dados georreferenciados que expressão informações espaciais, podendo ser formato vetorial ou matricial (raster)Dados matriciais (raster): consistem em uma matriz (com linhas e colunas) em que os elementos representam células, geralmente igualmente espaçadas (pixels)Dados tidy: dados organizados, com foco na limpeza e organização dos mesmos, de modo que os dados estão tidy quando: ) variáveis estão nas colunas, ii) observações estão nas linhas e iii) valores estão nas células, sendo que para esse último, não deve haver mais de um valor por célulaDados vetoriais: são representações geométricas (pontos, linhas e polígonos) usadas para mapear fenômenos ou objetos espacialmente explícitos que possuem localização ou dimensões bem definidas, aos quais são atribuídas informações tabularesData frame: classe de objetos que representa dados formato de tabela, com linhas e colunas, mas comportam mais de um modo em suas colunasData Science: nova área de conhecimento que vem se moldando partir desenvolvimento da sociedade em torno da era digital e da grande quantidade de dados gerados e disponíveis pela internetDatum: simplificadamente é relação sistema de coordenadas (geográfica ou projetada) com superfície da TerraDendrograma: diagrama representando uma árvore que organiza elementos, objetos e variáveis por suas semelhanças de maneira hierárquica ascendente. Desse modo, objetos mais próximos compartilham maior semelhança que objetos distantes dendrograma.Desagregação (raster): diminui o tamanho dos pixels (aumentando resolução) de um raster, preenchendo com novos valores que depende tipo de função de preenchimento utilizadaDeviance: é um termo estatístico que mede o ajuste modelo (goodness fit). Quanto menor o valor de deviance melhor o modeloDiagrama de Whittaker: método que utiliza informações visuais ao plotar espécies ranqueadas eixo X da mais abundante para menos abundante, enquanto eixo Y abundâncias relativas das espécies são plotadas em escala logarítmicaDiretório de trabalho: endereço da pasta (ou diretório) de onde o R importará ou exportar dadosDispersão filogenética: espécies coexistindo nas comunidades são menos aparentadas que esperado pelo acasoDistribuição de probabilidade: é uma função estatística que descreve todos os valores e probabilidades possíveis que uma variável aleatória pode assumir dentro de um determinado intervaloDistribuição Gaussiana: veja distribuição normalDistribuição normal: é uma distribuição de probabilidade em formato de sino que é simétrica em torno da média, mostrando que dados próximos à média são mais frequentes que dados distantes da média.Diversidade alfa: é um conceito caracterizado pela diversidade dentro habitat ou unidade amostralDiversidade beta: é um conceito caracterizado pela variação na diversidade entre habitats ou unidades amostraisDiversidade beta filogenética: engloba métricas que utilizam dados de presença e ausência ou abundância das espécies para determinar um valor que representa diferença entre comunidades em relação história evolutiva das linhagensDiversidade de espécies: é um conceito que representa o número de espécies e distribuição de abundância destas espécies em uma comunidadeDiversidade filogenética: engloba métricas que capturam ancestralidade compartilhada entre espécies em termos de quantidade da história evolutiva e o grau de parentesco entre espéciesDiversidade funcional: é um conceito que captura variação grau de expressão de diferentes atributos funcionais entre diferentes populações, comunidades ou ecossistemasDiversidade gama: é um conceito caracterizado pela combinação da diversidade alfa e beta ou definido como diversidade regional englobando todos os habitat ou unidades amostraisDoubletons: número de espécies observadas com abundância de dois indivíduosDuplicate: número de espécies observadas em apenas duas amostrasEquitabilidade de Pielou: é uma métrica que descreve o padrão de distribuição da abundância relativa das espécies na comunidadeErro Tipo : rejeitar hipótese nula de um teste estatístico quando ela é verdadeiraErro Tipo II: aceitar hipótese nula de um teste estatístico quando ela é falsaEscalar: número inteiro com o qual geralmente se faz operações com matrizes (e.g., multiplicação ou adição)Escores: posição das unidades amostrais ao longo de um eixo de ordenação. Pode se referir tanto objetos quanto à variáveis. Escores são fornecidos pela substituição dos valores assumidos pelas variáveis originais nas combinações lineares. São utilizados para ordenar unidades amostrais em um diagrama uni, bi ou tridimensionalESRI Shapefile (vetor): principal formato de dados vetoriais, composto por pelo menos de quatro arquivos: .shp (feição), .dbf (tabela de atributos), .shx (ligação entre .shp e .dbf) e .prj (projeção)Estatística frequentista: são análises paramétricas que estimam probabilidades das frequências observadas dos eventos e usam essas probabilidades como base para inferências.Estrutura dos objetos: diz respeito à organização dos elementos, com relação aos modos e dimensionalidade da disposição desses elementos. De modo bem simples, os elementos podem ser estruturados em seis tipos: ) vetor, ii) fator, iii) matriz, iv) array, v) data frame e vi) listasExtensão (vetor e raster): limites geográficos de dados geoespaciais (vetor e raster), composto por dois pares de coordenadas de longitude e de latitudeExtração (vetor e raster): identifica e retorna valores associados de pixels de um raster com base em um objeto vetorial (ponto, linha e polígono)Extrapolação: é o processo de estimar, além intervalo de observação original, o valor de uma variável com base em sua relação com outra variávelFator: classe de objetos que representa o encadeamento de elementos de um único modo (integer) numa sequência unidimensional, representando medidas de uma variável categórica, podendo ser nominal ou ordinalFator aleatório: pesquisador amostra aleatoriamente os níveis de um fator na populaçãoFator fixo: pesquisador controla todos os níveis fator sobre os quais inferências devem ser feitasFator de inflação da variância (VIF): é um teste que quantifica quanto erro padrão dos coeficientes estimados estão inflados devido à multicolinearidadeFeição (vetor): um elemento dado vetorial associado à cada linha da tabela de atributosFenômeno: Um evento, entidade ou relação observávelFunções: classe de objetos que possui códigos preparados para realizar uma tarefa específica de modo simples, realizando operações em argumentosGeneralized Least Squares (GLS): é um teste estatístico utilizado para estimar coeficientes desconhecidos de um modelo de regressão linear quando variável independente é correlacionada com os resíduosGeoPackage (vetor e raster): banco de dados geoespacial que armazena em apenas um arquivo, dados formato vetorial, raster e também dados não-espaciais (e.g., tabelas)GeoTIFF (raster): principal formato para dados raster, composto geralmente de um arquivo TIFF contendo metadados geoespaciais adicionaisGitHub: é um repositório de hospedagem de código-fonte e arquivos com controle de versões de projetos abertosGoodness fit: refere-se um teste estatístico que determina quão bem os dados da amostra se ajustam uma distribuição de uma populaçãoGrau de liberdade: é o número de observações nos dados que são livres para variar quando estimamos os valores dos parâmetros populacionais desconhecidosHipótese: Afirmação testável derivada ou representando vários componentes de uma teoriaHipótese alternativa (H1): é um conceito estatístico que sugere que diferenças entre grupos ou fenômenos medidos são maiores que o esperado devido variações aleatórias. H1 é o oposto da hipótese nula.Hipótese nula (H0): é um conceito estatístico que sugere que diferenças entre grupos ou fenômenos medidos não são maiores que o esperado devido variações aleatórias. Presume-se que hipótese nula é verdadeira até que evidências indiquem o contrário.Homocedasticidade: é um dos pressupostos dos testes paramétricos como ANOVA, Teste T e regressão linear simples que variância da variável dependente deve ser constante para os valores das variáveis preditoras. Sinônimo de homogeneidade da variânciaHomogeneidade da variância: veja homocedasticidadeIndependência estatística: dois eventos são independentes se ocorrência de um evento não influenciar probabilidade que o outro evento irá ou não ocorrerIndexação (linguagem R): acessa elementos de objetos por sua posição utilizando os operadores [] e [[]] ou por seu nome com o operador $ depois nome objetoÍndice de diversidade: métricas que calculam riqueza de espécies e distribuição de abundância para cada espécie dentro das comunidades.Índice de Gini-Simpson: métrica que quantifica probabilidade de dois indivíduos retirados ao acaso da comunidade pertencerem espécies diferentes. É o inverso índice de SimpsonÍndice de Margalef: métrica que calcula riqueza de espécies ponderando abundância total dentro de cada comunidadeÍndice de Menhinick: métrica que calcula riqueza de espécies ponderando abundância total dentro de cada comunidadeÍndice de Shannon-Wiener: métrica de diversidade de espécies que quantifica incerteza associada em predizer identidade de uma espécie dado o número de espécies e distribuição de abundância para cada espécieÍndice de Simpson: métrica que quantifica probabilidade de dois indivíduos retirados ao acaso da comunidade pertencerem à mesma espécieInf (Infinito): é um número muito grande ou um limite matemáticoInteração raster-vetor: operações derivadas da interação entre raster-vetor, como ajuste tamanho raster ou extração dos valores dos pixels para dados vetoriais (pontos, linhas e polígonos)Interpolação: é o processo de estimar, dentro um domínio de valores conhecidos, um valor desconhecido com base em sua relação com outra variávelJackknife 1: estimador de riqueza de espécies baseado número de espécies que ocorrem em somente uma amostra (uniques)Jackknife 2: estimador de riqueza de espécies baseado número de espécies que ocorrem em somente uma amostra (uniques) e número de espécies ocorrem em exatamente duas amostras (duplicates)Junção de tabelas: combinação de pares de conjunto de dados tabulares por uma ou mais colunas chavesLikelihood-ratio test (LRT): é um teste estatístico que mede o grau ajuste (goodness--fit) entre dois modelos aninhados. Um modelo relativamente mais complexo é comparado um modelo mais simples para ver se ele se ajusta significativamente melhor um determinado conjunto de dados. Ele testa se há necessidade de se incluir uma variável extra modelo para explicar os dadosLinguagem R: ambiente de software livre para computação estatística e criação de gráficosLista: classe de objetos que é um tipo especial de vetor que aceita objetos como elementosLoading: Correlações de Pearson entre cada variável original com cada eixo. Os valores serão maiores (positiva ou negativamente) para aquelas variáveis que forem mais importantes na formação de um dado eixo. Desse modo, representam o peso de uma variável para construção de um eixo e variam de -1 1.Loop : função em que um bloco de códigos é repetido mudando um contador de uma lista de possibilidadesMapa: representação bidimensional de elementos geoespaciais em uma proporção menor, podendo representar dados vetoriais ou raster, com diversos elementos visuais e textuais que facilitam interpretação dos elementos geoespaciais mapeadosMapas animados: mapas que incorporam animações para expressar mudanças nos padrões espaciais ou ao longo tempoMapas estáticos: mapas simples e fixos para visualização de dados, sendo o tipo mais comum de saída visualMapas interativos: mapas que incorporam capacidade de deslocar e ampliar qualquer parte de um conjunto de dados geoespaciais sobreposto em um “mapa da web”Matriz: classe de objetos que representa elementos de um único modo formato de tabela, com linhas e colunasMáxima Verossimilhança: é um método que determina valores para os parâmetros de um modelo. Os valores dos parâmetros são encontrados de tal forma que maximizam probabilidade de que o processo descrito pelo modelo produza os dados que foram realmente observadosMean Pairwise Distance (MPD): é uma métrica que utiliza matriz de distância filogenética para quantificar distância média parentesco entre pares de espécies em uma comunidadeMean Nearest Taxon Distance (MNTD): é uma métrica que utiliza matriz de distância filogenética para quantificar média dos valores mínimos de parentesco entre pares de espécies em uma comunidade. Ou seja, qual o valor médio da distância para o vizinho mais próximoMecanismo: Interação direta de uma relação causal que resulta em um fenômenoModo dos objetos: diz respeito à natureza dos elementos que compõem os dados e que foram atribuídos aos objetos. Os modos geralmente são: numérico tipo inteiro (integer), numérico tipo flutuante (double), texto (character), lógico (logical) ou complexo (complex)Modelo misto: pelo menos um fator experimento é fixo, e pelo menos um fator é aleatório.Modelo nulo: é um procedimento estatístico que usa aleatorizações para gerar distribuições de valores para uma determinada variável de interesse na ausência processo causal em questãoMulticolinearidade: é um conceito estatístico onde variáveis independentes em um modelo são correlacionadasNA (Available): significa dado faltante ou indisponívelNaN (Number): representa indefinições matemáticasNearest Relative Index (NRI): métrica que calcula o tamanho efeito padronizado para métrica Mean Pairwise Distance.Valores positivos de NRI indicam agrupamento filogenético enquanto valores negativos de NRI indicam dispersão filogenéticaNearest Taxon Index (NTI): métrica que calcula o tamanho efeito padronizado para métrica Mean Nearest Taxon Distance.Valores positivos de NTI indicam agrupamento filogenético enquanto valores negativos de NTI indicam dispersão filogenéticaNetCDF (raster): Network Common Data Form é um conjunto de bibliotecas de software e formatos de dados independentes que suportam criação, acesso e compartilhamento de dados científicos orientados arraysNó: o ponto onde uma linhagem dá origem duas ou mais linhagens descendentesNormalidade dos resíduos: é um dos pressupostos dos testes paramétricos como ANOVA, Teste T e regressão linear simples que dependem que os resíduos modelo apresentem distribuição normal ou gaussiana.NULL (Nulo): representa um objeto nulo, sendo útil para preenchimento em aplicações de programaçãoNúmeros de Hill: é uma métrica que transforma riqueza e distribuição da abundância das espécies em números efetivos de espécies ou diversidade verdadeiraNúmero efetivo de espécies: é o número de espécies igualmente abundantes (.e., todas espécies com mesma abundância) necessárias para produzir o valor observado para um determinado índiceObjetos: palavras às quais são atribuídos dados através da atribuição. criação de objetos possibilita manipulação de dados ou armazenar os resultados de análises. Em outras linguagens de programação são denominados variáveisOperações geoespaciais: são operações para acessar ou alterar propriedades não-espaciais, espaciais e geométricas dos dados geoespaciais, divididas em: operações de atributos, operações espaciais e operações geométricasOperações geoespaciais de atributos: modificação de objetos geoespaciais baseado em informações não espaciais associadas dados geoespaciais, como tabela de atributos ou valores das células e nome das camadas dos rastersOperações geoespaciais espaciais: modificações de objetos geoespaciais baseado em informações espaciais, como localização e formatoOperações geoespaciais geométricas: modificações em objetos geoespaciais baseado na geometria vetor ou raster e na interação e conversão entre vetor-rasterOperadores: conjuntos de caracteres que realiza operações, agrupados em cinco tipos principais: aritméticos, relacionais, lógicos, atribuição e diversosPacotes (linguagem R): conjuntos extras de funções para executar tarefas específicasPadrão: Eventos repetidos, entidades recorrentes ou relações replicadas tempo ou espaçoPhylogenetic Diversity (PD): é uma métrica definida pela soma comprimento dos ramos conectando todas espécies na comunidadePhylogenetic Endemism (PE): é uma métrica que calcula fração dos ramos restritos regiões específicasPhylogenetic index beta diversity (Phylosor): métrica de similaridade que determina o comprimento total dos ramos da filogenia que é compartilhado entre pares de comunidadesPhylogenetic Species Richness (PSR): é uma métrica diretamente comparável ao número de espécies na comunidade, mas inclui o parentesco filogenético entre espéciesPhylogenetic Species Variability (PSV): é uma métrica que estima quantidade relativa dos comprimentos dos ramos não compartilhados entre comunidadesPipe (%>%): operador implementado por uma função que faz com o que o resultado de uma função seja o primeiro argumento da função seguinte, permitindo o encadeamento de várias funções eliminando necessidade de criar objetos para armazenar resultados intermediáriosPivotagem de dados: transporte de dados que estão em linhas para colunas e vice-versa, fazendo referência cruzada ou rotacionando os dados. Partirmos de dados formato longo (long, muitas linhas e poucas colunas) e criamos dados formato largo (wide, poucas linhas e muitas colunas) e vice-versaPixel (raster): também chamado célula, é unidade geoespacial raster, representando o elemento da matriz de dadosPolígono convexo: operação que liga os pontos externos de um conjunto de pontos e cria um polígono partir delesPolígono de Voronoi: polígonos irregulares são criados partir da proximidade dos pontos, de modo estimar uma área de abrangência entorno dos mesmosPolitomia: três ou mais linhagens descendendo de um único nóPopulação: é um conjunto de indivíduos ou elementos semelhantes que interessa para alguma pergunta ou hipótesePopulação amostral: é um conjunto de indivíduos ou elementos semelhantes que estão de fato acessível para serem amostradosPredição espacial: utiliza os coeficientes de um modelo ajustado para gerar um raster de predição para todos os pixels considerando os dados preditores de entrada modelo, extrapolando predição da respostaPredição: uma declaração de expectativa deduzida da estrutura lógica ou derivada da estrutura causal de uma teoriaPressuposto: condições necessárias para sustentar uma hipótese ou construção da teoriaPrincipais elementos de um mapa: um mapa possui diversos elementos que facilitam sua interpretação, dentre eles: ) mapa principal, ii) mapa secundário iii) título, iv) legenda (apresentando informações detalhadas das classes ou escala de valores, v) barra de escala, vi) indicador de orientação (Norte), vii) gride de coordenadas e viii) descrição CRSProcesso: um subconjunto de fenômenos em que os eventos seguem uns aos outros tempo ou espaço, que podem ou não serem causalmente conectados. É causa, mecanismo ou limitação explicando um padrãoProgramação Funcional: organização código como funções e variáveis que trabalham de forma unificada para resolução de um problemaProjeto RStudio: arquivo formato .Rproj que facilita o trabalho com o RStudio, pois define o diretório automaticamente e permite o controle de versãoPull request: é um método de submeter contribuições que serão revisadas pelos responsáveis de um projeto de desenvolvimento abertoRaiz: representa o ancestral comum de todas espécies na filogeniaRamo: uma linha orientada ao longo de um eixo terminais-raiz que conecta os nós na filogeniaRarefação: é uma métrica usada para calcular o número esperado de espécies em cada comunidade tendo como base comparativa um valor em que todas amostras ou número de indivíduos atinjam um tamanho padrão entre comunidadesRarefação baseada nas amostras (Sampled-based): comparações são padronizadas pela comunidade com menor número de amostragensRarefação baseada na cobertura (Coverage-based): é uma medida que determina proporção de amostras ou número de indivíduos da comunidade que representa espécies amostradasRarefação baseada nos indivíduos (Individual-based): comparações são feitas considerando abundância da comunidade padronizada pelo menor número de indivíduosRasterização: conversão de dados vetoriais para raster realizada de pontos, linhas ou polígonos para rastersRegressão: é um teste usado para analisar relação entre uma ou mais variáveis preditoras (Xn) e uma variável resposta (Y). regressão assume uma relação de causa e efeito entre variáveisReprojeção: transformação Sistema de Referência de Coordenadas (CRS) de um dado geoespacial, alterando o CRS original para outro. Na reprojeção alteramos tanto unidade dado geoespacial (unidades em ‘longitude/latitude’ ou unidades de metros), quanto o Datum datumResíduo: é diferença entre os valores preditos e observados dos dados. São também chamados de erroResolução (raster): termo amplo que pode ser usado em diversos contextos. Para dados raster, é geralmente associado ao tamanho (dimensão) pixel (.e., altura e largura)RStudio: ambiente de desenvolvimento integrado (IDE) para R. Inclui um console, editor de realce de sintaxe que suporta execução direta de código, bem como ferramentas para plotagem, histórico, depuração e gerenciamento de espaço de trabalhoScript (RStudio): arquivos de texto simples, criados com extensão (terminação) .R onde os códigos são escritos e salvosSeleção de modelos: é um processo usado para comparar o valor relativo de diferentes modelos estatísticos e determinar qual deles é o mais adequado para os dados observadosSérie de Hill: veja Número de HillSingleton: número de espécies observadas com abundância de um indivíduoSistema de Coordenadas: composto por dois sistemas, o Sistema de Coordenadas Geográficas e o Sistema de Coordenadas Projetadas, que ângulos e metros, respectivamenteSistema de Referência de Coordenadas (CRS): também chamada projeção, define referência geoespacial dos dados vetor e raster na superfície da Terra, composto pelo Sistema de Coordenadas e DatumTabela de atributos: dado tabular que inclui dados geoespaciais e dados alfanuméricos, geralmente associada dados vetoriaisTerminal (inglês tip): o final ramo representando uma espécie atual ou extinta (pode também representar gêneros, indivíduos, genes, etc.)Teste de Levene: é um teste estatístico utilizado para verificar homogeneidade da variância entre dois ou mais gruposTeste de Shapiro-Wilk: é um teste estatístico utilizado para verificar se os dados apresentam distribuição normalTeste T (de Student): é um teste estatístico que segue uma distribuição t de Student para rejeitar ou não uma hipótese nula de médias iguais entre dois gruposTeste T pareado: é um teste estatístico que usa dados medidos duas vezes na mesma unidade amostral, resultando em pares de observações para cada amostra (amostras pareadas). Ele determina se diferença da média entre duas observações é zeroTibble: classe de objetos que é uma versão aprimorada data frame. É classe aconselhada para que funções tidyverse funcionem melhor sobre conjuntos de dados tabularestidyverse: “dialeto novo” para linguagem R, onde tidy quer dizer organizado, arrumado, ordenado, e verse é universo. Operacionalizado R através de uma coleção de pacotes que atuam fluxo de trabalho comum da ciência de dados: importação, manipulação, exploração, visualização, análise e comunicação de dados e análises. O principal objetivo tidyverse é aproximar linguagem para melhorar interação entre ser humano e computador sobre dados, de modo que os pacotes compartilham uma filosofia de design de alto nível e gramática, além da estrutura de dados de baixo nívelUltramétrica: distância de todos os terminais até raiz são idênticas. Característica requerida pela maioria dos índices de diversidade filogenéticaUnidade amostral: é o indivíduo (ou elemento) da população amostral sobre o qual medida de interesse será observadaUnique: número de espécies observadas em apenas uma amostraUnique Fraction metric (UniFrac): métrica de dissimilaridade que determina fração única da filogenia contida em cada uma das duas comunidadesValor de P: probabilidade de um teste estatístico ser igual ou maior que o observado, dado que hipótese nula é verdadeiraValores faltantes e especiais: valores reservados que representam dados faltantes, indefinições matemáticas, infinitos e objetos nulosVariance Pairwise Distance (VPD): é uma métrica que utiliza matriz de distância filogenética para quantificar variância parentesco entre pares de espécies em uma comunidadeVariável categórica: são variáveis que não possuem valores quantitativos e são definidas por categorias ou grupos distintosVariável contínua: são variáveis numéricas que têm um número infinito de valores entre dois valores quaisquer. Neste caso, valores fracionais fazem sentidoVariável dependente: é uma variável mensurada ou observada de interesse pesquisador que depende valor de outra variávelVariável discreta: é uma variável numérica que têm um número contável de valores inteirosVariável explicativa: ver variável independenteVariável independente: variável mensurada ou observada pelo pesquisador que prevê ou afeta variável dependenteVariável nominal: são variáveis categóricas que não apresentam ordenação dentre categorias (e.g., preto, branco e rosa)Variável ordinal: são variáveis categóricas com ordenação entre categorias (e.g., pequeno, médio e grande)Variável preditora: ver variável independenteVariável resposta: ver variável dependenteVariograma: é uma descrição da continuidade espacial dos dados. Ele mede variabilidade entre pares de pontos em várias distânciasVetor: classe de objetos que representa o encadeamento de elementos de um único modo numa sequência unidimensionalVetor (Multivariada): coluna de uma matrizVetorização (vetor): conversão de dados rasters para vetor, sendo que esse vetor receberá os valores dos pixels raster, podendo ser convertido em pontos, isolinhas ou polígonos","code":""},{"path":"referências.html","id":"referências","chapter":"Referências","heading":"Referências","text":"","code":""}]
